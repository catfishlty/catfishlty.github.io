[{"content":"简述 Dump 文件将某一时刻的 Java Heap 保存下来，便于分析问题。除了可以通过 jmap 命令手动触发 Dump 操作；也可以在 JVM 启动参数里，通过添加 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=\u0026lt;LOG_PATH\u0026gt; 来设置应用在发生 OOM 时将 Java Heap 保存下来，便于后面的问题分析与解决。\n在服务器中可以很容易获得 Dump 文件，但是分析却是一个非常消耗服务器资源的操作。对内存的影响，在默认情况下启动 jhat 可能在打开 3GB 大小的 Dump 文件时因为内存不足发生 OOM 的问题，但如果给 jhat 分配过大的堆内存，可能会影响应用正常运行。对 CPU 的影响，分析时会大幅度占用 CPU 时间，可能会影响应用的正常运行。因此，需要将 Dump 文件放置到本地进行分析。\n本地获取 Dump 文件 获取应用 PID $ jps 30444 rest-service-0.0.1-SNAPSHOT.jar 9460 Jps 获取 Dump 文件 所有对象 $ jmap -dump:format=b,file=./dump.hprof 30444 Dumping heap to E:\\temp\\dump.hprof ... Heap dump file created 存活对象 $ jmap -dump:live,format=b,file=./dump_live.hprof 30444 Dumping heap to E:\\temp\\dump_live.hprof ... Heap dump file created 存活对象的 Dump 操作将会触发一次 Full GC\n将 Dump 文件分割 $ split -b 100M -d dump.hprof $ ls dump.hprof x00 x01 x02 参数详情    参数 描述     -b 100M 分割文件大小为 100MB   -d 加上该参数后，分割后的文件将使用数字后缀；反之，分割文件名将是字母后缀。详情见下表。    分割文件格式    类型 样例     数字后缀 x01 x02 x03   字母后缀 xaa xab xac    分割文件传输 将分割文件传输至本地保存\n分割文件合并 该步骤样例在 Windows 10 上的 CMD 运行。（ * 使用 PowerShell 可能导致运行失败）\nC:\\dump\u0026gt;copy /b x* dump.hprof x00 x01 x02 Copied 1 file C:\\dump\u0026gt;dir dump.hprof x00 x01 x02 这样就在本地获得了服务器中的 Dump 文件\nDump 分析 jhat C:\\dump\u0026gt;jhat -port 8081 dump_live.hprof Reading from dump_live.hprof... Dump file created Thu Aug 19 16:47:50 CST 2021 Snapshot read, resolving... Resolving 188752 objects... Chasing references, expect 37 dots..................................... Eliminating duplicate references..................................... Snapshot resolved. Started HTTP server on port 8081 Server is ready. 使用 CMD 启动 jhat 分析 Dump 文件，并可以在本地 8081 端口上查看分析结果。\n jhat Heap Analysis \nVisual VM by Oracle JDK Visual VM 是仅包含在 Oracle JDK 中一款 GUI 工具，其功能主要能够完成 JConsole 的监控功能以及对 Dump 文件的可视化呈现。\n打开 Dump 文件选择框 点击 [文件] -\u0026gt; [装入] 打开文件选择框\n Visual VM - Open File Picker \n选择并打开 Dump 文件  选择文件类型为 堆Dump （*.hprof） 选择需要分析的 Dump 文件 点击确定开始分析   Visual VM - Open Dump File \n查看 Dump 文件概要  展示基本信息，包括文件名、日期、大小等文件基本信息和加载字节、类总数等堆内存基本信息。 展示运行环境。   Visual VM - Overview \n类分析  选择 类 选项卡 可以获得各对象的类型、数量、大小信息，为分析提供数据支持。   Visual VM - Class Analysis \nOQL  选择 OQL 选项卡 输入 OQL 语句 点击执行并等待执行完成 在 “查询结果” 中查看结果   Visual VM - OQL \nMAT by Eclipse MAT 全称为 Memory Analyzer Tool ，一个基于 Eclipse 的内存分析工具,是一个快速、功能丰富的 JAVA heap 分析工具,它可以帮助我们查找内存泄漏和减少内存消耗。\nMAT 可以在 www.eclipse.org/mat/ 下载\n打开 Dump 文件  点击 [File] 点击 [Open Heap Dump\u0026hellip;] 选择 Dump 文件 点击确定开始分析   MAT - Open Dump File \n MAT - Open Dump File \n默认启动报告 开始使用工具时，加载 Dump 文件完毕后将会有一个选择框弹出，可以选择一种默认的报告类型。 默认报告为内存泄漏报告。\n MAT - Startup \n概览 概览页中可以通过饼图查看大对象的分布。另外也有其他的多重功能。\n Actions  Histogram  直方图 列出每个类的实例数。   Dominator Tree  支配树 列出最大的类以及什么导致它们存活。   Top Comsumers  消费者 Top 列表 打印按类和包分组的最大的对象。   Duplicate Classes  重复类 检测由多个类加载器加载的类。     Reports  Leak Suspects  泄漏嫌疑 包括泄漏嫌疑和系统概述。   Top Components  组件 Top 列表 列出大于堆内存 1% 的组件。   Leak Suspects by Snapshot Comparison  通过快照比较检测泄漏 比较两个快照展示泄漏嫌疑和系统概述。     Step By Step  Component Report  组件报告 分析属于公共根包或类加载器的对象。       MAT - Overview \nMAT 启动参数配置 通过打开 MemoryAnalyzer.ini 文件，进行参数配置。 如图所示，加入 -xmx6g 指定 MAT 启动的堆内存最大为 6G，为防止 MAT 分析 Dump 文件抛出 OOM 错误\n MAT - Config \n总结 Dump 文件保存了 Java Heap 的完整信息，能够有助于我们分析内存。除此之外，也应与其他工具相结合，从多个维度上分析内存结构，便于我们尽快获得答案。\n","date":"2021-08-20T09:34:00+08:00","image":"https://www.catfish.top/p/jvm-opt-4/jvm-white_hu06bdc8659dce216bb36458f3911d03ef_23035_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.catfish.top/p/jvm-opt-4/","title":"JVM调优（四）"},{"content":"简述 Java 自带很多工具能够辅助我们完成对 Java 应用的性能指标分析。 本文将介绍下列 5 种命令行工具。\n jps jstat jstack jmap jhat  jps JPS 全程 Java Virtual Machine Process Status Tool，是 Java 提供的一个显示当前所有 Java 进程 PID 的命令，适合简单查看当前 Java 进程的信息。\n详情见：jps - Java Virtual Machine Process Status Tool\n运行格式 jps [ options ] [ hostid ] 参数介绍    参数 描述     -q 仅生成本地 VM 标识符列表，不打印 main 方法的类名、JAR 文件名和参数的输出。   -m 输出传递给 main 方法的参数。对于嵌入式 JVM，输出可能为 null。   -l 输出应用程序主类的完整包名或应用程序 JAR 文件的完整路径名。   -v 输出 JVM 的参数。   -V 通过标志文件（.hotspotrc 文件或 -XX:Flags=\u0026lsquo;filename\u0026rsquo; 参数指定的文件）输出传递给 JVM 的参数。   -Joption 将参数传递给 JPS 调用的 Java 启动器。例如，-J-Xms48m 将启动内存设置为 48Mb。 -J 将选项传递给执行应用程序的底层 VM 是一种一般规范。    使用 $ jps 30444 rest-service-0.0.1-SNAPSHOT.jar 9460 Jps $ jps -q 30444 9460 $ jps -m 30236 Jps -m 30444 rest-service-0.0.1-SNAPSHOT.jar -XX:+UseG1GC $ jps -l 31812 sun.tools.jps.Jps 30444 .\\rest-service-0.0.1-SNAPSHOT.jar $ jps -v 29260 Jps -Denv.class.path=.;C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\\\lib\\dt.jar;C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\\\lib\\tools.jar; -Dapplication.home=C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot -Xms8m 30444 rest-service-0.0.1-SNAPSHOT.jar $ jps -J-Xms48m -v 16060 Jps -Denv.class.path=.;C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\\\lib\\dt.jar;C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\\\lib\\tools.jar; -Dapplication.home=C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot -Xms8m -Xms48m 30444 rest-service-0.0.1-SNAPSHOT.jar jstat jstat 工具能够显示 HotSpot Java 虚拟机 (JVM) 的性能统计信息。目标 JVM 由其虚拟机标识符或下面描述的 vmid 参数来确定。\nJDK 1.7: jstat - Java Virtual Machine Statistics Monitoring Tool JDK 1.8: Java Platform, Standard Edition Tools Reference - jstat\n运行格式 jstat [ generalOption | outputOptions vmid [interval[s|ms] [count]] ] 输出参数    参数 描述     -help 显示帮助信息   -options 显示统计参数列表。       参数 显示内容     -class 类加载器行为统计信息。   -compiler HotSpot JIT 编译器行为统计信息。   -gc GC 堆的行为统计。   -gccapacity Heap 中对象年龄代的容量及其相应空间的统计。   -gccause 垃圾收集统计信息摘要（与 -gcutil 相同），以及上次和当前（如果适用）垃圾收集事件的原因。   -gcnew 新生代行为统计。   -gcnewcapacity 新生代的大小及其对应的空间的统计信息。   -gcold 老年代和永久代的行为统计。(JDK 1.7); 老年代和元空间的行为统计。(JDK 1.8);   -gcoldcapacity 老年代容量的统计。   -gcmetacapacity 元空间容量的统计。(JDK 1.8)   -gcpermcapacity 永久代容量的统计。(JDK 1.7)   -gcutil 垃圾收集统计摘要。   -printcompilation HotSpot 编译方法统计。    使用 -class Class loader 统计信息\n$ jstat -class 30444 Loaded Bytes Unloaded Bytes Time 5544 10174.8 0 0.0 1.73    参数名称 描述     Loaded 加载的类的数量。   Bytes 加载的类大小，单位：KB   Unloaded 卸载的类的数量。   Bytes 卸载的类大小，单位：KB   Time 执行类加载和卸载操作所花费的时间。    -compiler Java HotSpot VM Just-in-Time 编译器统计信息\n$ jstat -compiler 30444 Compiled Failed Invalid Time FailedType FailedMethod 3084 1 0 3.29 1 java/lang/StringCoding encode    参数名称 描述     Compiled 执行的编译数。   Failed 编译任务失败数。   Invalid 失效的编译数。   Time 执行编译任务所花费的时间。   FailedType 上次失败编译的编译类型。   FailedMethod 上次编译失败的类名和方法。    -gc 堆 GC 的统计信息。\n$ jstat -gc 30444 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 3072.0 10752.0 3008.0 0.0 96256.0 56913.5 104960.0 7314.0 27440.0 25574.4 3632.0 3186.9 4 0.020 1 0.018 0.038    参数名称 描述     S0C 当前 Survivor 0 的容量 (KB)。   S1C 当前 Survivor 1 的容量 (KB)。   S0U 当前 Survivor 0 的使用大小 (KB)。   S1U 当前 Survivor 1 的使用大小 (KB)。   EC 当前 Eden 区的容量 (KB)。   EU 当前 Eden 区的使用大小 (KB)。   OC 当前 Old 区的容量 (KB)。   OU 当前 Old 区的使用大小 (KB)。   PC 永久代的容量 (KB)。(JDK 1.7)   PU 永久代的使用大小 (KB)。(JDK 1.7)   MC 元空间的容量 (KB)。(JDK 1.8)   MU 元空间的使用大小 (KB)。(JDK 1.8)   CCSC 类压缩空间的容量 (KB)。(JDK 1.8)   CCSU 类压缩空间的使用大小 (KB)。(JDK 1.8)   YGC Young GC 触发次数。   YGCT Young GC 运行时间。   FGC Full GC 触发次数。   FGCT Full GC 运行时间。   GCT 总 GC 时间。    -gccapacity 内存池生成和空间容量。\n$ jstat -gccapacity 30444 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC 86528.0 1379328.0 143872.0 3072.0 10752.0 96256.0 173568.0 2759680.0 104960.0 104960.0 0.0 1073152.0 27440.0 0.0 1048576.0 3632.0 4 1    参数名称 描述     NGCMN 新生代最小容量 (KB)。   NGCMX 新生代最大容量 (KB)。   NGC 当前新生代容量 (KB)。   S0C 当前 Survivor 0 的容量 (KB)。   S1C 当前 Survivor 1 的容量 (KB)。   EC 当前 Eden 区的容量 (KB)。   OGCMN 老年代最小容量 (KB)。   OGCMX 老年代最大容量 (KB)。   OGC 当前老年代容量 (KB)。   OC 当前 Old 空间容量 (KB)。   PGCMN 最小永久代容量 (KB)。(JDK 1.7)   PGCMX 最大永久代容量 (KB)。(JDK 1.7)   PGC 当前永久代容量 (KB)。(JDK 1.7)   PC 当前 Permanent 空间容量 (KB)。(JDK 1.7)   MCMN 最小元空间容量 (KB)。 (JDK 1.8)   MCMX 最大元空间容量 (KB)。 (JDK 1.8)   MC 当前元空间大小 (KB)。 (JDK 1.8)   CCSMN 最小类压缩空间容量 (KB)。 (JDK 1.8)   CCSMX 最大类压缩空间容量 (KB)。 (JDK 1.8)   CCSC 当前类压缩空间容量 (KB)。 (JDK 1.8)   YGC Young GC 触发次数。   FGC Full GC 触发次数。    -gccause 此选项显示与 -gcutil 选项相同的垃圾收集统计摘要信息，但包括上次垃圾收集事件和当前垃圾收集事件的原因（如果适用）。除了 -gcutil 列出的之外，此选项还添加了以下列。\n$ jstat -gccause 30444 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT LGCC GCC 97.92 0.00 59.13 6.97 93.20 87.74 4 0.020 1 0.018 0.038 Allocation Failure No GC    参数名称 描述     LGCC 上次垃圾回收的原因。   GCC 当前垃圾回收的原因。    -gcnew 新生代统计信息。\n$ jstat -gcnew 30444 S0C S1C S0U S1U TT MTT DSS EC EU YGC YGCT 3072.0 10752.0 3008.0 0.0 7 15 10752.0 96256.0 56913.5 4 0.020    参数名称 描述     S0C 当前 Survivor 0 的容量 (KB)。   S1C 当前 Survivor 1 的容量 (KB)。   S0U 当前 Survivor 0 的使用大小 (KB)。   S1U 当前 Survivor 0 的使用大小 (KB)。   TT 对象在新生代存活的次数。   MTT 对象在新生代存活的最大次数。   DSS 期望的 Survivor 区大小 (KB)。   EC Eden 区的容量 (KB)。   EU Eden 区的使用大小 (KB)。   YGC Young GC 触发次数。   YGCT Young GC 运行时间。    -gcnewcapacity 新生代空间大小统计。\n$ jstat -gcnewcapacity 30444 NGCMN NGCMX NGC S0CMX S0C S1CMX S1C ECMX EC YGC FGC 86528.0 1379328.0 143872.0 459776.0 3072.0 459776.0 10752.0 1378304.0 96256.0 4 1    参数名称 描述     NGCMN 新生代最小容量 (KB)。   NGCMX 新生代最小容量 (KB)。   NGC 当前新生代容量 (KB)。   S0CMX 最大 Survivor 0 的容量 (KB)。   S0C 当前 Survivor 0 的容量 (KB)。   S1CMX 最大 Survivor 1 的容量 (KB)。   S1C 当前 Survivor 1 的容量 (KB)。   ECMX 最大 Eden 区的容量 (KB)。   EC 当前 Eden 区的容量 (KB)。   YGC Young GC 触发次数。   FGC Full GC 触发次数。    -gcold  老一代和永久代统计。(JDK 1.7) 老年代和元空间行为统计。(JDK 1.8)  $ jstat -gcold 30444 MC MU CCSC CCSU OC OU YGC FGC FGCT GCT 27440.0 25574.4 3632.0 3186.9 104960.0 7314.0 4 1 0.018 0.038    参数名称 描述     PC 永久代的容量 (KB)。(JDK 1.7)   PU 永久代的使用大小 (KB)。(JDK 1.7)   MC 元空间的容量 (KB)。(JDK 1.8)   MU 元空间的使用大小 (KB)。(JDK 1.8)   CCSC 类压缩空间的容量 (KB)。(JDK 1.8)   CCSU 类压缩空间的使用大小 (KB)。(JDK 1.8)   OC 当前 Old 区的容量 (KB)。   OU 当前 Old 区的使用大小 (KB)。   YGC Young GC 触发次数。   FGC Full GC 触发次数。   FGCT Full GC 运行时间。   GCT 总 GC 时间。    -gcoldcapacity 老年代大小统计信息。\n$ jstat -gcoldcapacity 30444 OGCMN OGCMX OGC OC YGC FGC FGCT GCT 173568.0 2759680.0 104960.0 104960.0 4 1 0.018 0.038    参数名称 描述     OGCMN 老年代最小容量 (KB)。   OGCMX 老年代最大容量 (KB)。   OGC 当前老年代容量 (KB)。   OC 当前 Old 区的容量 (KB)。   YGC Young GC 触发次数。   FGC Full GC 触发次数。   FGCT Full GC 运行时间。   GCT 总 GC 时间。    -gcpermcapacity (JDK 1.7 Only) 永久代大小统计信息\n   参数名称 描述     PGCMN 最小永久代容量 (KB)。   PGCMX 最大永久代容量 (KB)。   PGC 当前永久代容量 (KB)。   PC 当前 Permanent 空间容量 (KB)。   YGC Young GC 触发次数。   FGC Full GC 触发次数。   FGCT Full GC 运行时间。   GCT 总 GC 时间。    -gcmetacapacity (JDK 1.8 Only) 元空间大小统计信息。\n$ jstat -gcmetacapacity 30444 MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC FGCT GCT 0.0 1073152.0 27440.0 0.0 1048576.0 3632.0 4 1 0.018 0.038    参数名称 描述     MCMN 最小元空间容量 (KB)。   MCMX 最大元空间容量 (KB)。   MC 当前元空间大小 (KB)。   CCSMN 最小类压缩空间容量 (KB)。   CCSMX 最大类压缩空间容量 (KB)。   YGC Young GC 触发次数。   FGC Full GC 触发次数。   FGCT Full GC 运行时间。   GCT 总 GC 时间。    -gcutil 垃圾收集统计概要。\n$ jstat -gcutil 30444 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 97.92 0.00 59.13 6.97 93.20 87.74 4 0.020 1 0.018 0.038    参数名称 描述     S0 Survivor 0 空间利用率。单位：%。   S1 Survivor 1 空间利用率。单位：%。   E Eden 区空间利用率。单位：%。   O Old 区空间利用率。单位：%。   P Permanent 区空间利用率。单位：%。(JDK 1.7)   M Metaspace 空间利用率。单位：%。(JDK 1.8)   CCS 类压缩空间利用率。单位：%。(JDK 1.8)   YGC Young GC 触发次数。   YGCT Young GC 运行时间。   FGC Full GC 触发次数。   FGCT Full GC 运行时间。   GCT 总 GC 时间。    -printcompilation Java HotSpot VM 编译器方法统计信息。\n$ jstat -printcompilation 30444 Compiled Size Type Method 3107 172 1 java/util/concurrent/locks/AbstractQueuedSynchronizer release    参数名称 描述     Compiled 最近编译的方法执行的编译任务数。   Size 最近编译的方法的字节码的字节数。   Type 最近编译的方法的编译类型。   Method 最近编译的方法的类名和方法名。类名使用斜线 (/) 而不是点 (.) 作为名称空间分隔符。方法名称是指定类中的方法。这两个字段的格式与 HotSpot -XX:+PrintCompilation 选项一致。    jstack jstack 是 Java 虚拟机自带的一种堆栈跟踪工具。jstack 为给定的 Java 进程或核心文件或远程调试服务器打印 Java 线程的 Java 堆栈跟踪。对于每个 Java 框架，将打印完整的类名、方法名、bci（字节码索引）和行号（如果有）。使用 -m 选项，jstack 命令使用程序计数器 (PC) 打印所有线程的 Java 和本机帧。对于每个原生帧，打印最接近 PC 的原生符号（如果可用）。当指定的进程在 64 位 Java 虚拟机上运行时，您可能需要指定 -J-d64 选项，例如：jstack -J-d64 -m pid。\nOracle - Java Documentation - jstack\n格式 jstack [ options ] pid jstack [ options ] executable core jstack [ options ] [ server-id@ ] remote-hostname-or-IP    关键字 描述     options 命令行参数   pid 打印堆栈跟踪的进程 ID。该进程必须是 Java 进程。   executable 从中生成核心转储的 Java 可执行文件。   core 要打印堆栈跟踪的核心文件。   remote-hostname-or-IP 远程调试服务器主机名或 IP 地址。   server-id 当多个调试服务器在同一远程主机上运行时使用的可选唯一 ID。    参数    参数名称 描述     -F 当 jstack [-l] pid 没有响应时强制进行堆栈转储。   -l 大量列出信息。打印有关锁的附加信息，例如拥有的java.util.concurrent 可拥有的同步器列表。   -m 打印具有 Java 和 Native C/C++ 帧的混合模式堆栈跟踪。   -h 打印帮助消息。   -help 打印帮助消息。    使用 $ jstack 30444 2021-08-19 16:05:29 Full thread dump OpenJDK 64-Bit Server VM (25.282-b08 mixed mode): \u0026#34;DestroyJavaVM\u0026#34; #31 prio=5 os_prio=0 tid=0x00000187de146800 nid=0x78e8 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \u0026#34;http-nio-8080-Acceptor\u0026#34; #30 daemon prio=5 os_prio=0 tid=0x00000187de145800 nid=0x4790 runnable [0x0000005d8d2ff000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method) at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:421) at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:249) - locked \u0026lt;0x000000076ed7ec98\u0026gt; (a java.lang.Object) at org.apache.tomcat.util.net.NioEndpoint.serverSocketAccept(NioEndpoint.java:551) at org.apache.tomcat.util.net.NioEndpoint.serverSocketAccept(NioEndpoint.java:80) at org.apache.tomcat.util.net.Acceptor.run(Acceptor.java:106) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-ClientPoller\u0026#34; #29 daemon prio=5 os_prio=0 tid=0x00000187de145000 nid=0x5848 runnable [0x0000005d8d1fe000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method) at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:314) at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:293) at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:174) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked \u0026lt;0x000000076eef6158\u0026gt; (a sun.nio.ch.Util$3) - locked \u0026lt;0x000000076eef6148\u0026gt; (a java.util.Collections$UnmodifiableSet) - locked \u0026lt;0x000000076eef5ff8\u0026gt; (a sun.nio.ch.WindowsSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.tomcat.util.net.NioEndpoint$Poller.run(NioEndpoint.java:793) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-10\u0026#34; #28 daemon prio=5 os_prio=0 tid=0x00000187de143000 nid=0x5584 waiting on condition [0x0000005d8d0fe000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-9\u0026#34; #27 daemon prio=5 os_prio=0 tid=0x00000187de142800 nid=0x7bb8 waiting on condition [0x0000005d8cfff000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-8\u0026#34; #26 daemon prio=5 os_prio=0 tid=0x00000187de148800 nid=0x83b0 waiting on condition [0x0000005d8ceff000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-7\u0026#34; #25 daemon prio=5 os_prio=0 tid=0x00000187de149800 nid=0x8190 waiting on condition [0x0000005d8cdfe000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-6\u0026#34; #24 daemon prio=5 os_prio=0 tid=0x00000187de144000 nid=0x3574 waiting on condition [0x0000005d8ccff000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-5\u0026#34; #23 daemon prio=5 os_prio=0 tid=0x00000187de037000 nid=0x7b64 waiting on condition [0x0000005d8cbff000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-4\u0026#34; #22 daemon prio=5 os_prio=0 tid=0x00000187dcb9e000 nid=0x2a50 waiting on condition [0x0000005d8cafe000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-3\u0026#34; #21 daemon prio=5 os_prio=0 tid=0x00000187dcb9d800 nid=0x785c waiting on condition [0x0000005d8c9fe000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-2\u0026#34; #20 daemon prio=5 os_prio=0 tid=0x00000187ddede800 nid=0x44c0 waiting on condition [0x0000005d8c8fe000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-exec-1\u0026#34; #19 daemon prio=5 os_prio=0 tid=0x00000187dd29d800 nid=0x4444 waiting on condition [0x0000005d8c7fe000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x000000076ee9e510\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:108) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:33) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;http-nio-8080-BlockPoller\u0026#34; #18 daemon prio=5 os_prio=0 tid=0x00000187dea3b800 nid=0x1d70 runnable [0x0000005d8c6fe000] java.lang.Thread.State: RUNNABLE at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method) at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:314) at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:293) at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:174) at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86) - locked \u0026lt;0x000000076ed81670\u0026gt; (a sun.nio.ch.Util$3) - locked \u0026lt;0x000000076ed815e8\u0026gt; (a java.util.Collections$UnmodifiableSet) - locked \u0026lt;0x000000076ed811e8\u0026gt; (a sun.nio.ch.WindowsSelectorImpl) at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) at org.apache.tomcat.util.net.NioBlockingSelector$BlockPoller.run(NioBlockingSelector.java:313) \u0026#34;container-0\u0026#34; #17 prio=5 os_prio=0 tid=0x00000187de6c4800 nid=0x5488 waiting on condition [0x0000005d8c5ff000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at org.apache.catalina.core.StandardServer.await(StandardServer.java:570) at org.springframework.boot.web.embedded.tomcat.TomcatWebServer$1.run(TomcatWebServer.java:197) \u0026#34;Catalina-utility-2\u0026#34; #16 prio=1 os_prio=-2 tid=0x00000187dea67000 nid=0x66f4 waiting on condition [0x0000005d8c4fe000] java.lang.Thread.State: TIMED_WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x00000007746f4b70\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078) at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093) at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;Catalina-utility-1\u0026#34; #15 prio=1 os_prio=-2 tid=0x00000187de4b8800 nid=0x3bf4 waiting on condition [0x0000005d8c3fe000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for \u0026lt;0x00000007746f4b70\u0026gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039) at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1088) at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:748) \u0026#34;Service Thread\u0026#34; #10 daemon prio=9 os_prio=0 tid=0x00000187dc61f000 nid=0x2b7c runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE \u0026#34;C1 CompilerThread3\u0026#34; #9 daemon prio=9 os_prio=2 tid=0x00000187da84a800 nid=0x5184 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \u0026#34;C2 CompilerThread2\u0026#34; #8 daemon prio=9 os_prio=2 tid=0x00000187da841800 nid=0x42f0 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \u0026#34;C2 CompilerThread1\u0026#34; #7 daemon prio=9 os_prio=2 tid=0x00000187da840800 nid=0x5c34 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \u0026#34;C2 CompilerThread0\u0026#34; #6 daemon prio=9 os_prio=2 tid=0x00000187da844800 nid=0x4184 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \u0026#34;Attach Listener\u0026#34; #5 daemon prio=5 os_prio=2 tid=0x00000187da838000 nid=0x2638 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE \u0026#34;Signal Dispatcher\u0026#34; #4 daemon prio=9 os_prio=2 tid=0x00000187da7e5000 nid=0x5218 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE \u0026#34;Finalizer\u0026#34; #3 daemon prio=8 os_prio=1 tid=0x00000187da7b0800 nid=0x7a3c in Object.wait() [0x0000005d8b9ff000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on \u0026lt;0x00000006c3630078\u0026gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144) - locked \u0026lt;0x00000006c3630078\u0026gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216) \u0026#34;Reference Handler\u0026#34; #2 daemon prio=10 os_prio=2 tid=0x00000187da7a8800 nid=0x685c in Object.wait() [0x0000005d8b8ff000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on \u0026lt;0x00000006c36099b0\u0026gt; (a java.lang.ref.Reference$Lock) at java.lang.Object.wait(Object.java:502) at java.lang.ref.Reference.tryHandlePending(Reference.java:191) - locked \u0026lt;0x00000006c36099b0\u0026gt; (a java.lang.ref.Reference$Lock) at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153) \u0026#34;VM Thread\u0026#34; os_prio=2 tid=0x00000187da77d800 nid=0x2fd4 runnable \u0026#34;GC task thread#0 (ParallelGC)\u0026#34; os_prio=0 tid=0x00000187bf85c800 nid=0x715c runnable \u0026#34;GC task thread#1 (ParallelGC)\u0026#34; os_prio=0 tid=0x00000187bf85e000 nid=0x558c runnable \u0026#34;GC task thread#2 (ParallelGC)\u0026#34; os_prio=0 tid=0x00000187bf85f800 nid=0x2b90 runnable \u0026#34;GC task thread#3 (ParallelGC)\u0026#34; os_prio=0 tid=0x00000187bf861800 nid=0x83f4 runnable \u0026#34;GC task thread#4 (ParallelGC)\u0026#34; os_prio=0 tid=0x00000187bf863800 nid=0x3198 runnable \u0026#34;GC task thread#5 (ParallelGC)\u0026#34; os_prio=0 tid=0x00000187bf866000 nid=0x413c runnable \u0026#34;GC task thread#6 (ParallelGC)\u0026#34; os_prio=0 tid=0x00000187bf869000 nid=0x607c runnable \u0026#34;GC task thread#7 (ParallelGC)\u0026#34; os_prio=0 tid=0x00000187bf86b800 nid=0x49cc runnable \u0026#34;VM Periodic Task Thread\u0026#34; os_prio=2 tid=0x00000187dc631000 nid=0x5018 waiting on condition JNI global references: 1315 jmap 打印进程、核心文件或远程调试服务器的共享对象内存映射或堆内存详细信息。 jmap 命令打印指定进程、核心文件或远程调试服务器的共享对象内存映射或堆内存详细信息。如果指定的进程在 64 位 Java 虚拟机 (JVM) 上运行，那么您可能需要指定 -J-d64 选项，例如：jmap -J-d64 -heap pid。\nOracle - Java Documentation - jmap\n格式 jmap [ options ] pid jmap [ options ] executable core jmap [ options ] [ pid ] server-id@ ] remote-hostname-or-IP    关键字 描述     options 命令行参数   pid 要为其打印内存映射的进程 ID。该进程必须是 Java 进程。   executable 从中生成核心转储的 Java 可执行文件。   core 要为其打印内存映射的核心文件。   remote-hostname-or-IP 远程调试服务器主机名或 IP 地址。   server-id 当多个调试服务器在同一远程主机上运行时使用的可选唯一 ID。    参数    参数名称 描述     \u0026lt;no option\u0026gt; 当不使用任何选项时，jmap 命令打印共享对象映射。对于目标JVM 中加载的每个共享对象，都会打印出该共享对象文件的起始地址、映射大小和完整路径。。   -dump:[live,] format=b, file=filename 将 hprof 二进制格式的 Java 堆转储到指定文件。 live 是可选子选项，当指定时，只会转储堆中的活动对象，且会触发一次 Full GC。要浏览堆转储，您可以使用 jhat 命令读取生成的 Dump 文件。   -finalizerinfo 打印有关等待完成的对象的信息。   -heap 打印使用的垃圾收集的堆摘要、头配置和按代计算的堆使用情况。此外，还打印了已存入常量池字符串(Interned String)的数量和大小。   -histo[:live] 打印堆的直方图。对于每个 Java 类，将打印对象数、内存大小（以Byte 为单位）和完整的类名。JVM 内部类以星号 (*) 前缀打印。如果指定了 live 子选项，则只计算活动对象，且会触发 Full GC。   -clstats 打印 Java 堆的类加载器的统计信息。对于每个类加载器，它的名称、它的活跃程度、地址、父类加载器以及它已加载的类的数量和大小都被打印出来。   -F 强制执行。当 pid 没有响应时，将此选项与 jmap -dump 或 jmap -histo 选项一起使用。此模式不支持 live 子选项。   -h Prints a help message.   -help Prints a help message.   -Jflag 将 JVM参数传递给运行 jmap 命令的 Java 虚拟机。    使用 $ jmap 30444 Attaching to process ID 30444, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.282-b08 0x0000000067ec0000 8616K C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\jre\\bin\\server\\jvm.dll 0x0000000068c90000 664K C:\\windows\\System32\\SYSFER.DLL 0x00007ff7f0680000 232K C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\bin\\java.exe 0x00007ff88ce30000 956K C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\jre\\bin\\msvcr120.dll 0x00007ff88df40000 664K C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\jre\\bin\\msvcp120.dll 0x00007ff8bddc0000 72K C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\jre\\bin\\nio.dll 0x00007ff8bde40000 104K C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\jre\\bin\\net.dll 0x00007ff8c86d0000 88K C:\\Program Files\\AdoptOpenJDK\\jdk-8.0.282.8-hotspot\\jre\\bin\\zip.dll ... 打印堆信息 $ jmap -heap 30444 Attaching to process ID 30444, please wait... Debugger attached successfully. Server compiler detected. JVM version is 25.282-b08 using thread-local object allocation. Parallel GC with 8 thread(s) Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 4238344192 (4042.0MB) NewSize = 88604672 (84.5MB) MaxNewSize = 1412431872 (1347.0MB) OldSize = 177733632 (169.5MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB) Heap Usage: PS Young Generation Eden Space: capacity = 96468992 (92.0MB) used = 964752 (0.9200592041015625MB) free = 95504240 (91.07994079589844MB) 1.0000643522843071% used From Space: capacity = 5767168 (5.5MB) used = 0 (0.0MB) free = 5767168 (5.5MB) 0.0% used To Space: capacity = 8912896 (8.5MB) used = 0 (0.0MB) free = 8912896 (8.5MB) 0.0% used PS Old Generation capacity = 152567808 (145.5MB) used = 10801024 (10.3006591796875MB) free = 141766784 (135.1993408203125MB) 7.079490845146048% used 12198 interned Strings occupying 1055840 bytes. 打印直方图 $ jmap -histo 30444 num #instances #bytes class name ---------------------------------------------- 1: 174537 32059552 [C 2: 18029 10376192 [B 3: 12024 5422296 [I 4: 120292 2887008 java.lang.String 5: 23155 1576096 [Ljava.lang.Object; 6: 15764 1008896 java.net.URL 7: 9748 857824 java.lang.reflect.Method 8: 34905 721848 [Ljava.lang.Class; 9: 5977 660736 java.lang.Class 10: 19036 609152 java.util.concurrent.ConcurrentHashMap$Node 11: 6649 532944 [Ljava.util.WeakHashMap$Entry; 12: 6217 490744 [S 13: 6596 422144 org.springframework.boot.loader.jar.JarFileWrapper 14: 10008 400320 java.lang.ref.Finalizer 15: 12209 390688 org.springframework.boot.loader.jar.StringSequence 16: 4053 361672 [Ljava.util.HashMap$Node; 17: 8640 345600 java.util.LinkedHashMap$Entry 18: 10670 341440 java.util.ArrayList$Itr 19: 10071 322272 java.util.concurrent.locks.AbstractQueuedSynchronizer$Node 20: 6645 318960 java.util.WeakHashMap 21: 9621 307872 java.util.HashMap$Node 22: 12209 293016 org.springframework.boot.loader.jar.JarURLConnection$JarEntryName 23: 4851 271656 jdk.internal.org.objectweb.asm.Item 24: 133 258512 [Ljava.util.concurrent.ConcurrentHashMap$Node; 25: 7366 235712 java.lang.ref.ReferenceQueue 26: 6632 212224 java.util.zip.ZipCoder 27: 3646 204176 java.util.LinkedHashMap 28: 2516 201280 org.springframework.boot.loader.jar.JarURLConnection 29: 3454 193424 java.util.concurrent.ConcurrentHashMap$KeyIterator 30: 11965 191440 java.lang.Object 31: 6765 162360 java.util.ArrayDeque 32: 3341 160368 java.util.zip.Inflater 33: 168 158400 [Ljdk.internal.org.objectweb.asm.Item; 34: 3588 143520 java.util.HashMap$KeyIterator 35: 3614 137896 [Ljava.lang.reflect.Method; 36: 1980 132072 [Ljava.lang.String; 37: 1763 126936 java.lang.reflect.Field 38: 1052 126240 org.springframework.boot.loader.jar.JarEntry 39: 1556 124480 java.lang.reflect.Constructor 40: 7368 117888 java.lang.ref.ReferenceQueue$Lock 41: 4855 116520 java.lang.StringBuilder 42: 2278 109344 org.springframework.util.ConcurrentReferenceHashMap$SoftEntryReference 43: 2229 106992 java.util.HashMap 44: 4451 106824 java.util.ArrayList 45: 3169 101408 java.util.LinkedHashMap$LinkedKeyIterator 46: 1644 92064 java.lang.invoke.MemberName 47: 2297 91880 java.util.TreeMap$Entry 48: 1914 91872 org.springframework.core.ResolvableType 49: 3634 87216 java.util.Collections$UnmodifiableCollection$1 50: 2153 86120 java.lang.invoke.MethodType ... Total 767708 68849112 打印直方图 - 存活对象 $ jmap -histo:live 30444 num #instances #bytes class name ---------------------------------------------- 1: 29265 2933672 [C 2: 28974 695376 java.lang.String 3: 5977 660736 java.lang.Class 4: 17063 546016 java.util.concurrent.ConcurrentHashMap$Node 5: 6121 538648 java.lang.reflect.Method 6: 7804 477144 [Ljava.lang.Object; 7: 3686 366600 [I 8: 2560 321960 [B 9: 6117 244680 java.util.LinkedHashMap$Entry 10: 2719 235344 [Ljava.util.HashMap$Node; 11: 2605 209424 [Ljava.util.WeakHashMap$Entry; 12: 6069 194208 java.util.HashMap$Node 13: 11619 185904 java.lang.Object 14: 101 181904 [Ljava.util.concurrent.ConcurrentHashMap$Node; 15: 7568 170520 [Ljava.lang.Class; 16: 2553 163392 org.springframework.boot.loader.jar.JarFileWrapper 17: 2759 154504 java.util.LinkedHashMap 18: 3596 143840 java.lang.ref.Finalizer 19: 2602 124896 java.util.WeakHashMap 20: 3131 100192 java.lang.ref.ReferenceQueue 21: 2589 82848 java.util.zip.ZipCoder 22: 1002 80160 java.lang.reflect.Constructor 23: 1058 76176 java.lang.reflect.Field 24: 1308 73248 java.lang.invoke.MemberName 25: 2589 62136 java.util.ArrayDeque 26: 1261 60528 java.util.HashMap 27: 482 57840 org.springframework.boot.loader.jar.JarEntry 28: 941 52696 java.lang.Class$ReflectionData 29: 1254 50160 java.lang.ref.SoftReference 30: 3133 50128 java.lang.ref.ReferenceQueue$Lock 31: 984 47232 java.util.zip.Inflater 32: 937 46024 [Ljava.lang.String; 33: 792 45120 [Ljava.lang.reflect.Method; 34: 1090 43600 java.lang.invoke.MethodType 35: 939 37560 java.util.TreeMap$Entry 36: 1152 36864 java.lang.invoke.MethodType$ConcurrentWeakInternSet$WeakEntry 37: 752 36096 org.springframework.core.ResolvableType 38: 1 32784 [Ljava.util.concurrent.ForkJoinTask; 39: 1013 32416 java.util.concurrent.locks.ReentrantLock$NonfairSync 40: 1094 26256 java.util.ArrayList 41: 806 25792 java.lang.invoke.DirectMethodHandle 42: 128 24576 org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader$ConfigurationClassBeanDefinition 43: 984 23616 java.util.zip.ZStreamRef 44: 512 20480 org.springframework.util.ConcurrentReferenceHashMap$Segment 45: 512 20088 [Lorg.springframework.util.ConcurrentReferenceHashMap$Reference; 46: 351 19656 java.lang.Package 47: 809 19416 sun.reflect.annotation.AnnotationInvocationHandler 48: 535 15720 [Ljava.lang.CharSequence; 49: 487 15584 org.springframework.boot.loader.jar.AsciiBytes 50: 475 15200 java.lang.invoke.LambdaForm$Name ... Total 215157 10801024 Dump 文件生成 $ jmap -dump:format=b,file=./dump.hprof 30444 Dumping heap to E:\\temp\\dump.hprof ... Heap dump file created Dump 文件生成 - 存活对象 $ jmap -dump:live,format=b,file=./dump_live.hprof 30444 Dumping heap to E:\\temp\\dump_live.hprof ... Heap dump file created jhat jhat 命令解析 Java 堆转储文件并启动 Web 服务器。* jhat* 命令允许您使用您喜欢的 *Web* 浏览器浏览堆转储。 *jhat* 命令支持预先设计的查询，例如显示已知类 *MyClass* 和对象查询语言 (*OQL*) 的所有实例。除了查询堆转储之外，*OQL* 与 *SQL* 类似。可以从 *jhat* 命令显示的 *OQL* 帮助页面获得有关 *OQL* 的帮助。使用默认端口，可在 http://localhost:7000/oqlhelp/ 获得 *OQL* 帮助\nOracle - Java Documentation - jhat\n参数    参数 描述     `-stack false true`   `-refs false true`   -port port-number 设置 jhat HTTP 服务器的端口。默认值为 7000。   -exclude exclude-file 指定一个文件，该文件列出应从可达对象查询中排除的数据成员。例如，如果文件列出了 java.lang.String.value ，那么无论何时计算从特定对象 o 可达的对象列表，都不会考虑涉及 java.lang.String.value 字段的引用路径。   -baseline exclude-file 指定基线堆转储。两个堆转储中具有相同对象 ID 的对象都被标记为不是新对象。其他对象被标记为新对象。这对于比较两个不同的堆转储很有用。   -debug int 设置此工具的调试级别。级别 0 表示没有调试输出。为更详细的模式设置更高的值。   -version 输出版本号并退出   -h 显示帮助消息并退出。   -help 显示帮助消息并退出。   -Jflag 将 JVM 参数传递给运行 jhat 命令的 Java 虚拟机。例如，-J-Xmx512m 使用 512 MB 的最大堆大小。对于分析 Dump 大小较大的文件，需要将 JVM 调至 6GB 或更高。    使用 该命令将分析 dump_live.hprof 堆转储文件，并启动 Web 服务，发布至本地 8081 端口。\n$ jhat -port 8081 dump_live.hprof Reading from dump_live.hprof... Dump file created Thu Aug 19 16:47:50 CST 2021 Snapshot read, resolving... Resolving 188752 objects... Chasing references, expect 37 dots..................................... Eliminating duplicate references..................................... Snapshot resolved. Started HTTP server on port 8081 Server is ready. ","date":"2021-08-19T10:47:00+08:00","image":"https://www.catfish.top/p/jvm-opt-2/jvm-white_hu06bdc8659dce216bb36458f3911d03ef_23035_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.catfish.top/p/jvm-opt-2/","title":"JVM调优（二）"},{"content":"G1 GC，全称 Garbage-First Garbage Collector，通过 -XX:+UseG1GC参数来启用。 它是专门针对以下应用场景设计的:\n 像 CMS 收集器一样，能与应用程序线程并发执行。 整理空闲空间更快。 需要 GC 停顿时间更好预测。 不希望牺牲大量的吞吐性能。 不需要更大的 Java 堆内存。  G1 收集器的设计目标是取代 CMS 收集器，它同 CMS 相比，在以下方面表现的更出色： G1 是一个有整理内存过程的垃圾收集器，不会产生很多内存碎片。 G1 的 Stop The World(STW) 更可控，G1 在停顿时间上添加了预测机制，用户可以指定期望停顿时间。\n重要概念 Region 传统的 GC 收集器将连续的内存空间划分为新生代、老年代和永久代（JDK 8 去除了永久代，引入了元空间 Metaspace ），这种划分的特点是各代的存储地址（逻辑地址，下同）是连续的。如下图所示：\n JDK 8 Memory Structure \n而 G1 的各代存储地址是不连续的，每一代都使用了 n 个不连续的大小相同的 Region，每个* Region* 占有一块连续的虚拟内存地址。如下图所示：\n JDK 8 G1 GC Memory Structure \nG1 的堆结构就是把一整块内存区域切分成多个固定大小的块，每一块被称为一个 Region 。在JVM在启动时来决定每个小块，也就是 Region 的大小。 JVM一般是把一整块堆切分成大约 2000 个 Region 。每个小 Region 从 1 到 32Mb 不等，且是 2 的指数，如果不设定，那么G1会根据Heap大小自动决定。这些 Region 最后又被分别标记为Eden , Survivor 和 Old 。这里的 Eden ， Survivor 和old已经是一个标签，也就是说只是一个逻辑表示，不是物理表示。O表示老生代（ Old ），E表示 Eden ，S表示 Survivor 。为了明了，我们分别用三种不同的颜色区分。存活下来的对象就被虚拟机从一个 Region 里被移动到另一个中。这些小块 Region 的回收是并行回收的，期间其他的应用线程照常工作。和以往的回收器一样， G1 中也有 Eden , Survivor , Old 。在这三个之外，还增加了第四种类型，叫 Humongous 。这个单词翻译过来就是“堆积如山”的意思。这个类型主要是用来存储那些比标准块大50%，甚至更大的那些对象。这些大对象被保存到一整块连续的区域。这个堆积如山区就是堆里没有被使用的区域。记住一点： G1 不是像老一辈的那些垃圾回收器一样要求每一代的块是连续的，在 G1 中可以不是连续的。\nSTAB Snapshot-At-The-Beginning ，由字面理解，是 GC 开始时活着的对象的一个快照。它是通过Root Tracing 得到的，作用是维持并发 GC 的正确性。 根据三色标记算法，我们知道对象存在三种状态：\n 白：对象没有被标记到，标记阶段结束后，会被当做垃圾回收掉。 灰：对象被标记了，但是它的field还没有被标记或标记完。 黑：对象被标记了，且它的所有field也被标记完了。  由于并发阶段的存在，Mutator 和 Garbage Collector 线程同时对对象进行修改，就会出现白对象漏标的情况，这种情况发生的前提是：\n Mutator赋予一个黑对象该白对象的引用。 Mutator删除了所有从灰对象到该白对象的直接或者间接引用。  对于第一个条件，在并发标记阶段，如果该白对象是 new 出来的，并没有被灰对象持有，那么它会不会被漏标呢？Region中有两个 top-at-mark-start(TAMS) 指针，分别为 prevTAMS 和 nextTAMS。在 TAMS 以上的对象是新分配的，这是一种隐式的标记。对于在 GC 时已经存在的白对象，如果它是活着的，它必然会被另一个对象引用，即条件二中的灰对象。如果灰对象到白对象的直接引用或者间接引用被替换了，或者删除了，白对象就会被漏标，从而导致被回收掉，这是非常严重的错误，所以SATB 破坏了第二个条件。也就是说，一个对象的引用被替换时，可以通过 Write Barrier 将旧引用记录下来。\nSATB 也是有副作用的，如果被替换的白对象就是要被收集的垃圾，这次的标记会让它躲过GC，这就是 float garbage。因为 SATB的做法精度比较低，所以造成的 Float Garbage 也会比较多。\nRSet - Remembered Set 每个 Region 初始化时，会初始化一个 Remembered Set ，简称 RSet，该集合用来记录并跟踪其它 Region 指向该 Region 中对象的引用，每个 Region 默认按照 512Kb 划分成多个 Card ，所以 RSet 需要记录的东西应该是 Region-n 的 Card-n。\n JDK 8 G1 GC Remembered Set \nG1 GC 将一组或多组区域（称为回收集 (CSet)）中的存活对象以增量、并行的方式复制到不同的新区域来实现压缩，从而减少堆碎片。目标是从可回收空间最多的区域开始，尽可能回收更多的堆空间，同时尽可能不超出暂停时间目标（垃圾优先）。\nG1 GC 使用独立的记忆集 (RSet) 跟踪对区域的引用。独立的 RSet 可以并行、独立地回收区域，因为只需要对区域（而不是整个堆）的 RSet 进行区域引用扫描。G1 GC 使用后 Write Barrier 记录堆的更改和更新 RSet。\nPause Prediction Model Pause Prediction Model 即停顿预测模型，G1 使用暂停预测模型来满足用户定义的暂停时间目标，并根据指定的暂停时间目标选择要收集的区域数量。\nG1 GC 是一个响应时间优先的GC 算法，它与 CMS 最大的不同是，用户可以设定整个 GC 过程的期望停顿时间，参数 -XX:MaxGCPauseMillis 指定一个 G1 收集过程目标停顿时间，默认值 200ms，不过它不是硬性条件，只是期望值。那么 G1 怎么满足用户的期望呢？就需要这个停顿预测模型了。G1 根据这个模型统计计算出来的历史数据来预测本次收集需要选择的 Region 数量，从而尽量满足用户设定的目标停顿时间。 停顿预测模型是以衰减标准偏差为理论基础实现的。\nGC 过程 GC 步骤 年轻代 GC G1中的年轻代 堆内存将被分割为大约 2000 个 Region。最小为 1Mb，最大为 32Mb，一般为 1Mb ， 2Mb ， 4Mb ， 8Mb ， 16Mb ， 32Mb ，为 2 的倍数。\n Young Generation in G1 \nG1 中的 Young GC 存活对象被疏散（即，复制或移动）到一个或多个幸存区域。 如果满足老年化阈值，则将一些对象提升到老年代区域。\n A Young GC in G1 \n这是一个 STW 的暂停。 为下一个 Young GC 计算 Eden 大小和 Suvivor 大小。 保留暂停时间目标之类的信息来帮助计算大小。\n使用这样的方法可以很容易地调整区域的大小，根据需要弹性伸缩。\nYoung GC 结束后 存活对象被疏散到 Survivor 区域或 Old 区域。\n End of Young GC in G1 \n总结 综上所述，关于 G1 中的年轻代有如下描述：\n  堆是分割成区域的单个内存空间。\n  年轻代内存由一组不连续的区域组成。 能够在需要时调整大小。\n  年轻代垃圾回收，称为 Young GC，是 STW 事件，需要停止所有应用程序线程来进行操作。\n  Young GC 使用多个线程并行完成。\n  存活对象被复制到新的 Survivor 或 Old 区域。\n  老年代 GC 与 CMS GC 一样，G1 GC 被设计为老年代对象的低暂停收集器。 下表描述了老年代的 G1 收集阶段。\n   阶段 STW 描述     初始标记 STW 对于 G1，它将在 Young GC 中运行。标记可能引用老年代对象的幸存者区域（根区域）。   根区域扫描  在 Survivor 区中扫描引用到老年代的引用。本阶段需要确保在 Young GC 发生前完成。   并发标记  在整个堆中寻找存活对象，此阶段可能会被 Young GC 打断。   再次标记 STW 使用 SATB 算法完成对堆中存对象的标记，该算法比 CMS 中的算法更快。   清理 STW 1. 记录存活对象和完全空白的区域。（STW）; 2. 清理 RSet。（STW）; 3. 重置空区域并将它们返回到空闲列表。 （并发）   复制 STW 这些是以 STW 的形式将存活对象疏散或复制到新的未使用区域。 [GC pause (young)]为完成 Young GC 的记录。或者是以[GC Pause (mixed)]为记录的年轻代和年老代区域。    初始标记 存活对象的初始标记将在 Young GC 中进行。在日志中，这被标记为  GC pause (young)(inital-mark)。\n Initial Marking Phase \n并发标记 如果发现空 Region（如“X”所示），则在 再次标记 阶段立即将其删除。此外，也将记录计算的确定活跃度信息。\n Concurrent Marking Phase \n再次标记 删除并回收空 Region，并计算所有 Region 的区域活跃度。\n Remark Phase \n复制/清理 G1 选择 “活跃度” 最低的 Region ，那些可以最快完成收集的 Region 。然后这些 Region 在 Young GC 进行的同时被收集。在日志中表示为 [GC pause (mixed)]。所以年轻代和年老代将被同时收集。\n Copying/Cleanup Phase \n复制/清理 后 选定的区域已被收集并压缩为图中所示的深蓝色区域和深绿色区域。\n After Copying/Cleanup Phase \n总结 这里有几个老年代 GC 的关键点:\n 并发标记阶段  存活度信息是在程序运行期间并发计算的 存活度标明了在疏散暂停的时期哪些 Region 更适合被回收。 没有像 CMS 一样的清扫阶段   再次标记阶段  使用了比 CMS 快很多的 Snapshot-at-the-Beginning (SATB) 算法 完全空白的 Region 将会被回收   复制/清理阶段  年轻代和老年代将会在同一时间被回收 基于存活度选择需要回收的老年代 Region    GC 模式 Young GC 选定所有年轻代里的 Region 。通过控制年轻代的 Region 个数，即年轻代内存大小，来控制Young GC 的时间开销。\nMixed GC 选定所有年轻代里的 Region ，外加根据 Global Concurrent Marking 统计得出收集收益高的若干老年代 Region 。在用户指定的开销目标范围内尽可能选择收益高的老年代 Region 。\nMixed GC 不是 Full GC ，它只能回收部分老年代的 Region ，如果 Mixed GC 实在无法跟上程序分配内存的速度，导致老年代填满无法继续进行 Mixed GC ，就会使用 Serial Old GC (Full GC) 来收集整个 GC heap 。所以我们可以知道，G1 是不提供 Full GC 的。\nGC 日志 启动参数 java -jar gs-service-0.0.1-SNAPSHOT.jar -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintHeapAtGC -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -XX:+PrintReferenceGC -Xloggc:/home/logs/gc/gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=20 -XX:GCLogFileSize=5M    参数 描述     -XX:+PrintGCDetails 打印 GC 详情   -XX:+PrintGCDateStamps 打印 GC 时间戳   -XX:+PrintHeapAtGC 打印 GC 时的堆信息   -XX:+PrintTenuringDistribution 在每次 Young GC 时，打印出幸存区中对象的年龄分布   -XX:+PrintGCApplicationStoppedTime 打印 GC 时应用停顿时间   -XX:+PrintReferenceGC 记录回收了多少不同引用类型的引用   -Xloggc:/home/logs/gc/gc-%t.log 指定 GC 日志位置与文件格式   -XX:+UseGCLogFileRotation 启用滚动日志   -XX:NumberOfGCLogFiles=20 最大日志数量   -XX:GCLogFileSize=5M 单个日志最大大小    日志详情 {Heap before GC invocations=0 (full 0): garbage-first heap total 6291456K, used 313344K [0x0000000660800000, 0x0000000660a06000, 0x00000007e0800000) region size 2048K, 153 young (313344K), 0 survivors (0K) Metaspace used 32143K, capacity 32576K, committed 33024K, reserved 1077248K class space used 4199K, capacity 4318K, committed 4352K, reserved 1048576K 2021-08-04T17:09:15.956+0900: 4.167: [GC pause (G1 Evacuation Pause) (young), 0.0316564 secs] [Parallel Time: 15.5 ms, GC Workers: 4] [GC Worker Start (ms): Min: 4167.1, Avg: 4167.7, Max: 4168.1, Diff: 1.1] [Ext Root Scanning (ms): Min: 1.4, Avg: 2.9, Max: 4.9, Diff: 3.5, Sum: 11.5] [Update RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0] [Processed Buffers: Min: 0, Avg: 0.0, Max: 0, Diff: 0, Sum: 0] [Scan RS (ms): Min: 0.0, Avg: 0.0, Max: 0.1, Diff: 0.1, Sum: 0.2] [Code Root Scanning (ms): Min: 0.0, Avg: 0.8, Max: 2.3, Diff: 2.3, Sum: 3.4] [Object Copy (ms): Min: 9.9, Avg: 10.6, Max: 12.4, Diff: 2.5, Sum: 42.5] [Termination (ms): Min: 0.0, Avg: 0.5, Max: 0.6, Diff: 0.6, Sum: 1.9] [Termination Attempts: Min: 1, Avg: 11.2, Max: 16, Diff: 15, Sum: 45] [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.1] [GC Worker Total (ms): Min: 14.4, Avg: 14.9, Max: 15.5, Diff: 1.0, Sum: 59.5] [GC Worker End (ms): Min: 4182.5, Avg: 4182.5, Max: 4182.6, Diff: 0.0] [Code Root Fixup: 0.1 ms] [Code Root Purge: 0.0 ms] [Clear CT: 0.2 ms] [Other: 15.8 ms] [Choose CSet: 0.0 ms] [Ref Proc: 14.3 ms] [Ref Enq: 0.1 ms] [Redirty Cards: 0.1 ms] [Humongous Register: 0.2 ms] [Humongous Reclaim: 0.0 ms] [Free CSet: 0.3 ms] [Eden: 306.0M(306.0M)-\u0026gt;0.0B(278.0M) Survivors: 0.0B-\u0026gt;28672.0K Heap: 306.0M(6144.0M)-\u0026gt;26902.6K(6144.0M)] Heap after GC invocations=1 (full 0): garbage-first heap total 6291456K, used 26902K [0x0000000660800000, 0x0000000660a06000, 0x00000007e0800000) region size 2048K, 14 young (28672K), 14 survivors (28672K) Metaspace used 32143K, capacity 32576K, committed 33024K, reserved 1077248K class space used 4199K, capacity 4318K, committed 4352K, reserved 1048576K }    指标 值 解释     GC 原因 GC pause (G1 Evacuation Pause) (young) 发生 Young GC   GC 时间 0.0316564 secs    Eden 区容量变化 306.0M -\u0026gt; 278.0M    Eden 区大小变化 306.0M -\u0026gt; 0.0B    Survivor 区容量变化 0.0B -\u0026gt; 28672.0K    Heap 容量变化 6144.0M -\u0026gt; 6144.0M    Heap 大小变化 306.0M -\u0026gt; 26902.6K     参数调整 参数详解    参数 含义     -XX:+UseG1GC 启用G1 GC   `-XX:G1HeapRegionSize=n 设置Region大小，并非最终值   -XX:MaxGCPauseMillis=200 设置G1收集过程目标时间，默认值200ms，不是硬性条件   -XX:G1NewSizePercent=5 新生代最小值，默认值5%   -XX:G1MaxNewSizePercent=60 新生代最大值，默认值60%   -XX:ParallelGCThreads=n STW 期间，并行GC线程数   -XX:ConcGCThreads=n 并发标记阶段，并行执行的线程数   -XX:InitiatingHeapOccupancyPercent=45 设置触发标记周期的 Java 堆占用率阈值。默认值是 45%。这里的 Java 堆占比指的是非 Young Region 的大小占比 ，包括 Old 与 Humongous   -XX:G1MixedGCLiveThresholdPercent=65 为混合垃圾回收周期中要包括的旧区域设置占用率阈值，默认占用率为 65% 。   -XX:G1HeapWastePercent=10 设置您愿意浪费的堆百分比。如果可回收百分比小于堆废物百分比，Java HotSpot VM 不会启动混合垃圾回收周期。默认值是 10%。   -XX:G1MixedGCCountTarget=8 设置标记周期完成后，对存活数据上限为 G1MixedGCLIveThresholdPercent 的旧区域执行混合垃圾回收的目标次数。默认值是 8 次混合垃圾回收，混合回收的目标是要控制在此目标次数以内。   -XX:G1OldCSetRegionThresholdPercent=10 设置混合垃圾回收期间要回收的最大旧区域数，默认值是 Java 堆的 10%。   -XX:G1ReservePercent=10 设置作为空闲空间的预留内存百分比，以降低目标空间溢出的风险,默认值是 10%。    建议 评估和微调 G1 GC 时，请记住以下建议：\n  年轻代大小：避免使用 -Xmn 选项或 -XX:NewRatio 等其他相关选项显式设置年轻代大小。固定年轻代的大小会覆盖暂停时间目标。\n  暂停时间目标：每当对垃圾回收进行评估或调优时，都会涉及到延迟与吞吐量的权衡。G1 GC 是增量垃圾回收器，暂停统一，同时应用程序线程的开销也更多。G1 GC 的吞吐量目标是 90% 的应用程序时间和 10% 的垃圾回收时间。如果将其与 Java HotSpot VM 的吞吐量回收器相比较，目标则是 99% 的应用程序时间和 1% 的垃圾回收时间。因此，当您评估 G1 GC 的吞吐量时，暂停时间目标不要太严苛。目标太过严苛表示您愿意承受更多的垃圾回收开销，而这会直接影响到吞吐量。当您评估 G1 GC 的延迟时，请设置所需的（软）实时目标，G1 GC 会尽量满足。副作用是，吞吐量可能会受到影响。\n  掌握混合垃圾回收：当您调优混合垃圾回收时，请尝试以下选项：\n -XX:InitiatingHeapOccupancyPercent 用于更改标记阈值。 -XX:G1MixedGCLiveThresholdPercent 和 -XX:G1HeapWastePercent 当您想要更改混合垃圾回收决定时。 -XX:G1MixedGCCountTarget 和 -XX:G1OldCSetRegionThresholdPercent 当您想要调整旧区域的 CSet 时。    总结 G1 GC 是区域化、并行-并发、增量式垃圾回收器，相比其他 HotSpot 垃圾回收器，可提供更多可预测的暂停。增量的特性使 G1 GC 适用于更大的堆，在最坏的情况下仍能提供不错的响应。G1 GC 的自适应特性使 JVM 命令行只需要软实时暂停时间目标的最大值以及 Java 堆大小的最大值和最小值，即可开始工作。\n参考  Getting Started with the G1 Garbage Collector The Garbage First Garbage Collector 垃圾优先型垃圾回收器调优 Java Hotspot G1 GC的一些关键技术 一步步图解G1 徹底解剖「G1GC」実装編 Part 1: Introduction to the G1 Garbage Collector Collecting and reading G1 garbage collector logs - part 2  ","date":"2021-08-09T10:33:00+08:00","image":"https://www.catfish.top/p/jvm-gc-g1-1/jvm-black_hu477442ae6bda7bfc8bc3ac8fce34800c_43268_120x120_fill_q75_box_smart1.jpeg","permalink":"https://www.catfish.top/p/jvm-gc-g1-1/","title":"JVM 垃圾回收器 - G1 （一）"},{"content":" Kubernetes官网文档：使用Kubernetes对象\n本系列教程是在 Kubernetes初探 系列教程基础上，通过对官方文档的阅读完善各个重要知识点。\n本章节将基于官网文档简化文章理解难度。\n 理解 Kubernetes 对象 在 Kubernetes 中，Kubernetes 对象 是持久化的实体。 Kubernetes 使用这些实体去表示整个集群的状态。它们描述了如下信息：\n 哪些容器化应用在运行（以及在哪些节点上） 可以被应用使用的资源 关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略  操作 Kubernetes 对象 —— 无论是创建、修改，或者删除 —— 需要使用 Kubernetes API。 比如，当使用 kubectl 命令行接口时，CLI 会执行必要的 Kubernetes API 调用， 也可以在程序中使用 客户端库直接调用 Kubernetes API。\n对象规约（Spec）与状态（Status） 几乎每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置： 对象 spec（规约） 和 对象 status（状态） 。 对于具有 spec 的对象，你必须在创建对象时设置其内容，描述你希望对象所具有的特征： 期望状态（Desired State） 。\nstatus 描述了对象的 当前状态（Current State），它是由 Kubernetes 系统和组件 设置并更新的。在任何时刻，Kubernetes 控制平面 都一直积极地管理着对象的实际状态，以使之与期望状态相匹配。\n描述 Kubernetes 对象 创建 Kubernetes 对象时，必须提供对象的规约，用来描述该对象的期望状态， 以及关于对象的一些基本信息（例如名称）。\n这里有一个 .yaml 示例文件，展示了 Kubernetes Deployment 的必需字段和对象规约：\napiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:selector:matchLabels:app:nginxreplicas:2# tells deployment to run 2 pods matching the templatetemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.14.2ports:- containerPort:80使用类似于上面的 .yaml 文件来创建 Deployment 的一种方式是使用 kubectl 命令行接口（CLI）中的 kubectl apply 命令， 将 .yaml 文件作为参数。下面是一个示例：\nkubectl apply -f https://k8s.io/examples/application/deployment.yaml --record 输出类似如下这样：\ndeployment.apps/nginx-deployment created 必需字段 在想要创建的 Kubernetes 对象对应的 .yaml 文件中，需要配置如下的字段：\n apiVersion - 创建该对象所使用的 Kubernetes API 的版本 kind - 想要创建的对象的类别 metadata - 帮助唯一性标识对象的一些数据，包括一个 name 字符串、UID 和可选的 namespace  Kubernetes 对象管理 kubectl 命令行工具支持多种不同的方式来创建和管理 Kubernetes 对象。 本文档概述了不同的方法。 阅读 Kubectl book 来了解 kubectl 管理对象的详细信息。\n管理技巧 警告： 应该只使用一种技术来管理 Kubernetes 对象。混合和匹配技术作用在同一对象上将导致未定义行为。\n   管理技术 作用于 建议的环境 支持的写者 学习难度     指令式命令 活跃对象 开发项目 1+ 最低   指令式对象配置 单个文件 生产项目 1 中等   声明式对象配置 文件目录 生产项目 1+ 最高    指令式命令 使用指令式命令时，用户可以在集群中的活动对象上进行操作。用户将操作传给 kubectl 命令作为参数或标志。\n这是开始或者在集群中运行一次性任务的推荐方法。因为这个技术直接在活跃对象 上操作，所以它不提供以前配置的历史记录。\n例子 通过创建 Deployment 对象来运行 nginx 容器的实例：\nkubectl create deployment nginx --image nginx 比较 与对象配置相比的优点：\n 命令简单，易学且易于记忆。 命令仅需一步即可对集群进行更改。  与对象配置相比的缺点：\n 命令不与变更审查流程集成。 命令不提供与更改关联的审核跟踪。 除了实时内容外，命令不提供记录源。 命令不提供用于创建新对象的模板。  指令式对象配置 在指令式对象配置中，kubectl 命令指定操作（创建，替换等），可选标志和 至少一个文件名。指定的文件必须包含 YAML 或 JSON 格式的对象的完整定义。\n有关对象定义的详细信息，请查看 API 参考。\n例子 创建配置文件中定义的对象：\nkubectl create -f nginx.yaml 删除两个配置文件中定义的对象：\nkubectl delete -f nginx.yaml -f redis.yaml 通过覆盖活动配置来更新配置文件中定义的对象：\nkubectl replace -f nginx.yaml 比较 与指令式命令相比的优点：\n 对象配置可以存储在源控制系统中，比如 Git。 对象配置可以与流程集成，例如在推送和审计之前检查更新。 对象配置提供了用于创建新对象的模板。  与指令式命令相比的缺点：\n 对象配置需要对对象架构有基本的了解。 对象配置需要额外的步骤来编写 YAML 文件。  与声明式对象配置相比的优点：\n 指令式对象配置行为更加简单易懂。 从 Kubernetes 1.5 版本开始，指令对象配置更加成熟。  与声明式对象配置相比的缺点：\n 指令式对象配置更适合文件，而非目录。 对活动对象的更新必须反映在配置文件中，否则会在下一次替换时丢失。  声明式对象配置 使用声明式对象配置时，用户对本地存储的对象配置文件进行操作，但是用户 未定义要对该文件执行的操作。 kubectl 会自动检测每个文件的创建、更新和删除操作。 这使得配置可以在目录上工作，根据目录中配置文件对不同的对象执行不同的操作。\n 说明：\n声明式对象配置保留其他编写者所做的修改，即使这些更改并未合并到对象配置文件中。 可以通过使用 patch API 操作仅写入观察到的差异，而不是使用 replace API 操作来替换整个对象配置来实现。\n 例子 处理 configs 目录中的所有对象配置文件，创建并更新活跃对象。 可以首先使用 diff 子命令查看将要进行的更改，然后在进行应用：\nkubectl diff -f configs/ kubectl apply -f configs/ 递归处理目录：\nkubectl diff -R -f configs/ kubectl apply -R -f configs/ 比较 与指令式对象配置相比的优点：\n 对活动对象所做的更改即使未合并到配置文件中，也会被保留下来。 声明性对象配置更好地支持对目录进行操作并自动检测每个文件的操作类型（创建，修补，删除）。  与指令式对象配置相比的缺点：\n 声明式对象配置难于调试并且出现异常时结果难以理解。 使用 diff 产生的部分更新会创建复杂的合并和补丁操作。  名字空间（namespace） Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。 这些虚拟集群被称为名字空间。 在一些文档里名字空间也称为命名空间。\n使用场景 名字空间适用于存在很多跨多个团队或项目的用户的场景。对于只有几到几十个用户的集群，根本不需要创建或考虑名字空间。当需要名称空间提供的功能时，请开始使用它们。\n名字空间为名称提供了一个范围。资源的名称需要在名字空间内是唯一的，但不能跨名字空间。 名字空间不能相互嵌套，每个 Kubernetes 资源只能在一个名字空间中。\n名字空间是在多个用户之间划分集群资源的一种方法（通过资源配额）。\n不必使用多个名字空间来分隔仅仅轻微不同的资源，例如同一软件的不同版本： 应该使用标签 来区分同一名字空间中的不同资源。\n使用名字空间 名字空间的创建和删除在名字空间的管理指南文档描述。\n 说明： 避免使用前缀 kube- 创建名字空间，因为它是为 Kubernetes 系统名字空间保留的。\n 查看名字空间 你可以使用以下命令列出集群中现存的名字空间：\nkubectl get namespace NAME STATUS AGE default Active 1d kube-node-lease Active 1d kube-system Active 1d kube-public Active 1d Kubernetes 会创建四个初始名字空间：\n default 没有指明使用其它名字空间的对象所使用的默认名字空间 kube-system Kubernetes 系统创建对象所使用的名字空间 kube-public 这个名字空间是自动创建的，所有用户（包括未经过身份验证的用户）都可以读取它。 这个名字空间主要用于集群使用，以防某些资源在整个集群中应该是可见和可读的。 这个名字空间的公共方面只是一种约定，而不是要求。 kube-node-lease 此名字空间用于与各个节点相关的租期（Lease）对象； 此对象的设计使得集群规模很大时节点心跳检测性能得到提升。  kubectl 名字空间参数 要为当前请求设置名字空间，请使用 --namespace 参数。\n例如：\nkubectl run nginx --image=nginx --namespace=\u0026lt;名字空间名称\u0026gt; kubectl get pods --namespace=\u0026lt;名字空间名称\u0026gt; 设置默认名字空间 你可以永久保存名字空间，以用于对应上下文中所有后续 kubectl 命令。\nkubectl config set-context --current --namespace=\u0026lt;名字空间名称\u0026gt; # 验证之 kubectl config view | grep namespace: 并非所有对象都在名字空间中 大多数 kubernetes 资源（例如 Pod、Service、副本控制器等）都位于某些名字空间中。 但是名字空间资源本身并不在名字空间中。而且底层资源，例如 节点 和持久化卷不属于任何名字空间。\n查看哪些 Kubernetes 资源在名字空间中，哪些不在名字空间中：\n# 位于名字空间中的资源 kubectl api-resources --namespaced=true # 不在名字空间中的资源 kubectl api-resources --namespaced=false 自动打标签 Kubernetes 控制面会为所有名字空间设置一个不可变更的 标签 kubernetes.io/metadata.name，只要 NamespaceDefaultLabelName 这一 特性门控 被启用。标签的值是名字空间的名称。\n各类对象 工作负载资源 Pods Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。\nPod （就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个） 容器； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器， 这些容器是相对紧密的耦合在一起的。 在非云环境中，在相同的物理机或虚拟机上运行的应用类似于 在同一逻辑主机上运行的云应用。\n什么是 Pod？  说明： 除了 Docker 之外，Kubernetes 支持 很多其他容器运行时， Docker 是最有名的运行时， 使用 Docker 的术语来描述 Pod 会很有帮助。\n Pod 的共享上下文包括一组 Linux 名字空间、控制组（cgroup）和可能一些其他的隔离 方面，即用来隔离 Docker 容器的技术。 在 Pod 的上下文中，每个独立的应用可能会进一步实施隔离。\n就 Docker 概念的术语而言，Pod 类似于共享名字空间和文件系统卷的一组 Docker 容器。\n使用 Pod 通常你不需要直接创建 Pod，甚至单实例 Pod。 相反，你会使用诸如 Deployment 或 Job 这类工作负载资源 来创建 Pod。如果 Pod 需要跟踪状态， 可以考虑 StatefulSet 资源。\nKubernetes 集群中的 Pod 主要有两种用法：\n  运行单个容器的 Pod。\u0026ldquo;每个 Pod 一个容器\u0026quot;模型是最常见的 Kubernetes 用例； 在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。\n  运行多个协同工作的容器的 Pod。 Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。 这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众， 而另一个单独的“边车”（sidecar）容器则刷新或更新这些文件。 Pod 将这些容器和存储资源打包为一个可管理的实体。\n 说明： 将多个并置、同管的容器组织到一个 Pod 中是一种相对高级的使用场景。 只有在一些场景中，容器之间紧密关联时你才应该使用这种模式。\n   每个 Pod 都旨在运行给定应用程序的单个实例。如果希望横向扩展应用程序（例如，运行多个实例 以提供更多的资源），则应该使用多个 Pod，每个实例使用一个 Pod。 在 Kubernetes 中，这通常被称为 副本（Replication）。 通常使用一种工作负载资源及其控制器 来创建和管理一组 Pod 副本。\n参见 Pod 和控制器以了解 Kubernetes 如何使用工作负载资源及其控制器以实现应用的扩缩和自动修复。\nDeployments 一个 Deployment 为 Pods 和 ReplicaSets 提供声明式的更新能力。\n你负责描述 Deployment 中的 目标状态，而 Deployment 控制器（Controller） 以受控速率更改实际状态， 使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment， 并通过新的 Deployment 收养其资源。\n 说明： 不要管理 Deployment 所拥有的 ReplicaSet 。 如果存在下面未覆盖的使用场景，请考虑在 Kubernetes 仓库中提出 Issue。\n 用例 以下是 Deployments 的典型用例：\n 创建 Deployment 以将 ReplicaSet 上线。 ReplicaSet 在后台创建 Pods。 检查 ReplicaSet 的上线状态，查看其是否成功。 通过更新 Deployment 的 PodTemplateSpec，声明 Pod 的新状态 。 新的 ReplicaSet 会被创建，Deployment 以受控速率将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet。 每个新的 ReplicaSet 都会更新 Deployment 的修订版本。 如果 Deployment 的当前状态不稳定，回滚到较早的 Deployment 版本。 每次回滚都会更新 Deployment 的修订版本。 扩大 Deployment 规模以承担更多负载。 暂停 Deployment 以应用对 PodTemplateSpec 所作的多项修改， 然后恢复其执行以启动新的上线版本。 使用 Deployment 状态 来判定上线过程是否出现停滞。 清理较旧的不再需要的 ReplicaSet 。  创建 Deployment 下面是 Deployment 示例。其中创建了一个 ReplicaSet，负责启动三个 nginx Pods：\ncontrollers/nginx-deployment.yaml apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentlabels:app:nginxspec:replicas:3selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.14.2ports:- containerPort:80在该例中：\n  创建名为 nginx-deployment（由 .metadata.name 字段标明）的 Deployment。\n  该 Deployment 创建三个（由 replicas 字段标明）Pod 副本。\n  selector 字段定义 Deployment 如何查找要管理的 Pods。 在这里，你选择在 Pod 模板中定义的标签（app: nginx）。 不过，更复杂的选择规则是也可能的，只要 Pod 模板本身满足所给规则即可。\n 说明：\nspec.selector.matchLabels 字段是 {key,value} 键值对映射。 在 matchLabels 映射中的每个 {key,value} 映射等效于 matchExpressions 中的一个元素， 即其 key 字段是 “key”，operator 为 “In”，values 数组仅包含 “value”。 在 matchLabels 和 matchExpressions 中给出的所有条件都必须满足才能匹配。\n   template 字段包含以下子字段：\n Pod 被使用 labels 字段打上 app: nginx 标签。 Pod 模板规约（即 .template.spec 字段）指示 Pods 运行一个 nginx 容器， 该容器运行版本为 1.14.2 的 nginx Docker Hub镜像。 创建一个容器并使用 name 字段将其命名为 nginx。    开始之前，请确保的 Kubernetes 集群已启动并运行。 按照以下步骤创建上述 Deployment ：\n  通过运行以下命令创建 Deployment ：\nkubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml  说明： 你可以设置 --record 标志将所执行的命令写入资源注解 kubernetes.io/change-cause 中。 这对于以后的检查是有用的。例如，要查看针对每个 Deployment 修订版本所执行过的命令。\n   运行 kubectl get deployments 检查 Deployment 是否已创建。如果仍在创建 Deployment， 则输出类似于：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s 在检查集群中的 Deployment 时，所显示的字段有：\n NAME 列出了集群中 Deployment 的名称。 READY 显示应用程序的可用的 副本 数。显示的模式是“就绪个数/期望个数”。 UP-TO-DATE 显示为了达到期望状态已经更新的副本数。 AVAILABLE 显示应用可供用户使用的副本数。 AGE 显示应用程序运行的时间。  请注意期望副本数是根据 .spec.replicas 字段设置 3。\n  要查看 Deployment 上线状态，运行 kubectl rollout status deployment/nginx-deployment。\n输出类似于：\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment \u0026quot;nginx-deployment\u0026quot; successfully rolled out   几秒钟后再次运行 kubectl get deployments。输出类似于：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 18s 注意 Deployment 已创建全部三个副本，并且所有副本都是最新的（它们包含最新的 Pod 模板） 并且可用。\n  要查看 Deployment 创建的 ReplicaSet（rs），运行 kubectl get rs。 输出类似于：\nNAME DESIRED CURRENT READY AGE nginx-deployment-75675f5897 3 3 3 18s ReplicaSet 输出中包含以下字段：\n NAME 列出名字空间中 ReplicaSet 的名称； DESIRED 显示应用的期望副本个数，即在创建 Deployment 时所定义的值。 此为期望状态； CURRENT 显示当前运行状态中的副本个数； READY 显示应用中有多少副本可以为用户提供服务； AGE 显示应用已经运行的时间长度。  注意 ReplicaSet 的名称始终被格式化为[Deployment名称]-[随机字符串]。 其中的随机字符串是使用 pod-template-hash 作为种子随机生成的。\n  要查看每个 Pod 自动生成的标签，运行 kubectl get pods --show-labels。返回以下输出：\nNAME READY STATUS RESTARTS AGE LABELS nginx-deployment-75675f5897-7ci7o 1/1 Running 0 18s app=nginx,pod-template-hash=3123191453 nginx-deployment-75675f5897-kzszj 1/1 Running 0 18s app=nginx,pod-template-hash=3123191453 nginx-deployment-75675f5897-qqcnn 1/1 Running 0 18s app=nginx,pod-template-hash=3123191453 所创建的 ReplicaSet 确保总是存在三个 nginx Pod。\n   说明： 你必须在 Deployment 中指定适当的选择算符和 Pod 模板标签（在本例中为 app: nginx）。 标签或者选择算符不要与其他控制器（包括其他 Deployment 和 StatefulSet）重叠。 Kubernetes 不会阻止你这样做，但是如果多个控制器具有重叠的选择算符，它们可能会发生冲突 执行难以预料的操作。\n Pod-template-hash 标签  说明： 不要更改此标签。\n Deployment 控制器将 pod-template-hash 标签添加到 Deployment 所创建或收留的 每个 ReplicaSet 。\n此标签可确保 Deployment 的子 ReplicaSets 不重叠。 标签是通过对 ReplicaSet 的 PodTemplate 进行哈希处理。 所生成的哈希值被添加到 ReplicaSet 选择算符、Pod 模板标签，并存在于在 ReplicaSet 可能拥有的任何现有 Pod 中。\n更新 Deployment  说明： 仅当 Deployment Pod 模板（即 .spec.template）发生改变时，例如模板的标签或容器镜像被更新， 才会触发 Deployment 上线。 其他更新（如对 Deployment 执行扩缩容的操作）不会触发上线动作。\n 按照以下步骤更新 Deployment：\n  先来更新 nginx Pod 以使用 nginx:1.16.1 镜像，而不是 nginx:1.14.2 镜像。\nkubectl --record deployment.apps/nginx-deployment set image \\  deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 或者使用下面的命令：\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 --record 输出类似于：\ndeployment.apps/nginx-deployment image updated 或者，可以 edit Deployment 并将 .spec.template.spec.containers[0].image 从 nginx:1.14.2 更改至 nginx:1.16.1。\nkubectl edit deployment.v1.apps/nginx-deployment 输出类似于：\ndeployment.apps/nginx-deployment edited   要查看上线状态，运行：\nkubectl rollout status deployment/nginx-deployment 输出类似于：\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated... 或者\ndeployment \u0026quot;nginx-deployment\u0026quot; successfully rolled out   获取关于已更新的 Deployment 的更多信息：\n  在上线成功后，可以通过运行 kubectl get deployments 来查看 Deployment： 输出类似于：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 36s   运行 kubectl get rs 以查看 Deployment 通过创建新的 ReplicaSet 并将其扩容到 3 个副本并将旧 ReplicaSet 缩容到 0 个副本完成了 Pod 的更新操作：\nkubectl get rs 输出类似于：\nNAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 6s nginx-deployment-2035384211 0 0 0 36s   现在运行 get pods 应仅显示新的 Pods:\nkubectl get pods 输出类似于：\nNAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-khku8 1/1 Running 0 14s nginx-deployment-1564180365-nacti 1/1 Running 0 14s nginx-deployment-1564180365-z9gth 1/1 Running 0 14s 下次要更新这些 Pods 时，只需再次更新 Deployment Pod 模板即可。\nDeployment 可确保在更新时仅关闭一定数量的 Pod。默认情况下，它确保至少所需 Pods 75% 处于运行状态（最大不可用比例为 25%）。\nDeployment 还确保仅所创建 Pod 数量只可能比期望 Pods 数高一点点。 默认情况下，它可确保启动的 Pod 个数比期望个数最多多出 25%（最大峰值 25%）。\n例如，如果仔细查看上述 Deployment ，将看到它首先创建了一个新的 Pod，然后删除了一些旧的 Pods， 并创建了新的 Pods。它不会杀死老 Pods，直到有足够的数量新的 Pods 已经出现。 在足够数量的旧 Pods 被杀死前并没有创建新 Pods。它确保至少 2 个 Pod 可用，同时 最多总共 4 个 Pod 可用。\n  获取 Deployment 的更多信息\nkubectl describe deployments 输出类似于：\nName: nginx-deployment Namespace: default CreationTimestamp: Thu, 30 Nov 2017 10:56:25 +0000 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=2 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-deployment-1564180365 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 2m deployment-controller Scaled up replica set nginx-deployment-2035384211 to 3 Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 1 Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 0 可以看到，当第一次创建 Deployment 时，它创建了一个 ReplicaSet（nginx-deployment-2035384211） 并将其直接扩容至 3 个副本。更新 Deployment 时，它创建了一个新的 ReplicaSet （nginx-deployment-1564180365），并将其扩容为 1，然后将旧 ReplicaSet 缩容到 2， 以便至少有 2 个 Pod 可用且最多创建 4 个 Pod。 然后，它使用相同的滚动更新策略继续对新的 ReplicaSet 扩容并对旧的 ReplicaSet 缩容。 最后，你将有 3 个可用的副本在新的 ReplicaSet 中，旧 ReplicaSet 将缩容到 0。\n  翻转（多 Deployment 动态更新） Deployment 控制器每次注意到新的 Deployment 时，都会创建一个 ReplicaSet 以启动所需的 Pods。 如果更新了 Deployment，则控制标签匹配 .spec.selector 但模板不匹配 .spec.template 的 Pods 的现有 ReplicaSet 被缩容。最终，新的 ReplicaSet 缩放为 .spec.replicas 个副本， 所有旧 ReplicaSets 缩放为 0 个副本。\n当 Deployment 正在上线时被更新，Deployment 会针对更新创建一个新的 ReplicaSet 并开始对其扩容，之前正在被扩容的 ReplicaSet 会被翻转，添加到旧 ReplicaSets 列表 并开始缩容。\n例如，假定你在创建一个 Deployment 以生成 nginx:1.14.2 的 5 个副本，但接下来 更新 Deployment 以创建 5 个 nginx:1.16.1 的副本，而此时只有 3 个nginx:1.14.2 副本已创建。在这种情况下，Deployment 会立即开始杀死 3 个 nginx:1.14.2 Pods， 并开始创建 nginx:1.16.1 Pods。它不会等待 nginx:1.14.2 的 5 个副本都创建完成 后才开始执行变更动作。\n更改标签选择算符 通常不鼓励更新标签选择算符。建议你提前规划选择算符。 在任何情况下，如果需要更新标签选择算符，请格外小心，并确保自己了解 这背后可能发生的所有事情。\n 说明： 在 API 版本 apps/v1 中，Deployment 标签选择算符在创建后是不可变的。\n  添加选择算符时要求使用新标签更新 Deployment 规约中的 Pod 模板标签，否则将返回验证错误。 此更改是非重叠的，也就是说新的选择算符不会选择使用旧选择算符所创建的 ReplicaSet 和 Pod， 这会导致创建新的 ReplicaSet 时所有旧 ReplicaSet 都会被孤立。 选择算符的更新如果更改了某个算符的键名，这会导致与添加算符时相同的行为。 删除选择算符的操作会删除从 Deployment 选择算符中删除现有算符。 此操作不需要更改 Pod 模板标签。现有 ReplicaSet 不会被孤立，也不会因此创建新的 ReplicaSet， 但请注意已删除的标签仍然存在于现有的 Pod 和 ReplicaSet 中。  回滚 Deployment 有时，你可能想要回滚 Deployment；例如，当 Deployment 不稳定时（例如进入反复崩溃状态）。 默认情况下，Deployment 的所有上线记录都保留在系统中，以便可以随时回滚 （你可以通过修改修订历史记录限制来更改这一约束）。\n 说明： Deployment 被触发上线时，系统就会创建 Deployment 的新的修订版本。 这意味着仅当 Deployment 的 Pod 模板（.spec.template）发生更改时，才会创建新修订版本 \u0026ndash; 例如，模板的标签或容器镜像发生变化。 其他更新，如 Deployment 的扩缩容操作不会创建 Deployment 修订版本。 这是为了方便同时执行手动缩放或自动缩放。 换言之，当你回滚到较早的修订版本时，只有 Deployment 的 Pod 模板部分会被回滚。\n   假设你在更新 Deployment 时犯了一个拼写错误，将镜像名称命名设置为 nginx:1.161 而不是 nginx:1.16.1：\nkubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.161 --record=true 输出类似于：\ndeployment.apps/nginx-deployment image updated   此上线进程会出现停滞。你可以通过检查上线状态来验证：\nkubectl rollout status deployment/nginx-deployment 输出类似于：\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...   按 Ctrl-C 停止上述上线状态观测。有关上线停滞的详细信息，参考这里。\n  你可以看到旧的副本有两个（nginx-deployment-1564180365 和 nginx-deployment-2035384211）， 新的副本有 1 个（nginx-deployment-3066724191）：\nkubectl get rs 输出类似于：\nNAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 25s nginx-deployment-2035384211 0 0 0 36s nginx-deployment-3066724191 1 1 0 6s   查看所创建的 Pod，你会注意到新 ReplicaSet 所创建的 1 个 Pod 卡顿在镜像拉取循环中。\nkubectl get pods 输出类似于：\nNAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-70iae 1/1 Running 0 25s nginx-deployment-1564180365-jbqqo 1/1 Running 0 25s nginx-deployment-1564180365-hysrc 1/1 Running 0 25s nginx-deployment-3066724191-08mng 0/1 ImagePullBackOff 0 6s  说明： Deployment 控制器自动停止有问题的上线过程，并停止对新的 ReplicaSet 扩容。 这行为取决于所指定的 rollingUpdate 参数（具体为 maxUnavailable）。 默认情况下，Kubernetes 将此值设置为 25%。\n   获取 Deployment 描述信息：\nkubectl describe deployment 输出类似于：\nName: nginx-deployment Namespace: default CreationTimestamp: Tue, 15 Mar 2016 14:48:04 -0700 Labels: app=nginx Selector: app=nginx Replicas: 3 desired | 1 updated | 4 total | 3 available | 1 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.91 Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated OldReplicaSets: nginx-deployment-1564180365 (3/3 replicas created) NewReplicaSet: nginx-deployment-3066724191 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-2035384211 to 3 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 1 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 2 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 2 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 1 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 3 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 0 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 1 要解决此问题，需要回滚到以前稳定的 Deployment 版本。\n  检查 Deployment 上线历史 按照如下步骤检查回滚历史：\n  首先，检查 Deployment 修订历史：\nkubectl rollout history deployment.v1.apps/nginx-deployment 输出类似于：\ndeployments \u0026#34;nginx-deployment\u0026#34; REVISION CHANGE-CAUSE 1 kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true 2 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true 3 kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true CHANGE-CAUSE 的内容是从 Deployment 的 kubernetes.io/change-cause 注解复制过来的。 复制动作发生在修订版本创建时。你可以通过以下方式设置 CHANGE-CAUSE 消息：\n 使用 kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=\u0026quot;image updated to 1.9.1\u0026quot; 为 Deployment 添加注解。 追加 --record 命令行标志以保存正在更改资源的 kubectl 命令。 手动编辑资源的清单。    要查看修订历史的详细信息，运行：\nkubectl rollout history deployment.v1.apps/nginx-deployment --revision=2 输出类似于：\ndeployments \u0026#34;nginx-deployment\u0026#34; revision 2 Labels: app=nginx pod-template-hash=1159050644 Annotations: kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP QoS Tier: cpu: BestEffort memory: BestEffort Environment Variables: \u0026lt;none\u0026gt; No volumes.   回滚到之前的修订版本 按照下面给出的步骤将 Deployment 从当前版本回滚到以前的版本（即版本 2）。\n  假定现在你已决定撤消当前上线并回滚到以前的修订版本：\nkubectl rollout undo deployment.v1.apps/nginx-deployment 输出类似于：\ndeployment.apps/nginx-deployment 或者，你也可以通过使用 --to-revision 来回滚到特定修订版本：\nkubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2 输出类似于：\ndeployment.apps/nginx-deployment 与回滚相关的指令的更详细信息，请参考 kubectl rollout。\n现在，Deployment 正在回滚到以前的稳定版本。正如你所看到的，Deployment 控制器生成了 回滚到修订版本 2 的 DeploymentRollback 事件。\n  检查回滚是否成功以及 Deployment 是否正在运行，运行：\nkubectl get deployment nginx-deployment 输出类似于：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 30m   获取 Deployment 描述信息：\nkubectl describe deployment nginx-deployment 输出类似于：\nName: nginx-deployment Namespace: default CreationTimestamp: Sun, 02 Sep 2018 18:17:55 -0500 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=4 kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-deployment-c4747d96c (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-75675f5897 to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 0 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-595696685f to 1 Normal DeploymentRollback 15s deployment-controller Rolled back deployment \u0026quot;nginx-deployment\u0026quot; to revision 2 Normal ScalingReplicaSet 15s deployment-controller Scaled down replica set nginx-deployment-595696685f to 0   缩放 Deployment 你可以使用如下指令缩放 Deployment：\nkubectl scale deployment.v1.apps/nginx-deployment --replicas=10 输出类似于：\ndeployment.apps/nginx-deployment scaled 假设集群启用了Pod 的水平自动缩放， 你可以为 Deployment 设置自动缩放器，并基于现有 Pods 的 CPU 利用率选择 要运行的 Pods 个数下限和上限。\nkubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80 输出类似于：\ndeployment.apps/nginx-deployment scaled 比例缩放 RollingUpdate 的 Deployment 支持同时运行应用程序的多个版本。 当自动缩放器缩放处于上线进程（仍在进行中或暂停）中的 RollingUpdate Deployment 时， Deployment 控制器会平衡现有的活跃状态的 ReplicaSets（含 Pods 的 ReplicaSets）中的额外副本， 以降低风险。这称为 比例缩放（Proportional Scaling）。\n例如，你正在运行一个 10 个副本的 Deployment，其 maxSurge=3，maxUnavailable=2。\n  确保 Deployment 的这 10 个副本都在运行。\nkubectl get deploy 输出类似于：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 10 10 10 10 50s   更新 Deployment 使用新镜像，碰巧该镜像无法从集群内部解析。\nkubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:sometag 输出类似于：\ndeployment.apps/nginx-deployment image updated   镜像更新使用 ReplicaSet nginx-deployment-1989198191 启动新的上线过程， 但由于上面提到的 maxUnavailable 要求，该进程被阻塞了。检查上线状态：\nkubectl get rs 输出类似于：\nNAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 5 5 0 9s nginx-deployment-618515232 8 8 8 1m   然后，出现了新的 Deployment 扩缩请求。自动缩放器将 Deployment 副本增加到 15。 Deployment 控制器需要决定在何处添加 5 个新副本。如果未使用比例缩放，所有 5 个副本 都将添加到新的 ReplicaSet 中。使用比例缩放时，可以将额外的副本分布到所有 ReplicaSet。 较大比例的副本会被添加到拥有最多副本的 ReplicaSet，而较低比例的副本会进入到 副本较少的 ReplicaSet。所有剩下的副本都会添加到副本最多的 ReplicaSet。 具有零副本的 ReplicaSets 不会被扩容。\n  在上面的示例中，3 个副本被添加到旧 ReplicaSet 中，2 个副本被添加到新 ReplicaSet。 假定新的副本都很健康，上线过程最终应将所有副本迁移到新的 ReplicaSet 中。 要确认这一点，请运行：\nkubectl get deploy 输出类似于：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 15 18 7 8 7m 上线状态确认了副本是如何被添加到每个 ReplicaSet 的。\nkubectl get rs 输出类似于：\nNAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 7 7 0 7m nginx-deployment-618515232 11 11 11 7m 暂停、恢复 Deployment 你可以在触发一个或多个更新之前暂停 Deployment，然后再恢复其执行。 这样做使得你能够在暂停和恢复执行之间应用多个修补程序，而不会触发不必要的上线操作。\n  例如，对于一个刚刚创建的 Deployment： 获取 Deployment 信息：\nkubectl get deploy 输出类似于：\nNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 3 3 3 3 1m 获取上线状态：\nkubectl get rs 输出类似于：\nNAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 1m   使用如下指令暂停运行：\nkubectl rollout pause deployment.v1.apps/nginx-deployment 输出类似于：\ndeployment.apps/nginx-deployment paused   接下来更新 Deployment 镜像：\nkubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 输出类似于：\ndeployment.apps/nginx-deployment image updated   注意没有新的上线被触发：\nkubectl rollout history deployment.v1.apps/nginx-deployment 输出类似于：\ndeployments \u0026#34;nginx\u0026#34; REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt;   获取上线状态确保 Deployment 更新已经成功：\nkubectl get rs 输出类似于：\nNAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 2m   你可以根据需要执行很多更新操作，例如，可以要使用的资源：\nkubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi 输出类似于：\ndeployment.apps/nginx-deployment resource requirements updated 暂停 Deployment 之前的初始状态将继续发挥作用，但新的更新在 Deployment 被 暂停期间不会产生任何效果。\n  最终，恢复 Deployment 执行并观察新的 ReplicaSet 的创建过程，其中包含了所应用的所有更新：\nkubectl rollout resume deployment.v1.apps/nginx-deployment 输出：\ndeployment.apps/nginx-deployment resumed   观察上线的状态，直到完成。\nkubectl get rs -w 输出类似于：\nNAME DESIRED CURRENT READY AGE nginx-2142116321 2 2 2 2m nginx-3926361531 2 2 0 6s nginx-3926361531 2 2 1 18s nginx-2142116321 1 2 2 2m nginx-2142116321 1 2 2 2m nginx-3926361531 3 2 1 18s nginx-3926361531 3 2 1 18s nginx-2142116321 1 1 1 2m nginx-3926361531 3 3 1 18s nginx-3926361531 3 3 2 19s nginx-2142116321 0 1 1 2m nginx-2142116321 0 1 1 2m nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 20s   获取最近上线的状态：\nkubectl get rs 输出类似于：\nNAME DESIRED CURRENT READY AGE nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 28s    说明： 你不可以回滚处于暂停状态的 Deployment，除非先恢复其执行状态。\n Deployment 状态 Deployment 的生命周期中会有许多状态。上线新的 ReplicaSet 期间可能处于 Progressing（进行中），可能是 Complete（已完成），也可能是 Failed（失败）以至于无法继续进行。\n进行中的 Deployment 执行下面的任务期间，Kubernetes 标记 Deployment 为 进行中（Progressing）：\n Deployment 创建新的 ReplicaSet Deployment 正在为其最新的 ReplicaSet 扩容 Deployment 正在为其旧有的 ReplicaSet(s) 缩容 新的 Pods 已经就绪或者可用（就绪至少持续了 MinReadySeconds 秒）。  你可以使用 kubectl rollout status 监视 Deployment 的进度。\n完成的 Deployment 当 Deployment 具有以下特征时，Kubernetes 将其标记为 完成（Complete）：\n 与 Deployment 关联的所有副本都已更新到指定的最新版本，这意味着之前请求的所有更新都已完成。 与 Deployment 关联的所有副本都可用。 未运行 Deployment 的旧副本。  你可以使用 kubectl rollout status 检查 Deployment 是否已完成。 如果上线成功完成，kubectl rollout status 返回退出代码 0。\nkubectl rollout status deployment/nginx-deployment 输出类似于：\nWaiting for rollout to finish: 2 of 3 updated replicas are available... deployment \u0026#34;nginx-deployment\u0026#34; successfully rolled out $ echo $? 0 失败的 Deployment 你的 Deployment 可能会在尝试部署其最新的 ReplicaSet 受挫，一直处于未完成状态。 造成此情况一些可能因素如下：\n 配额（Quota）不足 就绪探测（Readiness Probe）失败 镜像拉取错误 权限不足 限制范围（Limit Ranges）问题 应用程序运行时的配置错误  检测此状况的一种方法是在 Deployment 规约中指定截止时间参数： （[.spec.progressDeadlineSeconds]（#progress-deadline-seconds））。 .spec.progressDeadlineSeconds 给出的是一个秒数值，Deployment 控制器在（通过 Deployment 状态） 标示 Deployment 进展停滞之前，需要等待所给的时长。\n以下 kubectl 命令设置规约中的 progressDeadlineSeconds，从而告知控制器 在 10 分钟后报告 Deployment 没有进展：\nkubectl patch deployment.v1.apps/nginx-deployment -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;progressDeadlineSeconds\u0026#34;:600}}\u0026#39; 输出类似于：\ndeployment.apps/nginx-deployment patched 超过截止时间后，Deployment 控制器将添加具有以下属性的 DeploymentCondition 到 Deployment 的 .status.conditions 中：\n Type=Progressing Status=False Reason=ProgressDeadlineExceeded  参考 Kubernetes API 约定 获取更多状态状况相关的信息。\n 说明：\n除了报告 Reason=ProgressDeadlineExceeded 状态之外，Kubernetes 对已停止的 Deployment 不执行任何操作。更高级别的编排器可以利用这一设计并相应地采取行动。 例如，将 Deployment 回滚到其以前的版本。\n如果你暂停了某个 Deployment，Kubernetes 不再根据指定的截止时间检查 Deployment 进展。 你可以在上线过程中间安全地暂停 Deployment 再恢复其执行，这样做不会导致超出最后时限的问题。\n Deployment 可能会出现瞬时性的错误，可能因为设置的超时时间过短， 也可能因为其他可认为是临时性的问题。例如，假定所遇到的问题是配额不足。 如果描述 Deployment，你将会注意到以下部分：\nkubectl describe deployment nginx-deployment 输出类似于：\n\u0026lt;...\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated ReplicaFailure True FailedCreate \u0026lt;...\u0026gt; 如果运行 kubectl get deployment nginx-deployment -o yaml，Deployment 状态输出 将类似于这样：\nstatus: availableReplicas: 2 conditions: - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: Replica set \u0026quot;nginx-deployment-4262182780\u0026quot; is progressing. reason: ReplicaSetUpdated status: \u0026quot;True\u0026quot; type: Progressing - lastTransitionTime: 2016-10-04T12:25:42Z lastUpdateTime: 2016-10-04T12:25:42Z message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: \u0026quot;True\u0026quot; type: Available - lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: 'Error creating: pods \u0026quot;nginx-deployment-4262182780-\u0026quot; is forbidden: exceeded quota: object-counts, requested: pods=1, used: pods=3, limited: pods=2' reason: FailedCreate status: \u0026quot;True\u0026quot; type: ReplicaFailure observedGeneration: 3 replicas: 2 unavailableReplicas: 2 最终，一旦超过 Deployment 进度限期，Kubernetes 将更新状态和进度状况的原因：\nConditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing False ProgressDeadlineExceeded ReplicaFailure True FailedCreate 可以通过缩容 Deployment 或者缩容其他运行状态的控制器，或者直接在命名空间中增加配额 来解决配额不足的问题。如果配额条件满足，Deployment 控制器完成了 Deployment 上线操作， Deployment 状态会更新为成功状况（Status=True and Reason=NewReplicaSetAvailable）。\nConditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable Type=Available 加上 Status=True 意味着 Deployment 具有最低可用性。 最低可用性由 Deployment 策略中的参数指定。 Type=Progressing 加上 Status=True 表示 Deployment 处于上线过程中，并且正在运行， 或者已成功完成进度，最小所需新副本处于可用。 请参阅对应状况的 Reason 了解相关细节。 在我们的案例中 Reason=NewReplicaSetAvailable 表示 Deployment 已完成。\n你可以使用 kubectl rollout status 检查 Deployment 是否未能取得进展。 如果 Deployment 已超过进度限期，kubectl rollout status 返回非零退出代码。\nkubectl rollout status deployment/nginx-deployment 输出类似于：\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated... error: deployment \u0026quot;nginx\u0026quot; exceeded its progress deadline kubectl rollout 命令的退出状态为 1（表明发生了错误）：\n$ echo $? 1 对失败 Deployment 的操作 可应用于已完成的 Deployment 的所有操作也适用于失败的 Deployment。 你可以对其执行扩缩容、回滚到以前的修订版本等操作，或者在需要对 Deployment 的 Pod 模板应用多项调整时，将 Deployment 暂停。\n清理策略 你可以在 Deployment 中设置 .spec.revisionHistoryLimit 字段以指定保留此 Deployment 的多少个旧有 ReplicaSet。其余的 ReplicaSet 将在后台被垃圾回收。 默认情况下，此值为 10。\n 说明： 显式将此字段设置为 0 将导致 Deployment 的所有历史记录被清空，因此 Deployment 将无法回滚。\n 金丝雀部署 如果要使用 Deployment 向用户子集或服务器子集上线版本，则可以遵循 资源管理 所描述的金丝雀模式，创建多个 Deployment，每个版本一个。\n编写 Deployment 规约 同其他 Kubernetes 配置一样， Deployment 需要 apiVersion，kind 和 metadata 字段。 有关配置文件的其他信息，请参考 部署 Deployment 、配置容器和 使用 kubectl 管理资源等相关文档。\nDeployment 对象的名称必须是合法的 DNS 子域名。 Deployment 还需要 .spec 部分。\nPod 模板 .spec 中只有 .spec.template 和 .spec.selector 是必需的字段。\n.spec.template 是一个 Pod 模板。 它和 Pod 的语法规则完全相同。 只是这里它是嵌套的，因此不需要 apiVersion 或 kind。\n除了 Pod 的必填字段外，Deployment 中的 Pod 模板必须指定适当的标签和适当的重新启动策略。 对于标签，请确保不要与其他控制器重叠。请参考选择算符。\n只有 .spec.template.spec.restartPolicy 等于 Always 才是被允许的，这也是在没有指定时的默认设置。\n副本 .spec.replicas 是指定所需 Pod 的可选字段。它的默认值是1。\n选择算符 .spec.selector 是指定本 Deployment 的 Pod 标签选择算符的必需字段。\n.spec.selector 必须匹配 .spec.template.metadata.labels，否则请求会被 API 拒绝。\n在 API apps/v1版本中，.spec.selector 和 .metadata.labels 如果没有设置的话， 不会被默认设置为 .spec.template.metadata.labels，所以需要明确进行设置。 同时在 apps/v1版本中，Deployment 创建后 .spec.selector 是不可变的。\n当 Pod 的标签和选择算符匹配，但其模板和 .spec.template 不同时，或者此类 Pod 的总数超过 .spec.replicas 的设置时，Deployment 会终结之。 如果 Pods 总数未达到期望值，Deployment 会基于 .spec.template 创建新的 Pod。\n 说明： 你不应直接创建、或者通过创建另一个 Deployment，或者创建类似 ReplicaSet 或 ReplicationController 这类控制器来创建标签与此选择算符匹配的 Pod。 如果这样做，第一个 Deployment 会认为它创建了这些 Pod。 Kubernetes 不会阻止你这么做。\n 如果有多个控制器的选择算符发生重叠，则控制器之间会因冲突而无法正常工作。\n策略 .spec.strategy 策略指定用于用新 Pods 替换旧 Pods 的策略。 .spec.strategy.type 可以是 “Recreate” 或 “RollingUpdate”。“RollingUpdate” 是默认值。\n重新创建 Deployment 如果 .spec.strategy.type==Recreate，在创建新 Pods 之前，所有现有的 Pods 会被杀死。\n滚动更新 Deployment Deployment 会在 .spec.strategy.type==RollingUpdate时，采取 滚动更新的方式更新 Pods。你可以指定 maxUnavailable 和 maxSurge 来控制滚动更新 过程。\n最大不可用 .spec.strategy.rollingUpdate.maxUnavailable 是一个可选字段，用来指定 更新过程中不可用的 Pod 的个数上限。该值可以是绝对数字（例如，5），也可以是 所需 Pods 的百分比（例如，10%）。百分比值会转换成绝对数并去除小数部分。 如果 .spec.strategy.rollingUpdate.maxSurge 为 0，则此值不能为 0。 默认值为 25%。\n例如，当此值设置为 30% 时，滚动更新开始时会立即将旧 ReplicaSet 缩容到期望 Pod 个数的70%。 新 Pod 准备就绪后，可以继续缩容旧有的 ReplicaSet，然后对新的 ReplicaSet 扩容，确保在更新期间 可用的 Pods 总数在任何时候都至少为所需的 Pod 个数的 70%。\n最大峰值 .spec.strategy.rollingUpdate.maxSurge 是一个可选字段，用来指定可以创建的超出 期望 Pod 个数的 Pod 数量。此值可以是绝对数（例如，5）或所需 Pods 的百分比（例如，10%）。 如果 MaxUnavailable 为 0，则此值不能为 0。百分比值会通过向上取整转换为绝对数。 此字段的默认值为 25%。\n例如，当此值为 30% 时，启动滚动更新后，会立即对新的 ReplicaSet 扩容，同时保证新旧 Pod 的总数不超过所需 Pod 总数的 130%。一旦旧 Pods 被杀死，新的 ReplicaSet 可以进一步扩容， 同时确保更新期间的任何时候运行中的 Pods 总数最多为所需 Pods 总数的 130%。\n进度期限秒数 .spec.progressDeadlineSeconds 是一个可选字段，用于指定系统在报告 Deployment 进展失败 之前等待 Deployment 取得进展的秒数。 这类报告会在资源状态中体现为 Type=Progressing、Status=False、 Reason=ProgressDeadlineExceeded。Deployment 控制器将持续重试 Deployment。 将来，一旦实现了自动回滚，Deployment 控制器将在探测到这样的条件时立即回滚 Deployment。\n如果指定，则此字段值需要大于 .spec.minReadySeconds 取值。\n最短就绪时间 .spec.minReadySeconds 是一个可选字段，用于指定新创建的 Pod 在没有任意容器崩溃情况下的最小就绪时间， 只有超出这个时间 Pod 才被视为可用。默认值为 0（Pod 在准备就绪后立即将被视为可用）。 要了解何时 Pod 被视为就绪，可参考容器探针。\n修订历史限制 Deployment 的修订历史记录存储在它所控制的 ReplicaSets 中。\n.spec.revisionHistoryLimit 是一个可选字段，用来设定出于会滚目的所要保留的旧 ReplicaSet 数量。 这些旧 ReplicaSet 会消耗 etcd 中的资源，并占用 kubectl get rs 的输出。 每个 Deployment 修订版本的配置都存储在其 ReplicaSets 中；因此，一旦删除了旧的 ReplicaSet， 将失去回滚到 Deployment 的对应修订版本的能力。 默认情况下，系统保留 10 个旧 ReplicaSet，但其理想值取决于新 Deployment 的频率和稳定性。\n更具体地说，将此字段设置为 0 意味着将清理所有具有 0 个副本的旧 ReplicaSet。 在这种情况下，无法撤消新的 Deployment 上线，因为它的修订历史被清除了。\npaused（暂停的） .spec.paused 是用于暂停和恢复 Deployment 的可选布尔字段。 暂停的 Deployment 和未暂停的 Deployment 的唯一区别是，Deployment 处于暂停状态时， PodTemplateSpec 的任何修改都不会触发新的上线。 Deployment 在创建时是默认不会处于暂停状态。\n服务 负载均衡 网络 存储 配置 ","date":"2021-07-26T17:00:00+08:00","image":"https://www.catfish.top/p/k8s-inter-1/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-inter-1/","title":"Kubernetes进阶（一）"},{"content":" Katacoda在线课：Helm Package Manager\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 这个场景教你如何使用 Kubernetes 的包管理器 Helm 来部署 Redis。 Helm 简化了服务发现和部署到 Kubernetes 集群的步骤。。\n \u0026ldquo;Helm is the best way to find, share, and use software built for Kubernetes.\u0026quot;\nHelm 是查找、共享和使用为 Kubernetes 构建的软件的最佳方式\n 更多细节可以前往官网：http://www.helm.sh/\n安装 Helm Helm 是一个单独的二进制文件，用于管理将 Charts 部署到 Kubernetes。 Chart 是 kubernetes 应用的一个打包单元。Helm 可以从 https://github.com/kubernetes/helm/releases 下载。\ncontrolplane $ curl -LO https://storage.googleapis.com/kubernetes-helm/helm-v2.8.2-linux-amd64.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 14.2M 100 14.2M 0 0 19.4M 0 --:--:-- --:--:-- --:--:-- 19.4M controlplane $ tar -xvf helm-v2.8.2-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/README.md linux-amd64/LICENSE controlplane $ mv linux-amd64/helm /usr/local/bin/ 安装后，初始化更新本地缓存将最新的包与本地安装环境同步。\ncontrolplane $ helm init --stable-repo-url https://charts.helm.sh/stable Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://charts.helm.sh/stable Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure \u0026#39;allow unauthenticated users\u0026#39; policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! controlplane $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Skip local chart repository ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. ⎈ Happy Helming!⎈ 搜索 Chart 您现在可以开始部署软件。可以使用搜索命令查找可用图表 Chart 。\n例如，要部署 Redis，我们需要找到一个 Redis 的 Chart 。\ncontrolplane $ helm search redis NAME CHART VERSION APP VERSION DESCRIPTION stable/prometheus-redis-exporter 3.5.1 1.3.4 DEPRECATED Prometheus exporter for Redis metrics stable/redis 10.5.7 5.0.7 DEPRECATED Open source, advanced key-value stor... stable/redis-ha 4.4.6 5.0.6 DEPRECATED - Highly available Kubernetes implem... stable/sensu 0.2.5 0.28 DEPRECATED Sensu monitoring framework backed by.. 通过 inspect 命令，我们可以查看更多关于 stable/redis 的信息，信息量很大。\ncontrolplane $ helm inspect stable/redis apiVersion: v1 appVersion: 5.0.7 deprecated: true description: DEPRECATED Open source, advanced key-value store. It is often referred to as a data structure server since keys can contain strings, hashes, lists, sets and sorted sets. engine: gotpl home: http://redis.io/ icon: https://bitnami.com/assets/stacks/redis/img/redis-stack-220x234.png keywords: - redis - keyvalue - database name: redis sources: - https://github.com/bitnami/bitnami-docker-redis version: 10.5.7 --- ## Global Docker image parameters ## Please, note that this will override the image parameters, including dependencies, configured to use the global value ## Current available global Docker image parameters: imageRegistry and imagePullSecrets ## global: # imageRegistry: myRegistryName # imagePullSecrets: # - myRegistryKeySecretName # storageClass: myStorageClass redis: {} ## Bitnami Redis image version ## ref: https://hub.docker.com/r/bitnami/redis/tags/ ## image: registry: docker.io repository: bitnami/redis ## Bitnami Redis image tag ## ref: https://github.com/bitnami/bitnami-docker-redis#supported-tags-and-respective-dockerfile-links ## tag: 5.0.7-debian-10-r32 ## Specify a imagePullPolicy ## Defaults to \u0026#39;Always\u0026#39; if image tag is \u0026#39;latest\u0026#39;, else set to \u0026#39;IfNotPresent\u0026#39; ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images ## pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName ## String to partially override redis.fullname template (will maintain the release name) ## # nameOverride: ## String to fully override redis.fullname template ## # fullnameOverride: ## Cluster settings cluster: enabled: true slaveCount: 2 ## Use redis sentinel in the redis pod. This will disable the master and slave services and ## create one redis service with ports to the sentinel and the redis instances sentinel: enabled: false ## Require password authentication on the sentinel itself ## ref: https://redis.io/topics/sentinel usePassword: true ## Bitnami Redis Sentintel image version ## ref: https://hub.docker.com/r/bitnami/redis-sentinel/tags/ ## image: registry: docker.io repository: bitnami/redis-sentinel ## Bitnami Redis image tag ## ref: https://github.com/bitnami/bitnami-docker-redis-sentinel#supported-tags-and-respective-dockerfile-links ## tag: 5.0.7-debian-10-r27 ## Specify a imagePullPolicy ## Defaults to \u0026#39;Always\u0026#39; if image tag is \u0026#39;latest\u0026#39;, else set to \u0026#39;IfNotPresent\u0026#39; ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images ## pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName masterSet: mymaster initialCheckTimeout: 5 quorum: 2 downAfterMilliseconds: 60000 failoverTimeout: 18000 parallelSyncs: 1 port: 26379 ## Additional Redis configuration for the sentinel nodes ## ref: https://redis.io/topics/config ## configmap: ## Enable or disable static sentinel IDs for each replicas ## If disabled each sentinel will generate a random id at startup ## If enabled, each replicas will have a constant ID on each start-up ## staticID: false ## Configure extra options for Redis Sentinel liveness and readiness probes ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes) ## livenessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 1 successThreshold: 1 failureThreshold: 5 ## Redis Sentinel resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ # resources: # requests: # memory: 256Mi # cpu: 100m ## Redis Sentinel Service properties service: ## Redis Sentinel Service type type: ClusterIP sentinelPort: 26379 redisPort: 6379 ## Specify the nodePort value for the LoadBalancer and NodePort service types. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport ## # sentinelNodePort: # redisNodePort: ## Provide any additional annotations which may be required. This can be used to ## set the LoadBalancer service type to internal only. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer ## annotations: {} labels: {} loadBalancerIP: ## Specifies the Kubernetes Cluster\u0026#39;s Domain Name. ## clusterDomain: cluster.local networkPolicy: ## Specifies whether a NetworkPolicy should be created ## enabled: false ## The Policy model to apply. When set to false, only pods with the correct ## client label will have network access to the port Redis is listening ## on. When true, Redis will accept connections from any source ## (with the correct destination port). ## # allowExternal: true ## Allow connections from other namespacess. Just set label for namespace and set label for pods (optional). ## ingressNSMatchLabels: {} ingressNSPodMatchLabels: {} serviceAccount: ## Specifies whether a ServiceAccount should be created ## create: false ## The name of the ServiceAccount to use. ## If not set and create is true, a name is generated using the fullname template name: rbac: ## Specifies whether RBAC resources should be created ## create: false role: ## Rules to create. It follows the role specification # rules: # - apiGroups: # - extensions # resources: # - podsecuritypolicies # verbs: # - use # resourceNames: # - gce.unprivileged rules: [] ## Redis pod Security Context securityContext: enabled: true fsGroup: 1001 runAsUser: 1001 ## sysctl settings for master and slave pods ## ## Uncomment the setting below to increase the net.core.somaxconn value ## # sysctls: # - name: net.core.somaxconn # value: \u0026#34;10000\u0026#34; ## Use password authentication usePassword: true ## Redis password (both master and slave) ## Defaults to a random 10-character alphanumeric string if not set and usePassword is true ## ref: https://github.com/bitnami/bitnami-docker-redis#setting-the-server-password-on-first-run ## password: \u0026#34;\u0026#34; ## Use existing secret (ignores previous password) # existingSecret: ## Password key to be retrieved from Redis secret ## # existingSecretPasswordKey: ## Mount secrets as files instead of environment variables usePasswordFile: false ## Persist data to a persistent volume (Redis Master) persistence: {} ## A manually managed Persistent Volume and Claim ## Requires persistence.enabled: true ## If defined, PVC must be created manually before volume will be bound # existingClaim: # Redis port redisPort: 6379 ## ## Redis Master parameters ## master: ## Redis command arguments ## ## Can be used to specify command line arguments, for example: ## command: \u0026#34;/run.sh\u0026#34; ## Additional Redis configuration for the master nodes ## ref: https://redis.io/topics/config ## configmap: ## Redis additional command line flags ## ## Can be used to specify command line flags, for example: ## ## extraFlags: ## - \u0026#34;--maxmemory-policy volatile-ttl\u0026#34; ## - \u0026#34;--repl-backlog-size 1024mb\u0026#34; extraFlags: [] ## Comma-separated list of Redis commands to disable ## ## Can be used to disable Redis commands for security reasons. ## Commands will be completely disabled by renaming each to an empty string. ## ref: https://redis.io/topics/security#disabling-of-specific-commands ## disableCommands: - FLUSHDB - FLUSHALL ## Redis Master additional pod labels and annotations ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ podLabels: {} podAnnotations: {} ## Redis Master resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ # resources: # requests: # memory: 256Mi # cpu: 100m ## Use an alternate scheduler, e.g. \u0026#34;stork\u0026#34;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: ## Configure extra options for Redis Master liveness and readiness probes ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes) ## livenessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 1 successThreshold: 1 failureThreshold: 5 ## Redis Master Node selectors and tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature ## # nodeSelector: {\u0026#34;beta.kubernetes.io/arch\u0026#34;: \u0026#34;amd64\u0026#34;} # tolerations: [] ## Redis Master pod/node affinity/anti-affinity ## affinity: {} ## Redis Master Service properties service: ## Redis Master Service type type: ClusterIP port: 6379 ## Specify the nodePort value for the LoadBalancer and NodePort service types. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport ## # nodePort: ## Provide any additional annotations which may be required. This can be used to ## set the LoadBalancer service type to internal only. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer ## annotations: {} labels: {} loadBalancerIP: # loadBalancerSourceRanges: [\u0026#34;10.0.0.0/8\u0026#34;] ## Enable persistence using Persistent Volume Claims ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## persistence: enabled: true ## The path the volume will be mounted at, useful when using different ## Redis images. path: /data ## The subdirectory of the volume to mount to, useful in dev environments ## and one PV for multiple services. subPath: \u0026#34;\u0026#34; ## redis data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026#34;-\u0026#34;, storageClassName: \u0026#34;\u0026#34;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## # storageClass: \u0026#34;-\u0026#34; accessModes: - ReadWriteOnce size: 8Gi ## Persistent Volume selectors ## https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector matchLabels: {} matchExpressions: {} ## Update strategy, can be set to RollingUpdate or onDelete by default. ## https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets statefulset: updateStrategy: RollingUpdate ## Partition update strategy ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions # rollingUpdatePartition: ## Redis Master pod priorityClassName # priorityClassName: {} ## ## Redis Slave properties ## Note: service.type is a mandatory parameter ## The rest of the parameters are either optional or, if undefined, will inherit those declared in Redis Master ## slave: ## Slave Service properties service: ## Redis Slave Service type type: ClusterIP ## Redis port port: 6379 ## Specify the nodePort value for the LoadBalancer and NodePort service types. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport ## # nodePort: ## Provide any additional annotations which may be required. This can be used to ## set the LoadBalancer service type to internal only. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer ## annotations: {} labels: {} loadBalancerIP: # loadBalancerSourceRanges: [\u0026#34;10.0.0.0/8\u0026#34;] ## Redis slave port port: 6379 ## Can be used to specify command line arguments, for example: ## command: \u0026#34;/run.sh\u0026#34; ## Additional Redis configuration for the slave nodes ## ref: https://redis.io/topics/config ## configmap: ## Redis extra flags extraFlags: [] ## List of Redis commands to disable disableCommands: - FLUSHDB - FLUSHALL ## Redis Slave pod/node affinity/anti-affinity ## affinity: {} ## Configure extra options for Redis Slave liveness and readiness probes ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes) ## livenessProbe: enabled: true initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 10 timeoutSeconds: 10 successThreshold: 1 failureThreshold: 5 ## Redis slave Resource # resources: # requests: # memory: 256Mi # cpu: 100m ## Redis slave selectors and tolerations for pod assignment # nodeSelector: {\u0026#34;beta.kubernetes.io/arch\u0026#34;: \u0026#34;amd64\u0026#34;} # tolerations: [] ## Use an alternate scheduler, e.g. \u0026#34;stork\u0026#34;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: ## Redis slave pod Annotation and Labels podLabels: {} podAnnotations: {} ## Redis slave pod priorityClassName # priorityClassName: {} ## Enable persistence using Persistent Volume Claims ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## persistence: enabled: true ## The path the volume will be mounted at, useful when using different ## Redis images. path: /data ## The subdirectory of the volume to mount to, useful in dev environments ## and one PV for multiple services. subPath: \u0026#34;\u0026#34; ## redis data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026#34;-\u0026#34;, storageClassName: \u0026#34;\u0026#34;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## # storageClass: \u0026#34;-\u0026#34; accessModes: - ReadWriteOnce size: 8Gi ## Persistent Volume selectors ## https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector matchLabels: {} matchExpressions: {} ## Update strategy, can be set to RollingUpdate or onDelete by default. ## https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets statefulset: updateStrategy: RollingUpdate ## Partition update strategy ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions # rollingUpdatePartition: ## Prometheus Exporter / Metrics ## metrics: enabled: false image: registry: docker.io repository: bitnami/redis-exporter tag: 1.4.0-debian-10-r3 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName ## Metrics exporter resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## # resources: {} ## Extra arguments for Metrics exporter, for example: ## extraArgs: ## check-keys: myKey,myOtherKey # extraArgs: {} ## Metrics exporter pod Annotation and Labels podAnnotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;9121\u0026#34; # podLabels: {} # Enable this if you\u0026#39;re using https://github.com/coreos/prometheus-operator serviceMonitor: enabled: false ## Specify a namespace if needed # namespace: monitoring # fallback to the prometheus default unless specified # interval: 10s ## Defaults to what\u0026#39;s used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr) ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1) ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters) selector: prometheus: kube-prometheus ## Custom PrometheusRule to be defined ## The value is evaluated as a template, so, for example, the value can depend on .Release or .Chart ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions prometheusRule: enabled: false additionalLabels: {} namespace: \u0026#34;\u0026#34; rules: [] ## These are just examples rules, please adapt them to your needs. ## Make sure to constraint the rules to the current postgresql service. # - alert: RedisDown # expr: redis_up{service=\u0026#34;{{ template \u0026#34;redis.fullname\u0026#34; . }}-metrics\u0026#34;} == 0 # for: 2m # labels: # severity: error # annotations: # summary: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} down # description: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} is down. # - alert: RedisMemoryHigh # expr: \u0026gt; # redis_memory_used_bytes{service=\u0026#34;{{ template \u0026#34;redis.fullname\u0026#34; . }}-metrics\u0026#34;} * 100 # / # redis_memory_max_bytes{service=\u0026#34;{{ template \u0026#34;redis.fullname\u0026#34; . }}-metrics\u0026#34;} # \u0026gt; 90 =\u0026lt; 100 # for: 2m # labels: # severity: error # annotations: # summary: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} is using too much memory # description: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} is using {{ \u0026#34;{{ $value }}\u0026#34; }}% of its available memory. # - alert: RedisKeyEviction # expr: increase(redis_evicted_keys_total{service=\u0026#34;{{ template \u0026#34;redis.fullname\u0026#34; . }}-metrics\u0026#34;}[5m]) \u0026gt; 0 # for: 1s # labels: # severity: error # annotations: # summary: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} has evicted keys # description: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} has evicted {{ \u0026#34;{{ $value }}\u0026#34; }} keys in the last 5 minutes. ## Metrics exporter pod priorityClassName # priorityClassName: {} service: type: ClusterIP ## Use serviceLoadBalancerIP to request a specific static IP, ## otherwise leave blank # loadBalancerIP: annotations: {} labels: {} ## ## Init containers parameters: ## volumePermissions: Change the owner of the persist volume mountpoint to RunAsUser:fsGroup ## volumePermissions: enabled: false image: registry: docker.io repository: bitnami/minideb tag: buster pullPolicy: Always ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName resources: {} # resources: # requests: # memory: 128Mi # cpu: 100m ## Redis config file ## ref: https://redis.io/topics/config ## configmap: |- # Enable AOF https://redis.io/topics/persistence#append-only-file appendonly yes # Disable RDB persistence, AOF persistence already enabled. save \u0026#34;\u0026#34; ## Sysctl InitContainer ## used to perform sysctl operation to modify Kernel settings (needed sometimes to avoid warnings) sysctlImage: enabled: false command: [] registry: docker.io repository: bitnami/minideb tag: buster pullPolicy: Always ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName mountHostSys: false resources: {} # resources: # requests: # memory: 128Mi # cpu: 100m ## PodSecurityPolicy configuration ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ ## podSecurityPolicy: ## Specifies whether a PodSecurityPolicy should be created ## create: false 部署 Redis 使用 install 命令部署 Chart 至集群中。\ncontrolplane $ helm install stable/redis NAME: iced-ibis LAST DEPLOYED: Mon Jul 26 05:44:02 2021 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iced-ibis-redis-headless ClusterIP None \u0026lt;none\u0026gt; 6379/TCP 1s iced-ibis-redis-master ClusterIP 10.105.234.211 \u0026lt;none\u0026gt; 6379/TCP 0s iced-ibis-redis-slave ClusterIP 10.107.214.148 \u0026lt;none\u0026gt; 6379/TCP 0s ==\u0026gt; v1/StatefulSet NAME DESIRED CURRENT AGE iced-ibis-redis-master 1 1 0s iced-ibis-redis-slave 2 1 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE iced-ibis-redis-master-0 0/1 Pending 0 0s iced-ibis-redis-slave-0 0/1 Pending 0 0s ==\u0026gt; v1/Secret NAME TYPE DATA AGE iced-ibis-redis Opaque 1 1s ==\u0026gt; v1/ConfigMap NAME DATA AGE iced-ibis-redis 3 1s iced-ibis-redis-health 6 1s NOTES: This Helm chart is deprecated Given the `stable` deprecation timeline (https://github.com/helm/charts#deprecation-timeline), the Bitnami maintained Redis Helm chart is now located at bitnami/charts (https://github.com/bitnami/charts/). The Bitnami repository is already included in the Hubs and we will continue providing the same cadence of updates, support, etc that we\u0026#39;ve been keeping here these years. Installation instructions are very similar, just adding the _bitnami_ repo and using it during the installation (`bitnami/\u0026lt;chart\u0026gt;` instead of `stable/\u0026lt;chart\u0026gt;`) \\\\`\\\\`\\\\`bash $ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm install my-release bitnami/\u0026lt;chart\u0026gt; # Helm 3 $ helm install --name my-release bitnami/\u0026lt;chart\u0026gt; # Helm 2 \\\\`\\\\`\\\\` To update an exisiting _stable_ deployment with a chart hosted in the bitnami repository you can execute \\\\`\\\\`\\\\`bash $ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm upgrade my-release bitnami/\u0026lt;chart\u0026gt; \\\\`\\\\`\\\\` Issues and PRs related to the chart itself will be redirected to `bitnami/charts` GitHub repository. In the same way, we\u0026#39;ll be happy to answer questions related to this migration process in this issue (https://github.com/helm/charts/issues/20969) created as a common place for discussion. ** Please be patient while the chart is being deployed ** Redis can be accessed via port 6379 on the following DNS names from within your cluster: iced-ibis-redis-master.default.svc.cluster.local for read/write operations iced-ibis-redis-slave.default.svc.cluster.local for read-only operations To get your password run: export REDIS_PASSWORD=$(kubectl get secret --namespace default iced-ibis-redis -o jsonpath=\u0026#34;{.data.redis-password}\u0026#34; | base64 --decode) To connect to your Redis server: 1. Run a Redis pod that you can use as a client: kubectl run --namespace default iced-ibis-redis-client --rm --tty -i --restart=\u0026#39;Never\u0026#39; \\  --env REDIS_PASSWORD=$REDIS_PASSWORD \\  --image docker.io/bitnami/redis:5.0.7-debian-10-r32 -- bash 2. Connect using the Redis CLI: redis-cli -h iced-ibis-redis-master -a $REDIS_PASSWORD redis-cli -h iced-ibis-redis-slave -a $REDIS_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace default svc/iced-ibis-redis-master 6379:6379 \u0026amp; redis-cli -h 127.0.0.1 -p 6379 -a $REDIS_PASSWORD Helm 现在将启动所需的 Pod。您可以使用 helm ls 查看所有包。\ncontrolplane $ helm ls NAME REVISION UPDATED STATUS CHART NAMESPACE iced-ibis 1 Mon Jul 26 05:44:02 2021 DEPLOYED redis-10.5.7 default 如果您收到 Helm 无法找到准备好的 Pod 的错误消息，则表示 helm 仍在部署中。稍等片刻，直到 Docker Image 完成下载。\n在下一步中，我们将验证部署状态。\n查看结果 Helm 部署了所有 Pod、Replication Controller 和 Controller 。使用 kubectl 找出部署的内容。\ncontrolplane $ kubectl get all NAME READY STATUS RESTARTS AGE pod/iced-ibis-redis-master-0 0/1 Pending 0 12m pod/iced-ibis-redis-slave-0 0/1 Pending 0 12m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/iced-ibis-redis-headless ClusterIP None \u0026lt;none\u0026gt; 6379/TCP 12m service/iced-ibis-redis-master ClusterIP 10.105.234.211 \u0026lt;none\u0026gt; 6379/TCP 12m service/iced-ibis-redis-slave ClusterIP 10.107.214.148 \u0026lt;none\u0026gt; 6379/TCP 12m service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 53m NAME READY AGE statefulset.apps/iced-ibis-redis-master 0/1 12m statefulset.apps/iced-ibis-redis-slave 0/2 12m 在下载 Docker 镜像时，Pod 将处于 pending 状态，直到 Persistent Volume 可用。\ncontrolplane $ kubectl apply -f pv.yaml persistentvolume/pv-volume1 created persistentvolume/pv-volume2 created persistentvolume/pv-volume3 created pv.yaml\nkind:PersistentVolumeapiVersion:v1metadata:name:pv-volume1labels:type:localspec:capacity:storage:10GiaccessModes:- ReadWriteOncehostPath:path:\u0026#34;/mnt/data1\u0026#34;---kind:PersistentVolumeapiVersion:v1metadata:name:pv-volume2labels:type:localspec:capacity:storage:10GiaccessModes:- ReadWriteOncehostPath:path:\u0026#34;/mnt/data2\u0026#34;---kind:PersistentVolumeapiVersion:v1metadata:name:pv-volume3labels:type:localspec:capacity:storage:10GiaccessModes:- ReadWriteOncehostPath:path:\u0026#34;/mnt/data3\u0026#34;Redis 需要写入权限。\ncontrolplane $ chmod 777 -R /mnt/data* controlplane $ kubectl get all NAME READY STATUS RESTARTS AGE pod/iced-ibis-redis-master-0 1/1 Running 3 18m pod/iced-ibis-redis-slave-0 0/1 CrashLoopBackOff 3 18m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/iced-ibis-redis-headless ClusterIP None \u0026lt;none\u0026gt; 6379/TCP 18m service/iced-ibis-redis-master ClusterIP 10.105.234.211 \u0026lt;none\u0026gt; 6379/TCP 18m service/iced-ibis-redis-slave ClusterIP 10.107.214.148 \u0026lt;none\u0026gt; 6379/TCP 18m service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 58m NAME READY AGE statefulset.apps/iced-ibis-redis-master 1/1 18m statefulset.apps/my-release-redis-master 1/1 36s 一旦完成，它将进入running 状态。您现在将拥有一个在 Kubernetes 之上运行的 Redis 集群。\nHelm 能够为部署的组件提供自定义的名称，例如：\nhelm install --name my-release stable/redis ","date":"2021-07-23T14:54:00+08:00","image":"https://www.catfish.top/p/k8s-basic-11/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-11/","title":"Kubernetes初探（十一）"},{"content":" Katacoda在线课：Use Kubernetes To Manage Secrets And Passwords\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，您将了解如何使用 Kubernetes 管理 Secrets 。 Kubernetes 允许您创建通过环境变量或作为卷挂载到 Pod 的 Secrets 。\n只允许 Secrets （例如 SSL 证书或密码）通过基础架构团队以安全的方式进行管理，而不是将密码存储在应用程序的部署工件中。\n启动 Kubernetes 首先，我们需要启动一个 Kubernetes 集群。\n执行以下命令启动集群组件并下载 Kubectl CLI。\ncontrolplane $ launch.sh Waiting for Kubernetes to start... Kubernetes started 创建 Secrets Kubernetes 要求将 Secrets 编码为 Base64 的字符串。\n使用命令行工具，我们可以创建 Base64 字符串并将它们存储为变量在文件中使用。\ncontrolplane $ username=$(echo -n \u0026#34;admin\u0026#34; | base64) controlplane $ password=$(echo -n \u0026#34;a62fjbd37942dcs\u0026#34; | base64) Secrets 是使用 YAML 定义的。下面我们将使用上面定义的变量，并为它们提供我们的应用程序可以使用的标签。这将创建一个可以通过名称访问的键/值秘密的集合。\necho \u0026#34;apiVersion: v1 kind: Secret metadata: name: test-secret type: Opaque data: username: $usernamepassword: $password\u0026#34; \u0026gt;\u0026gt; secret.yaml secret.yaml\napiVersion:v1kind:Secretmetadata:name:test-secrettype:Opaquedata:username:YWRtaW4=password:YTYyZmpiZDM3OTQyZGNz这个 YAML 文件中定义可以与 Kubectl 一起使用来创建我们的 Secrets 。在启动需要访问密钥的 Pod 时，我们将通过名称引用集合。\n任务: 创建 Secret 使用 kubectl 创建 Secret\ncontrolplane $ kubectl create -f secret.yaml secret/test-secret created 以下命令允许您查看定义的所有Secret集合。\ncontrolplane $ kubectl get secrets NAME TYPE DATA AGE default-token-z8shh kubernetes.io/service-account-token 3 14m test-secret Opaque 2 26s 在下一步中，我们将通过 Pod 使用这些 Secret 。\n通过环境变量使用 Secret 在文件 secret-env.yaml 中，我们定义了一个有先前创建的 Secret 为环境变量的 Pod 。\n使用 cat secret-env.yaml 查看文件\nsecret-env.yaml\napiVersion:v1kind:Podmetadata:name:secret-env-podspec:containers:- name:mycontainerimage:alpine:latestcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;9999\u0026#34;]env:- name:SECRET_USERNAMEvalueFrom:secretKeyRef:name:test-secretkey:username- name:SECRET_PASSWORDvalueFrom:secretKeyRef:name:test-secretkey:passwordrestartPolicy:Never为了填充环境变量，我们定义了名称，在本例中为 SECRET_USERNAME，以及 Secret 集合的名称和包含数据的密钥。\n结构如下所示：\n- name:SECRET_USERNAMEvalueFrom:secretKeyRef:name:test-secretkey:username任务 使用 kubectl create -f secret-env.yaml 启动 Pod 。\ncontrolplane $ kubectl create -f secret-env.yaml pod/secret-env-pod created Pod 启动后，将输出填充的环境变量。\ncontrolplane $ kubectl exec -it secret-env-pod env | grep SECRET_ SECRET_USERNAME=admin SECRET_PASSWORD=a62fjbd37942dcs Kubernetes 在填充环境变量时解码 base64 值。您应该会看到我们定义的原始用户名/密码组合。这些变量现在可用于访问 API、数据库等。\n您可以使用 kubectl get pods 检查 Pod 的状态。\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE secret-env-pod 1/1 Running 0 34s 在下一步中，我们将把 Secret 挂载为文件。\n通过文件使用 Secret 使用环境变量在内存中存储 Secret 可能会导致它们意外泄漏。推荐的方法是将它们作为卷安装。\nPod 定义可以使用 cat secret-pod.yaml 查看。\nsecret-pod.yaml\napiVersion:v1kind:Podmetadata:name:secret-vol-podspec:volumes:- name:secret-volumesecret:secretName:test-secretcontainers:- name:test-containerimage:alpine:latestcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;9999\u0026#34;]volumeMounts:- name:secret-volumemountPath:/etc/secret-volume要将 Secret 安装为卷，我们首先定义一个具有名称的卷，在本例中为 Secret 卷，并为其提供我们存储的 Secret 。\nvolumes:- name:secret-volumesecret:secretName:test-secret当我们定义容器时，我们将创建的卷挂载到特定目录。应用程序将从该路径读取Secret 作为文件。\nvolumeMounts:- name:secret-volumemountPath:/etc/secret-volume任务 使用 kubectl create -f secret-pod.yaml 创建我们的新 Pod 。\ncontrolplane $ kubectl create -f secret-pod.yaml pod/secret-vol-pod created 启动后，您可以与安装的Secret 进行交互。例如，您可以列出所有可用的 Secret ，就好像它们是常规数据一样。\ncontrolplane $ kubectl exec -it secret-vol-pod ls /etc/secret-volume password username 读取文件允许我们访问解码的 Secret 值。要访问我们使用的用户名\ncontrolplane $ kubectl exec -it secret-vol-pod cat /etc/secret-volume/username admin 对于密码，我们会读取密码文件。\nadmincontrolplane $ kubectl exec -it secret-vol-pod cat /etc/secret-volume/password a62fjbd37942dcs ","date":"2021-07-23T14:54:00+08:00","image":"https://www.catfish.top/p/k8s-basic-10/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-10/","title":"Kubernetes初探（十）"},{"content":" Katacoda在线课：Running Stateful Services on Kubernetes\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 部署 NFS 服务器 NFS 是一种允许节点通过网络读 / 写数据的协议。该协议的工作原理是让主节点运行 NFS 守护程序并存储数据。此主节点使某些目录可通过网络使用。\n客户端访问通过驱动器挂载共享的主服务器。从应用程序的角度来看，它们正在写入本地磁盘。在背后，NFS 协议将其写入主服务器。\n任务 在此场景中，出于演示和学习目的，NFS 服务器的角色由自定义容器处理。容器通过 NFS 提供目录并将数据存储在容器内。在生产环境中，建议配置专用的 NFS Server。\n使用 docker run -d --net=host \\ --privileged --name nfs-server \\ katacoda/contained-nfs-server:centos7 \\ /exports/data-0001 /exports/data-0002 命令启动 NFS 服务器\ncontrolplane $ docker run -d --net=host \\ \u0026gt; --privileged --name nfs-server \\ \u0026gt; katacoda/contained-nfs-server:centos7 \\ \u0026gt; /exports/data-0001 /exports/data-0002 Unable to find image \u0026#39;katacoda/contained-nfs-server:centos7\u0026#39; locally centos7: Pulling from katacoda/contained-nfs-server 8d30e94188e7: Pull complete 2b2b27f1f462: Pull complete 133e63cf95fe: Pull complete Digest: sha256:5f2ea4737fe27f26be5b5cabaa23e24180079a4dce8d5db235492ec48c5552d1 Status: Downloaded newer image for katacoda/contained-nfs-server:centos7 65699a0a96bfc489fe2141a815ef12b03917f9bb667340b1be68dfe838d14bf3 NFS 服务器公开两个目录，data-0001 和 data-0002。在接下来的步骤中，这将用于存储数据。\n部署 Persistent Volume 为了让 Kubernetes 了解可用的 NFS 共享，它需要一个 PersistentVolume 配置。 PersistentVolume 支持不同的数据存储协议，例如 AWS EBS 卷、GCE 存储、OpenStack Cinder、Glusterfs 和 NFS。该配置提供了存储和 API 之间的抽象，从而实现了一致的体验。\n在使用 NFS 的情况下，一个 PersistentVolume 与一个 NFS 目录相关联。当容器使用完卷后，数据可以 保留 以备将来使用，也可以 回收，这意味着所有数据都将被删除。该策略由 persistentVolumeReclaimPolicy 选项定义。\nYAML 文件结构：\napiVersion:v1kind:PersistentVolumemetadata:name:\u0026lt;friendly-name\u0026gt;spec:capacity:storage:1GiaccessModes:- ReadWriteOnce- ReadWriteManypersistentVolumeReclaimPolicy:Recyclenfs:server:\u0026lt;server-name\u0026gt;path:\u0026lt;shared-path\u0026gt;该规范定义了关于 PersistentVolume 的额外元数据，包括有多少可用空间以及它是否具有读 / 写访问权限。\n任务 使用 cat nfs-0001.yaml nfs-0002.yaml 查看 YAML 文件的内容。\nnfs-0001.yaml\napiVersion:v1kind:PersistentVolumemetadata:name:nfs-0001spec:capacity:storage:2GiaccessModes:- ReadWriteOnce- ReadWriteManypersistentVolumeReclaimPolicy:Recyclenfs:server:172.17.0.45path:/exports/data-0001nfs-0002.yaml\napiVersion:v1kind:PersistentVolumemetadata:name:nfs-0002spec:capacity:storage:5GiaccessModes:- ReadWriteOnce- ReadWriteManypersistentVolumeReclaimPolicy:Recyclenfs:server:172.17.0.45path:/exports/data-0002创建两个新的 PersistentVolume 定义并指向两个可用的 NFS 共享。\ncontrolplane $ kubectl create -f nfs-0001.yaml persistentvolume/nfs-0001 created controlplane $ kubectl create -f nfs-0002.yaml persistentvolume/nfs-0002 created 创建后，使用 kubectl get pv 命令查看集群中的所有 PersistentVolumes\ncontrolplane $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs-0001 2Gi RWO,RWX Recycle Available 66s nfs-0002 5Gi RWO,RWX Recycle Available 61s 部署 Persistent Volume Claim 一旦 PersistentVolume 可用，应用程序就可以声明该卷供其使用。该声明旨在阻止应用程序意外写入同一卷并导致冲突和数据损坏。\nPersistent Volume Claim 指定了卷的要求。这包括所需的读/写访问和存储空间。一个例子如下：\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:claim-mysqlspec:accessModes:- ReadWriteOnceresources:requests:storage:3Gi任务 使用 cat pvc-mysql.yaml pvc-http.yaml 查看文件的内容\npvc-mysql.yaml\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:claim-mysqlspec:accessModes:- ReadWriteOnceresources:requests:storage:3Gipvc-http.yaml\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:claim-httpspec:accessModes:- ReadWriteOnceresources:requests:storage:1Gi为两个不同的应用程序创建两个声明。 MySQL Pod 将使用一个声明，另一个由 HTTP 服务器使用。\ncontrolplane $ kubectl create -f pvc-mysql.yaml persistentvolumeclaim/claim-mysql created controlplane $ kubectl create -f pvc-http.yaml persistentvolumeclaim/claim-http created 创建后，使用 kubectl get pvc 查看集群中的所有 PersistentVolumesClaims，将打印出声明映射到的卷。\ncontrolplane $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim-http Bound nfs-0001 2Gi RWO,RWX 53s claim-mysql Bound nfs-0002 5Gi RWO,RWX 56s Volume 的使用 定义部署后，它可以将自己分配给先前的声明。以下代码段定义了目录 /var/lib/mysql/data 的卷挂载，该目录映射到存储 mysql-persistent-storage。名为 mysql-persistent-storage 的存储映射到名为 claim-mysql 的声明中。\nspec:volumeMounts:- name:mysql-persistent-storagemountPath:/var/lib/mysql/datavolumes:- name:mysql-persistent-storagepersistentVolumeClaim:claimName:claim-mysql任务 使用以下命令查看 Pod 的定义。\ncontrolplane $ cat pod-mysql.yaml pod-www.yaml pod-mysql.yaml\napiVersion:v1kind:Podmetadata:name:mysqllabels:name:mysqlspec:containers:- name:mysqlimage:openshift/mysql-55-centos7env:- name:MYSQL_ROOT_PASSWORDvalue:yourpassword- name:MYSQL_USERvalue:wp_user- name:MYSQL_PASSWORDvalue:wp_pass- name:MYSQL_DATABASEvalue:wp_dbports:- containerPort:3306name:mysqlvolumeMounts:- name:mysql-persistent-storagemountPath:/var/lib/mysql/datavolumes:- name:mysql-persistent-storagepersistentVolumeClaim:claimName:claim-mysqlpod-http.yaml\napiVersion:v1kind:Podmetadata:name:wwwlabels:name:wwwspec:containers:- name:wwwimage:nginx:alpineports:- containerPort:80name:wwwvolumeMounts:- name:www-persistent-storagemountPath:/usr/share/nginx/htmlvolumes:- name:www-persistent-storagepersistentVolumeClaim:claimName:claim-http启动两个具有 Persistent Volume Claims 的新 Pod。当 Pod 开始允许应用程序像本地目录一样读/写时，卷被映射到正确的目录。\ncontrolplane $ kubectl create -f pod-mysql.yaml pod/mysql created controlplane $ kubectl create -f pod-www.yaml pod/www created 可以使用 kubectl get pods 开始查看 Pod 的状态\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE mysql 1/1 Running 0 27s www 1/1 Running 0 25s 如果 Persistent Volume Claim 未分配给 Persistent Volume，则 Pod 将处于 Pending 模式，直到它变为可用。在下一步中，我们将向卷读/写数据。\n数据读写 我们的 Pod 现在可以进行读/写。 MySQL 将所有数据库更改存储到 NFS 服务器，而 HTTP 服务器将从 NFS 驱动器提供静态服务。升级、重新启动或将容器移动到不同的机器时，数据仍然可以访问。\n要测试 HTTP 服务器，请编写一个“Hello World”index.html 主页。在这种情况下，我们知道 HTTP 目录将基于 data-0001，因为卷定义没有驱动足够的空间来满足 MySQL 大小要求。\ncontrolplane $ docker exec -it nfs-server bash -c \u0026#34;echo \u0026#39;Hello World\u0026#39; \u0026gt; /exports/data-0001/index.html\u0026#34; 根据 Pod 的 IP，访问 Pod 时，应该返回预期的响应。\ncontrolplane $ ip=$(kubectl get pod www -o yaml |grep podIP | awk \u0026#39;{split($0,a,\u0026#34;:\u0026#34;); print a[2]}\u0026#39;); echo $ip 10.32.0.6 controlplane $ curl $ip Hello World 更新数据 当 NFS 共享上的数据发生变化时，Pod 会读取新更新的数据。\ncontrolplane $ docker exec -it nfs-server bash -c \u0026#34;echo \u0026#39;Hello NFS World\u0026#39; \u0026gt; /exports/data-0001/index.html\u0026#34; controlplane $ curl $ip Hello NFS World 重建 Pod 因为使用远程 NFS 服务器存储数据，如果 Pod 或主机宕机，那么数据仍然可用。\n任务 删除 Pod 将导致它删除对任何持久卷的声明。新 Pod 可以获取并重新使用 NFS 共享。\npod-www2.yaml\napiVersion:v1kind:Podmetadata:name:www2labels:name:www2spec:containers:- name:www2image:nginx:alpineports:- containerPort:80name:www2volumeMounts:- name:www-persistent-storagemountPath:/usr/share/nginx/htmlvolumes:- name:www-persistent-storagepersistentVolumeClaim:claimName:claim-httpcontrolplane $ kubectl delete pod www pod \u0026#34;www\u0026#34; deleted controlplane $ kubectl create -f pod-www2.yaml pod/www2 created controlplane $ ip=$(kubectl get pod www2 -o yaml |grep podIP | awk \u0026#39;{split($0,a,\u0026#34;:\u0026#34;); print a[2]}\u0026#39;); curl $ip Hello NFS World 应用程序现在使用远程 NFS 进行数据存储。根据具体要求，这种相同的方法适用于其他存储引擎，例如 GlusterFS、AWS EBS、GCE 存储或 OpenStack Cinder。\n","date":"2021-07-23T14:23:00+08:00","image":"https://www.catfish.top/p/k8s-basic-9/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-9/","title":"Kubernetes初探（九）"},{"content":" Katacoda在线课：Liveness and Readiness Healthchecks\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，您将了解 Kubernetes 如何使用 Readiness and Liveness Probes 检查容器运行状况。\nReadiness Probe 检查应用是否准备好开始处理流量。此探针解决了容器已启动的问题，但该进程仍在预热和配置自身，这意味着它尚未准备好接收流量。\nLiveness Probe 确保应用程序健康并能够处理请求。\n启动集群 首先，我们需要启动一个 Kubernetes 集群。\n执行以下命令启动集群组件并下载 Kubectl CLI\ncontrolplane $ launch.sh Waiting for Kubernetes to start... Kubernetes started 集群启动后，使用 kubectl apply -f deploy.yaml 部署演示应用程序。\ndeploy.yaml\nkind:ListapiVersion:v1items:- kind:ReplicationControllerapiVersion:v1metadata:name:frontendlabels:name:frontendspec:replicas:1selector:name:frontendtemplate:metadata:labels:name:frontendspec:containers:- name:frontendimage:katacoda/docker-http-server:healthreadinessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1livenessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1- kind:ReplicationControllerapiVersion:v1metadata:name:bad-frontendlabels:name:bad-frontendspec:replicas:1selector:name:bad-frontendtemplate:metadata:labels:name:bad-frontendspec:containers:- name:bad-frontendimage:katacoda/docker-http-server:unhealthyreadinessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1livenessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1- kind:ServiceapiVersion:v1metadata:labels:app:frontendkubernetes.io/cluster-service:\u0026#34;true\u0026#34;name:frontendspec:type:NodePortports:- port:80nodePort:30080selector:app:frontendcontrolplane $ kubectl apply -f deploy.yaml replicationcontroller/frontend created replicationcontroller/bad-frontend created service/frontend created Readiness Probe 在部署集群时，还部署了两个 Pod 来演示健康检查。您可以使用 cat deploy.yaml 查看部署。\nlivenessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1可以根据您的应用程序更改设置来调用不同的端点，例如 /ping。\n获取状态 第一个 Pod bad-frontend 是一个 HTTP 服务，它总是返回 500 错误，表明它没有正确启动。您可以使用 kubectl get pods --selector=\u0026quot;name=bad-frontend\u0026quot; 命令查看 Pod 的状态。\ncontrolplane $ kubectl get pods --selector=\u0026#34;name=bad-frontend\u0026#34; NAME READY STATUS RESTARTS AGE bad-frontend-jk5z2 0/1 Running 2 78s Kubectl 将返回使用我们的特定标签部署的 Pod。因为健康检查失败，它会显示没有容器为准备就绪的状态，同时它还将显示容器的重启尝试次数。\n要了解有关失败原因的更多详细信息，请对该 Pod 使用描述命令。\ncontrolplane $ pod=$(kubectl get pods --selector=\u0026#34;name=bad-frontend\u0026#34; --output=jsonpath={.items..metadata.name}) controlplane $ kubectl describe pod $pod Name: bad-frontend-jk5z2 Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: controlplane/172.17.0.67 Start Time: Fri, 23 Jul 2021 05:54:59 +0000 Labels: name=bad-frontend Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.32.0.6 Controlled By: ReplicationController/bad-frontend Containers: bad-frontend: Container ID: docker://811c3fa6d76c13f4b5bc7a6b2d6f514292e9673be46572a79b8f2d3d5e36bc62 Image: katacoda/docker-http-server:unhealthy Image ID: docker-pullable://katacoda/docker-http-server@sha256:bea95c69c299c690103c39ebb3159c39c5061fee1dad13aa1b0625e0c6b52f22 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 2 Started: Fri, 23 Jul 2021 05:57:07 +0000 Finished: Fri, 23 Jul 2021 05:57:37 +0000 Ready: False Restart Count: 4 Liveness: http-get http://:80/ delay=1s timeout=1s period=10s #success=1 #failure=3 Readiness: http-get http://:80/ delay=1s timeout=1s period=10s #success=1 #failure=3 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5n24z (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: default-token-5n24z: Type: Secret (a volume populated by a Secret) SecretName: default-token-5n24z Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m53s default-scheduler Successfully assigned default/bad-frontend-jk5z2 to controlplane Normal Pulling 2m52s kubelet, controlplane Pulling image \u0026#34;katacoda/docker-http-server:unhealthy\u0026#34; Normal Pulled 2m45s kubelet, controlplane Successfully pulled image \u0026#34;katacoda/docker-http-server:unhealthy\u0026#34; Normal Created 105s (x3 over 2m45s) kubelet, controlplane Created container bad-frontend Normal Started 105s (x3 over 2m45s) kubelet, controlplane Started container bad-frontend Warning Unhealthy 105s (x6 over 2m35s) kubelet, controlplane Liveness probe failed: HTTP probe failed with statuscode: 500 Normal Killing 105s (x2 over 2m15s) kubelet, controlplane Container bad-frontend failed liveness probe, will be restarted Normal Pulled 105s (x2 over 2m15s) kubelet, controlplane Container image \u0026#34;katacoda/docker-http-server:unhealthy\u0026#34; already present on machine Warning Unhealthy 103s (x7 over 2m43s) kubelet, controlplane Readiness probe failed: HTTP probe failed with statuscode: 500 Readiness 我们的第二个 Pod，frontend，在启动时返回 OK 状态。\ncontrolplane $ kubectl get pods --selector=\u0026#34;name=frontend\u0026#34; NAME READY STATUS RESTARTS AGE frontend-54czd 1/1 Running 0 4m9s Liveness Probe 由于我们的第二个 Pod 当前处于健康状态，我们可以模拟发生的故障。\n目前，该 Pod 应该没有发生崩溃。\ncontrolplane $ kubectl get pods --selector=\u0026#34;name=frontend\u0026#34; NAME READY STATUS RESTARTS AGE frontend-54czd 1/1 Running 0 7m17s 让服务崩溃 HTTP 服务器有一个额外的端点，这将导致它返回 500 错误。使用 kubectl exec 可以调用端点。\ncontrolplane $ pod=$(kubectl get pods --selector=\u0026#34;name=frontend\u0026#34; --output=jsonpath={.items..metadata.name}) controlplane $ kubectl exec $pod -- /usr/bin/curl -s localhost/unhealthy Liveness Kubernetes 将根据配置执行 Liveness Probe。如果探测器失败，Kubernetes 将销毁并重新创建失败的容器。执行上面的命令使服务崩溃并观察 Kubernetes 自动恢复它。\ncontrolplane $ kubectl get pods --selector=\u0026#34;name=frontend\u0026#34; NAME READY STATUS RESTARTS AGE frontend-54czd 1/1 Running 1 9m42s 检查可能需要一些时间才能检测到。\n","date":"2021-07-23T13:07:00+08:00","image":"https://www.catfish.top/p/k8s-basic-8/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-8/","title":"Kubernetes初探（八）"},{"content":" Katacoda在线课：Create Ingress Routing\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n Kubernetes 具有先进的网络功能，允许 Pod 和 Service 在集群网络内部进行通信。 Ingress 开启了到集群的入站连接，允许外部流量到达正确的 Pod。\nIngress 能够提供外部可访问的 URL、负载平衡流量、终止 SSL、为 Kubernetes 集群提供基于名称的虚拟主机。\n在此场景中，您将学习如何部署和配置 Ingress 规则来管理传入的 HTTP 请求。\n创建 Deployment 首先，部署一个示例 HTTP 服务器，它将成为我们请求的目标。部署中包含三个部署，分别为 webapp1， webapp2， webapp3，每个部署都有一个服务。\ndeployment.yaml\napiVersion:apps/v1kind:Deploymentmetadata:name:webapp1spec:replicas:1selector:matchLabels:app:webapp1template:metadata:labels:app:webapp1spec:containers:- name:webapp1image:katacoda/docker-http-server:latestports:- containerPort:80---apiVersion:apps/v1kind:Deploymentmetadata:name:webapp2spec:replicas:1selector:matchLabels:app:webapp2template:metadata:labels:app:webapp2spec:containers:- name:webapp2image:katacoda/docker-http-server:latestports:- containerPort:80---apiVersion:apps/v1kind:Deploymentmetadata:name:webapp3spec:replicas:1selector:matchLabels:app:webapp3template:metadata:labels:app:webapp3spec:containers:- name:webapp3image:katacoda/docker-http-server:latestports:- containerPort:80---apiVersion:v1kind:Servicemetadata:name:webapp1-svclabels:app:webapp1spec:ports:- port:80selector:app:webapp1---apiVersion:v1kind:Servicemetadata:name:webapp2-svclabels:app:webapp2spec:ports:- port:80selector:app:webapp2---apiVersion:v1kind:Servicemetadata:name:webapp3-svclabels:app:webapp3spec:ports:- port:80selector:app:webapp3任务 使用 kubectl apply -f deployment.yaml 命令部署 YAML 定义。\ncontrolplane $ kubectl apply -f deployment.yaml deployment.apps/webapp1 created deployment.apps/webapp2 created deployment.apps/webapp3 created service/webapp1-svc created service/webapp2-svc created service/webapp3-svc created 可以用 kubectl get deployment 查看状态。\ncontrolplane $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE webapp1 1/1 1 1 15s webapp2 1/1 1 1 15s webapp3 1/1 1 1 15s controlplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 7m41s webapp1-svc ClusterIP 10.98.236.192 \u0026lt;none\u0026gt; 80/TCP 41s webapp2-svc ClusterIP 10.105.201.139 \u0026lt;none\u0026gt; 80/TCP 41s webapp3-svc ClusterIP 10.103.159.185 \u0026lt;none\u0026gt; 80/TCP 41s 部署 Ingress ingress.yaml 文件中定义了一个基于 Nginx 的 Ingress 控制器以及一个 Service ，开放 80 端口用于使用 External IPs 的外部连接。如果 Kubernetes 集群在云服务商上运行，那么它将为 LoadBalancer 服务类型。\nServiceAccount 定义了带有权限的、连接集群以操作定义的入口规则的一组的帐户。默认服务器密钥是其他 Nginx 示例 SSL 连接的自签名证书，并且是Nginx Default Example 中所必须的。\ningress.yaml\napiVersion:v1kind:Namespacemetadata:name:nginx-ingress---apiVersion:v1kind:Secretmetadata:name:default-server-secretnamespace:nginx-ingresstype:kubernetes.io/tlsdata:tls.crt:LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=tls.key: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---apiVersion:v1kind:ServiceAccountmetadata:name:nginx-ingress namespace:nginx-ingress---kind:ConfigMapapiVersion:v1metadata:name:nginx-confignamespace:nginx-ingressdata:---# Described at: https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/# Source from: https://github.com/nginxinc/kubernetes-ingress/blob/master/deployments/common/ingress-class.yamlapiVersion:networking.k8s.io/v1beta1kind:IngressClassmetadata:name:nginx# annotations:# ingressclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34;spec:controller:nginx.org/ingress-controller---apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-ingressnamespace:nginx-ingressspec:replicas:1selector:matchLabels:app:nginx-ingresstemplate:metadata:labels:app:nginx-ingressspec:serviceAccountName:nginx-ingresscontainers:- image:nginx/nginx-ingress:edgeimagePullPolicy:Alwaysname:nginx-ingressports:- name:httpcontainerPort:80- name:httpscontainerPort:443env:- name:POD_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespace- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.nameargs:- -nginx-configmaps=$(POD_NAMESPACE)/nginx-config- -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret---apiVersion:v1kind:Servicemetadata:name:nginx-ingressnamespace:nginx-ingressspec:type:NodePort ports:- port:80targetPort:80protocol:TCPname:http- port:443targetPort:443protocol:TCPname:httpsselector:app:nginx-ingressexternalIPs:- 172.17.0.46任务 Ingress 控制器以同样的方式部署到 Kubernetes 集群，使用 kubectl create -f ingress.yaml命令操作。\ncontrolplane $ kubectl create -f ingress.yaml namespace/nginx-ingress created secret/default-server-secret created serviceaccount/nginx-ingress created configmap/nginx-config created ingressclass.networking.k8s.io/nginx created deployment.apps/nginx-ingress created service/nginx-ingress created 可以使用 kubectl get deployment -n nginx-ingress 查看状态。\ncontrolplane $ kubectl get deployment -n nginx-ingress NAME READY UP-TO-DATE AVAILABLE AGE nginx-ingress 1/1 1 1 35s 部署 Ingress Rules Ingress Rules 是 Kubernetes 的对象类型。规则可以基于请求主机（域），或请求的路径，或两者的组合。\ncat ingress-rules.yaml 中定义了一组示例规则。\ningress-rules.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: webapp-ingress spec: ingressClassName: nginx rules: - host: my.kubernetes.example http: paths: - path: /webapp1 backend: serviceName: webapp1-svc servicePort: 80 - path: /webapp2 backend: serviceName: webapp2-svc servicePort: 80 - backend: serviceName: webapp3-svc servicePort: 80 规则的重要部分定义如下。\n这些规则适用于对主机 my.kubernetes.example 的请求。基于路径请求定义了两个规则，并使用一个 catch all 定义。对路径 /webapp1 的请求被转发到服务 webapp1-svc。同样，对 /webapp2 的请求被转发到 webapp2-svc。如果没有规则适用，将使用 webapp3-svc。\n这演示了应用的 URL 结构如何独立于应用程序的部署方式。\n任务 与所有 Kubernetes 对象一样，它们可以通过 kubectl create -f ingress-rules.yaml 进行部署。\ncontrolplane $ kubectl create -f ingress-rules.yaml ingress.extensions/webapp-ingress created 部署后，可以通过 kubectl get ing 查看所有 Ingress 规则的状态\ncontrolplane $ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE webapp-ingress nginx my.kubernetes.example 80 41s 测试 应用入口规则后，流量将被路由到定义的位置。\n第一个请求将由 webapp1 处理。\ncontrolplane $ curl -H \u0026#34;Host: my.kubernetes.example\u0026#34; 172.17.0.68/webapp1 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-7c456784b7-cg6sk\u0026lt;/h1\u0026gt; 第二个请求将由 webapp2 处理。\ncontrolplane $ curl -H \u0026#34;Host: my.kubernetes.example\u0026#34; 172.17.0.68/webapp2 \u0026lt;h1\u0026gt;This request was processed by host: webapp2-79f9947d45-25j6l\u0026lt;/h1\u0026gt; 最后，所有的其他请求将由 webapp3 处理。\ncontrolplane $ curl -H \u0026#34;Host: my.kubernetes.example\u0026#34; 172.17.0.68 \u0026lt;h1\u0026gt;This request was processed by host: webapp3-777f4dd675-n2pqs\u0026lt;/h1\u0026gt; ","date":"2021-07-20T15:30:00+08:00","image":"https://www.catfish.top/p/k8s-basic-7/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-7/","title":"Kubernetes初探（七）"},{"content":" Katacoda在线课：Networking Introduction\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n Kubernetes 具有先进的网络功能，允许 Pod 和 Service 在集群网络内部和外部进行通信。\n在此场景中，您将学习以下类型的 Kubernetes Service。\n Cluster IP Target Ports NodePort External IPs Load Balancer  Kubernetes 服务是一个抽象，它定义了如何访问一组 Pod 的策略和方法。可以通过 Service 访问基于标签选择器的 Pod 集合。\nCluster IP Cluster IP 是创建 Kubernetes 服务时的默认方法。该服务被分配了一个内部 IP，其他组件可以使用它来访问 Pod。\nService 能够通过单个 IP 地址在多个 Pod 之间进行负载平衡。\n通过 kubectl apply -f clusterip.yaml 命令部署服务。\n在 cat clusterip.yaml 可以查看相关定义。\nclusterip.yaml\napiVersion:v1kind:Servicemetadata:name:webapp1-clusterip-svclabels:app:webapp1-clusteripspec:ports:- port:80selector:app:webapp1-clusterip---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:webapp1-clusterip-deploymentspec:replicas:2template:metadata:labels:app:webapp1-clusteripspec:containers:- name:webapp1-clusterip-podimage:katacoda/docker-http-server:latestports:- containerPort:80---controlplane $ kubectl apply -f clusterip.yaml service/webapp1-clusterip-svc created deployment.extensions/webapp1-clusterip-deployment created 该文件定义了部署具有两个副本的 Web 应用程序，用来展示负载平衡和服务。 可以通过 kubectl get pods 命令查看 Pod 状态。\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE webapp1-clusterip-deployment-669c7c65c4-r6k2l 1/1 Running 0 15s webapp1-clusterip-deployment-669c7c65c4-xdsd4 1/1 Running 0 15s 同时也部署了一个 Service，可以通过 kubectl get svc 命令查看。\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 8m12s webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 39s 可以通过 kubectl describe svc/webapp1-clusterip-svc 命令查看有关服务配置和活动端点 (Pod) 的更多详细信息\ncontrolplane $ kubectl describe svc/webapp1-clusterip-svc Name: webapp1-clusterip-svc Namespace: default Labels: app=webapp1-clusterip Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-clusterip\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-clusterip-svc\u0026#34;,\u0026#34;name... Selector: app=webapp1-clusterip Type: ClusterIP IP: 10.108.32.169 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP Endpoints: 10.32.0.5:80,10.32.0.6:80 Session Affinity: None Events: \u0026lt;none\u0026gt; 部署完成后，可以通过分配的 Cluster IP 访问该服务。\ncontrolplane $ export CLUSTER_IP=$(kubectl get services/webapp1-clusterip-svc -o go-template=\u0026#39;{{(index .spec.clusterIP)}}\u0026#39;) controlplane $ echo CLUSTER_IP=$CLUSTER_IP CLUSTER_IP=10.108.32.169 controlplane $ curl $CLUSTER_IP:80 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-deployment-669c7c65c4-r6k2l\u0026lt;/h1\u0026gt; 将通过多个请求来展示基于公共标签选择器的跨多个* Pod* 的负载均衡器。\ncontrolplane $ curl $CLUSTER_IP:80 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-deployment-669c7c65c4-xdsd4\u0026lt;/h1\u0026gt; controlplane $ curl $CLUSTER_IP:80 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-deployment-669c7c65c4-r6k2l\u0026lt;/h1\u0026gt; Target Port TargetPort 允许我们将服务可用的端口与应用程序正在侦听的端口分开。 TargetPort 是应用配置为侦听的端口。端口是从外部访问应用的方式。\n与之前类似，Service 和额外的 Pod 是通过 kubectl apply -f clusterip-target.yaml 命令来部署的。\n**clusterip-target.yaml **\napiVersion:v1kind:Servicemetadata:name:webapp1-clusterip-targetport-svclabels:app:webapp1-clusterip-targetportspec:ports:- port:8080targetPort:80selector:app:webapp1-clusterip-targetport---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:webapp1-clusterip-targetport-deploymentspec:replicas:2template:metadata:labels:app:webapp1-clusterip-targetportspec:containers:- name:webapp1-clusterip-targetport-podimage:katacoda/docker-http-server:latestports:- containerPort:80---以下命令将创建服务。\ncontrolplane $ kubectl apply -f clusterip-target.yaml service/webapp1-clusterip-targetport-svc created deployment.extensions/webapp1-clusterip-targetport-deployment created controlplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 13m webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 5m29s webapp1-clusterip-targetport-svc ClusterIP 10.102.59.206 \u0026lt;none\u0026gt; 8080/TCP 40s controlplane $ kubectl describe svc/webapp1-clusterip-targetport-svc Name: webapp1-clusterip-targetport-svc Namespace: default Labels: app=webapp1-clusterip-targetport Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-clusterip-targetport\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-clusterip... Selector: app=webapp1-clusterip-targetport Type: ClusterIP IP: 10.102.59.206 Port: \u0026lt;unset\u0026gt; 8080/TCP TargetPort: 80/TCP Endpoints: 10.32.0.7:80,10.32.0.8:80 Session Affinity: None Events: \u0026lt;none\u0026gt; Service 和 Pod 部署完成后，可以像以前一样通过 Cluster IP 访问，但这次是在定义的端口 8080 上。\ncontrolplane $ export CLUSTER_IP=$(kubectl get services/webapp1-clusterip-targetport-svc -o go-template=\u0026#39;{{(index .spec.clusterIP)}}\u0026#39;) controlplane $ echo CLUSTER_IP=$CLUSTER_IP CLUSTER_IP=10.102.59.206 controlplane $ curl $CLUSTER_IP:8080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-targetport-deployment-5599945ff4-96fqh\u0026lt;/h1\u0026gt; controlplane $ curl $CLUSTER_IP:8080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-targetport-deployment-5599945ff4-zwr8f\u0026lt;/h1\u0026gt; 应用程序本身仍然配置为侦听 80 端口。Kubernetes 服务管理着两者之间的转换。\nNodePort 虽然 TargetPort 和 ClusterIP 使其可用于集群内部，但 NodePort 通过定义的静态端口在每个节点的 IP 上公开服务。无论访问集群内的哪个节点，都可以通过定义的端口号直接访问该服务。\ncontrolplane $ kubectl apply -f nodeport.yaml service/webapp1-nodeport-svc created deployment.extensions/webapp1-nodeport-deployment created 查看服务定义时，注意定义的附加类型和 NodePort 属性。\nnodeport.yaml\napiVersion:v1kind:Servicemetadata:name:webapp1-nodeport-svclabels:app:webapp1-nodeportspec:type:NodePortports:- port:80nodePort:30080selector:app:webapp1-nodeport---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:webapp1-nodeport-deploymentspec:replicas:2template:metadata:labels:app:webapp1-nodeportspec:containers:- name:webapp1-nodeport-podimage:katacoda/docker-http-server:latestports:- containerPort:80---controlplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 22m webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 14m webapp1-clusterip-targetport-svc ClusterIP 10.102.59.206 \u0026lt;none\u0026gt; 8080/TCP 9m44s webapp1-nodeport-svc NodePort 10.102.135.99 \u0026lt;none\u0026gt; 80:30080/TCP 5m52s controlplane $ kubectl describe svc/webapp1-nodeport-svc Name: webapp1-nodeport-svc Namespace: default Labels: app=webapp1-nodeport Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-nodeport\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-nodeport-svc\u0026#34;,\u0026#34;namesp... Selector: app=webapp1-nodeport Type: NodePort IP: 10.102.135.99 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 30080/TCP Endpoints: 10.32.0.10:80,10.32.0.9:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; 可以通过 NodePort 定义的 Node 的 IP 地址访问到服务。\ncontrolplane $ curl 172.17.0.31:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-nodeport-deployment-677bd89b96-j256b\u0026lt;/h1\u0026gt; controlplane $ curl 172.17.0.31:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-nodeport-deployment-677bd89b96-88qj5\u0026lt;/h1\u0026gt; External IP 另一种方法是通过外部 IP 地址让服务在集群外可用。\n将定义中的 externalIPs 更新为当前集群的 IP 地址\ncontrolplane $ sed -i \u0026#39;s/HOSTIP/172.17.0.31/g\u0026#39; externalip.yaml externalip.yaml\napiVersion: v1 kind: Service metadata: name: webapp1-externalip-svc labels: app: webapp1-externalip spec: ports: - port: 80 externalIPs: - 172.17.0.30 selector: app: webapp1-externalip --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: webapp1-externalip-deployment spec: replicas: 2 template: metadata: labels: app: webapp1-externalip spec: containers: - name: webapp1-externalip-pod image: katacoda/docker-http-server:latest ports: - containerPort: 80 --- controlplane $ cat externalip.yaml apiVersion: v1 kind: Service metadata: name: webapp1-externalip-svc labels: app: webapp1-externalip spec: ports: - port: 80 externalIPs: - 172.17.0.30 selector: app: webapp1-externalip --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: webapp1-externalip-deployment spec: replicas: 2 template: metadata: labels: app: webapp1-externalip spec: containers: - name: webapp1-externalip-pod image: katacoda/docker-http-server:latest ports: - containerPort: 80 --- controlplane $ kubectl apply -f externalip.yaml service/webapp1-externalip-svc created deployment.extensions/webapp1-externalip-deployment created controlplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 28m webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 20m webapp1-clusterip-targetport-svc ClusterIP 10.102.59.206 \u0026lt;none\u0026gt; 8080/TCP 15m webapp1-externalip-svc ClusterIP 10.109.28.108 172.17.0.31 80/TCP 8s webapp1-nodeport-svc NodePort 10.102.135.99 \u0026lt;none\u0026gt; 80:30080/TCP 12m controlplane $ kubectl describe svc/webapp1-externalip-svc Name: webapp1-externalip-svc Namespace: default Labels: app=webapp1-externalip Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-externalip\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-externalip-svc\u0026#34;,\u0026#34;na... Selector: app=webapp1-externalip Type: ClusterIP IP: 10.109.28.108 External IPs: 172.17.0.31 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP Endpoints: 10.32.0.11:80,10.32.0.12:80 Session Affinity: None Events: \u0026lt;none\u0026gt; 该服务现在绑定到主节点的 IP 地址和 80端口 。\ncontrolplane $ curl 172.17.0.31 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-externalip-deployment-6446b488f8-dxdb5\u0026lt;/h1\u0026gt; controlplane $ curl 172.17.0.31 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-externalip-deployment-6446b488f8-2zw2p\u0026lt;/h1\u0026gt; Load Balancer 在例如 EC2 或 Azure 的云服务中运行时，可以通过云服务商配置和分配发布的公网 IP 地址。这将通过负载均衡器（例如 ELB）发出，允许将额外的公网 IP 地址分配给 Kubernetes 集群，而无需直接与云提供商交互。\n由于 Katacoda 不是云服务商，因此仍然可以为 LoadBalancer 类型的服务动态分配 IP 地址。这是通过使用 kubectl apply -f cloudprovider.yaml 部署 Cloud Provider 来完成的。并不是必需在云服务商提供的服务中运行的。\ncloudprovider.yaml\napiVersion:extensions/v1beta1kind:DaemonSetmetadata:name:kube-keepalived-vipnamespace:kube-systemspec:template:metadata:labels:name:kube-keepalived-vipspec:hostNetwork:truecontainers:- image:gcr.io/google_containers/kube-keepalived-vip:0.9name:kube-keepalived-vipimagePullPolicy:AlwayssecurityContext:privileged:truevolumeMounts:- mountPath:/lib/modulesname:modulesreadOnly:true- mountPath:/devname:dev# use downward APIenv:- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:POD_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespace# to use unicastargs:- --services-configmap=kube-system/vip-configmap# unicast uses the ip of the nodes instead of multicast# this is useful if running in cloud providers (like AWS)#- --use-unicast=truevolumes:- name:moduleshostPath:path:/lib/modules- name:devhostPath:path:/devnodeSelector:# type: worker # adjust this to match your worker nodes---## We also create an empty ConfigMap to hold our configapiVersion:v1kind:ConfigMapmetadata:name:vip-configmapnamespace:kube-systemdata:---apiVersion:apps/v1beta1kind:Deploymentmetadata:labels:app:keepalived-cloud-providername:keepalived-cloud-providernamespace:kube-systemspec:replicas:1revisionHistoryLimit:2selector:matchLabels:app:keepalived-cloud-providerstrategy:type:RollingUpdatetemplate:metadata:annotations:scheduler.alpha.kubernetes.io/critical-pod:\u0026#34;\u0026#34;scheduler.alpha.kubernetes.io/tolerations:\u0026#39;[{\u0026#34;key\u0026#34;:\u0026#34;CriticalAddonsOnly\u0026#34;, \u0026#34;operator\u0026#34;:\u0026#34;Exists\u0026#34;}]\u0026#39;labels:app:keepalived-cloud-providerspec:containers:- name:keepalived-cloud-providerimage:quay.io/munnerz/keepalived-cloud-provider:0.0.1imagePullPolicy:IfNotPresentenv:- name:KEEPALIVED_NAMESPACEvalue:kube-system- name:KEEPALIVED_CONFIG_MAPvalue:vip-configmap- name:KEEPALIVED_SERVICE_CIDRvalue:10.10.0.0/26# pick a CIDR that is explicitly reserved for keepalivedvolumeMounts:- name:certsmountPath:/etc/ssl/certsresources:requests:cpu:200mlivenessProbe:httpGet:path:/healthzport:10252host:127.0.0.1initialDelaySeconds:15timeoutSeconds:15failureThreshold:8volumes:- name:certshostPath:path:/etc/ssl/certs---controlplane $ kubectl apply -f cloudprovider.yaml daemonset.extensions/kube-keepalived-vip configured configmap/vip-configmap configured deployment.apps/keepalived-cloud-provider created 当服务请求负载均衡时，负载均衡的提供商将从配置中定义的 10.10.0.0/26 范围中分配一个 IP。\ncontrolplane $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-fb8b8dccf-6rvjq 1/1 Running 0 39m coredns-fb8b8dccf-p5lxm 1/1 Running 0 39m etcd-controlplane 1/1 Running 0 38m katacoda-cloud-provider-66d7758d5d-bn748 1/1 Running 0 39m keepalived-cloud-provider-78fc4468b-bk8wn 1/1 Running 0 6m16s kube-apiserver-controlplane 1/1 Running 0 39m kube-controller-manager-controlplane 1/1 Running 2 39m kube-keepalived-vip-42ql5 1/1 Running 0 38m kube-proxy-d5q4m 1/1 Running 0 39m kube-scheduler-controlplane 1/1 Running 3 39m weave-net-9rc2g 2/2 Running 1 39m 该服务是通过负载均衡配置的，如 cat loadbalancer.yaml 中所定义所示。\nloadbalancer.yaml\napiVersion:v1kind:Servicemetadata:name:webapp1-loadbalancer-svclabels:app:webapp1-loadbalancerspec:type:LoadBalancerports:- port:80selector:app:webapp1-loadbalancer---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:webapp1-loadbalancer-deploymentspec:replicas:2template:metadata:labels:app:webapp1-loadbalancerspec:containers:- name:webapp1-loadbalancer-podimage:katacoda/docker-http-server:latestports:- containerPort:80controlplane $ kubectl apply -f loadbalancer.yaml service/webapp1-loadbalancer-svc created deployment.extensions/webapp1-loadbalancer-deployment created 在定义 IP 地址时，服务将显示 Pending。分配后，它将出现在服务列表中。\ncontrolplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 38m webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 31m webapp1-clusterip-targetport-svc ClusterIP 10.102.59.206 \u0026lt;none\u0026gt; 8080/TCP 26m webapp1-externalip-svc ClusterIP 10.109.28.108 172.17.0.31 80/TCP 10m webapp1-loadbalancer-svc LoadBalancer 10.105.60.108 172.17.0.31 80:30626/TCP 7s webapp1-nodeport-svc NodePort 10.102.135.99 \u0026lt;none\u0026gt; 80:30080/TCP 22m controlplane $ kubectl describe svc/webapp1-loadbalancer-svc Name: webapp1-loadbalancer-svc Namespace: default Labels: app=webapp1-loadbalancer Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-loadbalancer\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-loadbalancer-svc\u0026#34;... Selector: app=webapp1-loadbalancer Type: LoadBalancer IP: 10.105.60.108 LoadBalancer Ingress: 172.17.0.31 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 30626/TCP Endpoints: 10.32.0.14:80,10.32.0.15:80 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CreatingLoadBalancer 95s service-controller Creating load balancer Normal CreatedLoadBalancer 95s service-controller Created load balancer 现在可以通过分配的 IP 地址访问该服务，在本例中的范围是 10.10.0.0/26 。\ncontrolplane $ export LoadBalancerIP=$(kubectl get services/webapp1-loadbalancer-svc -o go-template=\u0026#39;{{(index .status.loadBalancer.ingress 0).ip}}\u0026#39;) controlplane $ echo LoadBalancerIP=$LoadBalancerIP LoadBalancerIP=172.17.0.31 controlplane $ curl $LoadBalancerIP \u0026lt;h1\u0026gt;This request was processed by host: webapp1-externalip-deployment-6446b488f8-2zw2p\u0026lt;/h1\u0026gt; controlplane $ curl $LoadBalancerIP \u0026lt;h1\u0026gt;This request was processed by host: webapp1-externalip-deployment-6446b488f8-dxdb5\u0026lt;/h1\u0026gt; ","date":"2021-07-20T13:36:00+08:00","image":"https://www.catfish.top/p/k8s-basic-6/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-6/","title":"Kubernetes初探（六）"},{"content":" Katacoda在线课：Deploy Guestbook Web App Example\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 本场景说明了如何使用 Kubernetes 和 Docker 启动简单的多层 Web 应用。留言簿示例应用程序通过调用* JavaScript API* 将访客的笔记存储在 *Redis* 中。 *Redis* 包含一个 master（用于存储）和一组 *slave* 复制集。\n核心概念 在此场景中将涵盖以下核心概念。这些是理解 Kubernetes 的基础。\n Pods Replication Controllers Services NodePorts  启动 Kubernetes 首先，我们需要一个正在运行的 Kubernetes 集群。详细信息在 Launch Kubernetes cluster\n任务 使用初始化程序脚本启动单节点集群。初始化脚本将启动 API、Master、Proxy 和 DNS Discovery。 Web App 使用 DNS Discovery 来查找 Redis slave 来存储数据。\nlaunch.sh 健康检查 使用 kubectl cluster-info 和 kubectl get nodes命令来检查部署的集群的节点健康信息。\ncontrolplane $ kubectl cluster-info Kubernetes master is running at https://172.17.0.81:6443 KubeDNS is running at https://172.17.0.81:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. controlplane $ kubectl get nodes NAME STATUS ROLES AGE VERSION controlplane Ready master 2m15s v1.14.0 node01 Ready \u0026lt;none\u0026gt; 114s v1.14.0 如果节点返回 NotReady，则它仍在等待。请等待几秒钟后再重新启动。\nRedis Master - Replication Controller 启动应用的第一步是启动Redis Master。 Kubernetes 服务部署至少有两个部分，分别为 Replication Controller 和 Service。\nReplication Controller 定义了运行的实例数量、要使用的 Docker 镜像以及服务标识名称。其他选项可用于配置和发现。使用上面的编辑器查看 YAML 定义。\n如果 Redis 出现故障，Replication Controller 将在活动节点上重新启动它。\n创建 Replication Controller redis-master-controller.yaml\napiVersion:v1kind:ReplicationControllermetadata:name:redis-masterlabels:name:redis-masterspec:replicas:1selector:name:redis-mastertemplate:metadata:labels:name:redis-masterspec:containers:- name:masterimage:redis:3.0.7-alpineports:- containerPort:6379在本示例中，YAML 使用官方 redis 设置端口 6379 运行了一个名为 redis-master 的 redis 服务器。\nkubectl create 命令采用 YAML 定义并指示 master 启动控制器。\ncontrolplane $ kubectl create -f redis-master-controller.yaml replicationcontroller/redis-master created 查看运行的组件 上面的命令创建了一个 Replication Controller 。\ncontrolplane $ kubectl get rc NAME DESIRED CURRENT READY AGE redis-master 1 1 1 2m9s 所有容器都被描述为 Pod。 Pod 是构成特定应用程序（例如 Redis）的容器集合。可以使用 kubectl 查看 Pod 信息。\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE redis-master-qkfxw 1/1 Running 0 2m54s Redis Master - Service 第二步是 Service。 Kubernetes Service 是一种命名负载均衡器，它将流量代理到一个或多个容器。即使容器位于不同的节点上，代理也能工作。\n服务代理在集群内通信，很少将端口暴露给外部接口。\n当启动服务时，似乎无法使用 curl 或 netcat 进行连接，除非=将其作为 Kubernetes 的一部分启动。推荐的方法是使用 LoadBalancer 服务来处理外部通信。\nredis-master-service.yaml\napiVersion:v1kind:Servicemetadata:name:redis-masterlabels:name:redis-masterspec:ports:# the port that this service should serve on- port:6379targetPort:6379selector:name:redis-master创建 Service YAML 定义了 Service 的名称redis-master，以及应该被代理的端口。\ncontrolplane $ kubectl create -f redis-master-service.yaml service/redis-master created 查看 Service 列表与详情 bash controlplane $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 10m redis-master ClusterIP 10.100.28.180 6379/TCP 43s\ncontrolplane $ kubectl describe services redis-master Name: redis-master Namespace: default Labels: name=redis-master Annotations: Selector: name=redis-master Type: ClusterIP IP: 10.100.28.180 Port: 6379/TCP TargetPort: 6379/TCP Endpoints: 10.32.0.193:6379 Session Affinity: None Events:  ## Redis Slave Pods 在本示例中，我们将运行 *Redis Slaves*，它会从 *master* 复制数据。有关 *Redis* 复制的更多详细信息，请访问 [http://redis.io/topics/replication](http://redis.io/topics/replication) 如前所述，*Controller* 定义了 *Service* 的运行方式。在这个例子中，我们需要确定 *Service* 是如何发现其他 *Pod*。 *YAML* 将 *GET_HOSTS_FROM* 属性表示为 *DNS*。您可以将其更改为在 *YAML* 中使用环境变量，但这会引入创建顺序依赖关系，因为需要运行服务才能定义环境变量。 **redis-slave-controller.yaml** ```yaml apiVersion: v1 kind: ReplicationController metadata: name: redis-slave labels: name: redis-slave spec: replicas: 2 selector: name: redis-slave template: metadata: labels: name: redis-slave spec: containers: - name: worker image: gcr.io/google_samples/gb-redisslave:v1 env: - name: GET_HOSTS_FROM value: dns # If your cluster config does not include a dns service, then to # instead access an environment variable to find the master # service's host, comment out the 'value: dns' line above, and # uncomment the line below. # value: env ports: - containerPort: 6379 启动 Redis Slave Controller 在这种情况下，我们将使用镜像 kubernetes/redis-slave:v2 启动 Pod 的两个实例。它将通过 DNS 链接到 redis-master。\ncontrolplane $ kubectl create -f redis-slave-controller.yaml replicationcontroller/redis-slave created 列出 Replication Controller NAME DESIRED CURRENT READY AGE redis-master 1 1 1 11m redis-slave 2 2 2 26s Redis Slave Service 和以前一样，我们需要启动一个与 redis-slave 通信的Service来让Slave能够接收传入的请求。\n因为我们有两个复制的 Pod，该服务还将在两个节点之间提供负载平衡。\nredis-slave-service.yaml\napiVersion:v1kind:Servicemetadata:name:redis-slavelabels:name:redis-slavespec:ports:# the port that this service should serve on- port:6379selector:name:redis-slaveStart Redis Slave Service controlplane $ kubectl create -f redis-slave-service.yaml service/redis-slave created controlplane $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2m20s redis-master ClusterIP 10.100.144.249 \u0026lt;none\u0026gt; 6379/TCP 46s redis-slave ClusterIP 10.105.27.32 \u0026lt;none\u0026gt; 6379/TCP 16s 前端 Replicated Pods 启动数据服务后，我们现在可以部署 Web 应用。部署 Web 应用的模式与我们之前部署的 Pod 相同。\n启动前端 YAML 定义了一个名为 frontend 的服务，该服务使用 gcr.io/google_samples/gb-frontend:v3 的镜像。Replication Controller 将确保三个 Pod 始终存在。\nfrontend-controller.yaml\napiVersion:v1kind:ReplicationControllermetadata:name:frontendlabels:name:frontendspec:replicas:3selector:name:frontendtemplate:metadata:labels:name:frontendspec:containers:- name:php-redisimage:gcr.io/google_samples/gb-frontend:v3env:- name:GET_HOSTS_FROMvalue:dns# If your cluster config does not include a dns service, then to# instead access environment variables to find service host# info, comment out the \u0026#39;value: dns\u0026#39; line above, and uncomment the# line below.# value: envports:- containerPort:80controlplane $ kubectl create -f frontend-controller.yaml replicationcontroller/frontend created 列出 Controllers 和 Pods controlplane $ kubectl get rc NAME DESIRED CURRENT READY AGE frontend 3 3 3 34s redis-master 1 1 1 8m20s redis-slave 2 2 2 8m6s controlplane $ kubectl get pods NAME READY STATUS RESTARTS AGE frontend-88lvf 1/1 Running 0 42s frontend-phhhv 1/1 Running 0 42s frontend-rq8tt 1/1 Running 0 42s redis-master-7h9t2 1/1 Running 0 8m28s redis-slave-gnhfh 1/1 Running 0 8m14s redis-slave-mtn2r 1/1 Running 0 8m14s PHP 代码 PHP 代码使用 HTTP 和 JSON 与 Redis 通信。当设置一个值时，请求转到 redis-master，而读取的数据来自 redis-slave 节点。\n用户手册前端 Service 为了能够访问前端，我们需要启动一个 Service 来配置代理。\n启动代理 YAML 将服务定义为 NodePort 。 NodePort 允许您设置在整个集群中共享的特点端口，这就像 Docker 中的 -p 80:80。\n在这种情况下，我们定义我们的 Web 应用在端口 80 上运行，在 30080 上开放服务。\nfrontend-service.yaml\napiVersion:v1kind:Servicemetadata:name:frontendlabels:name:frontendspec:# if your cluster supports it, uncomment the following to automatically create# an external load-balanced IP for the frontend service.# type: LoadBalancertype:NodePortports:# the port that this service should serve on- port:80nodePort:30080selector:name:frontendcontrolplane $ kubectl create -f frontend-service.yaml service/frontend created controlplane $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend NodePort 10.98.211.171 \u0026lt;none\u0026gt; 80:30080/TCP 3s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 13m redis-master ClusterIP 10.100.144.249 \u0026lt;none\u0026gt; 6379/TCP 11m redis-slave ClusterIP 10.105.27.32 \u0026lt;none\u0026gt; 6379/TCP 11m 我们将在未来的场景中讨论 NodePort 。\n访问用户手册前端 定义了所有 Controller 和 Service 后，Kubernetes 将开始将它们作为 Pod 启动。根据实际发生的情况，Pod 可以具有不同的状态。例如，如果 Docker 映像仍在下载无法启动，则 Pod 将处于 pending 状态。准备就绪后，状态将会变更为 running。\n查看 Pods 状态 可以使用下列命令查看所有 Pod 的状态。\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE frontend-88lvf 1/1 Running 0 9m3s frontend-phhhv 1/1 Running 0 9m3s frontend-rq8tt 1/1 Running 0 9m3s redis-master-7h9t2 1/1 Running 0 16m redis-slave-gnhfh 1/1 Running 0 16m redis-slave-mtn2r 1/1 Running 0 16m 查找 NodePort 如果您没有分配 众所周知（well-known） 的 NodePort，那么 Kubernetes 将随机分配一个可用端口。您可以使用 kubectl 找到分配的 NodePort。\ncontrolplane $ kubectl describe service frontend | grep NodePort Type: NodePort NodePort: \u0026lt;unset\u0026gt; 30080/TCP 查看 UI 一旦 Pod 处于 running 状态，您将能够通过端口 30080 查看 UI。使用 URL 查看页面 https://2886795308-30080-kitek05.environments.katacoda.com\n在背后（Under the covers） PHP 服务通过 DNS 发现 Redis 实例。您现在已经在 Kubernetes 上部署了一个有效的多层应用程序。\n","date":"2021-07-20T13:18:00+08:00","image":"https://www.catfish.top/p/k8s-basic-5/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-5/","title":"Kubernetes初探（五）"},{"content":" Katacoda在线课：Deploy Containers Using YAML\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，您将学习如何使用 Kubectl 创建和启动 Deployment、Replication Controller，并通过编写 yaml 定义使用服务开暴露它们。\nYAML 定义了计划部署的 Kubernetes 对象。可以以更新对象并将其重新部署到集群的方式来更改配置。\n创建 Deployment Deployment 对象是最常见的 Kubernetes 对象之一。Deployment 对象定义了所需的容器规范，以及 Kubernetes 其他部分用来发现和连接到应用程序的名称和标签。\n任务 将以下定义复制到编辑器的YAML文件中。该 YAML 定义了如何使用在端口 80 上运行的应用，该应用使用 Docker 映像 katacoda/docker-http-server ，启动名为 webapp1 。\ndeployment.yaml\napiVersion:apps/v1kind:Deploymentmetadata:name:webapp1spec:replicas:1selector:matchLabels:app:webapp1template:metadata:labels:app:webapp1spec:containers:- name:webapp1image:katacoda/docker-http-server:latestports:- containerPort:80使用 kubectl create -f deployment.yaml 命令向集群部署。\n$ kubectl create -f deployment.yaml deployment.apps/webapp1 created 由于它是一个 Deployment 对象，因此可以通过 kubectl get deployment 获取所有已部署的 Deployment 对象的列表。\n$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE webapp1 1/1 1 1 10s 可以使用 kubectl describe deployment webapp1 输出单个部署的详细信息。\n$ kubectl describe deployment webapp1 Name: webapp1 Namespace: default CreationTimestamp: Wed, 21 Jul 2021 09:31:47 +0000 Labels: \u0026lt;none\u0026gt; Annotations: deployment.kubernetes.io/revision: 1 Selector: app=webapp1 Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=webapp1 Containers: webapp1: Image: katacoda/docker-http-server:latest Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: webapp1-6b54fb89d9 (1/1 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 49s deployment-controller Scaled up replica set webapp1-6b54fb89d9 to 1 创建Service Kubernetes 具有强大的网络功能，可以控制应用程序的通信方式。这些网络配置也可以通过 YAML 定义。\n任务 将 Service 定义复制到编辑器。该 Service 选择带有标签 webapp1 的所有应用程序。当部署多个副本或实例时，它们将根据此公共标签自动进行负载平衡。该 Service 通过 NodePort 的网络连接方式部署。\nservice.yaml\napiVersion:v1kind:Servicemetadata:name:webapp1-svclabels:app:webapp1spec:type:NodePortports:- port:80nodePort:30080selector:app:webapp1所有 Kubernetes 对象都一致使用 kubectl 完成部署。\n使用 kubectl create -f service.yaml 命令部署 Service\n$ kubectl create -f service.yaml service/webapp1-svc created 和以往一样，使用 kubectl get svc 可以查看所有已部署的 Service 对象的信息。通过 kubectl describe svc webapp1-svc 命令可以查看该 Service 对象更多的配置信息。\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 6m38s webapp1-svc NodePort 10.111.56.111 \u0026lt;none\u0026gt; 80:30080/TCP 20s $ kubectl describe svc webapp1-svc Name: webapp1-svc Namespace: default Labels: app=webapp1 Annotations: \u0026lt;none\u0026gt; Selector: app=webapp1 Type: NodePort IP: 10.111.56.111 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 30080/TCP Endpoints: 172.18.0.4:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-5rzpv\u0026lt;/h1\u0026gt; 缩放 Deployment 由于 Deployment 需要不同的配置，因此可以更改 YAML 的详细信息。这遵循基础设施即代码的思维方式。清单应在源代码控制下，并且保证生产环境的配置和源代码控制中的配置一致。\n任务 更新 deployment.yaml 文件以增加运行的实例数。例如，该文件应如下所示：\nreplicas:1# --\u0026gt; replicas:4使用 kubectl apply 提交对现有定义的更新。要扩展副本数量，请使用 kubectl apply -f deployment.yaml 部署更新的 YAML 文件\n$ kubectl apply -f deployment.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply deployment.apps/webapp1 configured 集群的状态将立即更新，可通过 kubectl get deployment 命令 查看\n$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE webapp1 4/4 4 4 15m 根据定义将有额外的 Pods 被调度到集群中， 使用 kubectl get pods 查看 Pod 信息\n$ kubectl get pods NAME READY STATUS RESTARTS AGE webapp1-6b54fb89d9-5rzpv 1/1 Running 0 16m webapp1-6b54fb89d9-ktvz4 1/1 Running 0 2m15s webapp1-6b54fb89d9-kzdjw 1/1 Running 0 2m15s webapp1-6b54fb89d9-x8vtv 1/1 Running 0 2m15s 由于所有 Pod 具有相同的标签选择器，因此将由已部署的 Service NodePort 为这些 Pod 提供负载平衡能力。\n向目标端口发出多个请求，可以通过返回信息发现，请求是由多个容器进行处理的。\n$ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-5rzpv\u0026lt;/h1\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-kzdjw\u0026lt;/h1\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-kzdjw\u0026lt;/h1\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-x8vtv\u0026lt;/h1\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-ktvz4\u0026lt;/h1\u0026gt; 其他 Kubernetes 网络细节和对象定义将在未来的其他场景中介绍。\n","date":"2021-07-20T11:39:00+08:00","image":"https://www.catfish.top/p/k8s-basic-4/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-4/","title":"Kubernetes初探（四）"},{"content":" Katacoda在线课：Deploy Containers Using Kubectl\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，将学习如何使用 Kubectl 创建和启动 Deployment 和 Replication Controllers 并通过 Services 对外暴露接口。本场景中无需编写 yaml 定义，便可以快速将容器启动到集群上。\n启动集群 首先，我们需要启动一个 Kubernetes 集群。\n执行以下命令启动集群并下载 Kubectl CLI。\n$ minikube start --wait=false * minikube v1.8.1 on Ubuntu 18.04 * Using the none driver based on user configuration * Running on localhost (CPUs=2, Memory=2460MB, Disk=145651MB) ... * OS release is Ubuntu 18.04.4 LTS * Preparing Kubernetes v1.17.3 on Docker 19.03.6 ... - kubelet.resolv-conf=/run/systemd/resolve/resolv.conf * Launching Kubernetes ... * Enabling addons: default-storageclass, storage-provisioner * Configuring local host environment ... * Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; 通过 kubectl get nodes 命令来检查节点是否准备就绪。\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 19s v1.17.3 Kubectl Run Run 命令根据指定的参数（例如映像或副本）创建部署。该部署发布给 Kubernetes 主节点，启动所需 Pod 和容器的 。 Kubectl run 类似于 docker run，但 Kubectl run 是在集群中运行的。\n命令的格式为 kubectl run \u0026lt;name of deployment\u0026gt; \u0026lt;properties\u0026gt;\n任务 以下命令将启动一个名为 http 的部署，它将启动一个基于 Docker 镜像 katacoda/docker-http-server:latest 的容器。\n$ kubectl run http --image=katacoda/docker-http-server:latest --replicas=1 kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. deployment.apps/http created 使用 kubectl 查看各部署的状态。\n$ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE http 1/1 1 1 2m7s 可以通过 describe 查看 Kubernetes 创建了哪些内容。\n$ kubectl describe deployment http Name: http Namespace: default CreationTimestamp: Wed, 21 Jul 2021 08:04:55 +0000 Labels: run=http Annotations: deployment.kubernetes.io/revision: 1 Selector: run=http Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: run=http Containers: http: Image: katacoda/docker-http-server:latest Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: http-774bb756bb (1/1 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 4m16s deployment-controller Scaled up replica set http-774bb756bb to 1 描述中包含有多少副本可用、指定的标签以及与部署相关的事件等信息。这些事件将突出显示可能发生的任何问题和错误。\n在下一步中，我们将开放正在运行的服务。\nKubectl Expose 创建部署后，我们可以使用 kubectl 创建一个服务，该服务在特定端口上开放 Pod。\n通过 kubectl expose 开放新部署的 http 部署。该命令允许定义服务的不同参数以及如何开放该部署。\n任务 使用以下命令将主机上的容器端口 80 公开 8000 绑定到主机的 external-ip。\n$ kubectl expose deployment http --external-ip=\u0026#34;172.17.0.44\u0026#34; --port=8000 --target-port=80 service/http exposed 接下来就能 ping 主机查看 HTTP 服务返回的结果。\n$ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-m6tjp\u0026lt;/h1\u0026gt; Kubectl Run and Expose 可以单独使用命令 kubectl run 来创建部署并将其开放。\n任务 使用命令创建在端口 8001 上开放的第二个 http 服务。\n$ kubectl run httpexposed --image=katacoda/docker-http-server:latest --replicas=1 --port=80 --hostport=8001 kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. deployment.apps/httpexposed created 接下来可以使用 curl http://172.17.0.41:8001 来访问该服务。\n$ curl http://172.17.0.44:8001 \u0026lt;h1\u0026gt;This request was processed by host: httpexposed-68cb8c8d4-r7qtt\u0026lt;/h1\u0026gt; 在服务内，通过 Docker 端口映射开放了 Pod。因此，您将看不到使用 kubectl get svc 列出的服务\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE http ClusterIP 10.97.103.237 172.17.0.44 8000/TCP 4m47s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 44m 可以通过 docker ps | grep httpexposed 命令查看详细信息。\n$ docker ps | grep httpexposed 1690c6682f93 katacoda/docker-http-server \u0026#34;/app\u0026#34; 2 minutes ago Up 2 minutes k8s_httpexposed_httpexposed-68cb8c8d4-r7qtt_default_6fb23c14-277e-4298-b325-c52ef4eb09a9_0 d9ae0076a0d4 k8s.gcr.io/pause:3.1 \u0026#34;/pause\u0026#34; 2 minutes ago Up 2 minutes 0.0.0.0:8001-\u0026gt;80/tcp k8s_POD_httpexposed-68cb8c8d4-r7qtt_default_6fb23c14-277e-4298-b325-c52ef4eb09a9_0 暂停容器 运行上面的命令，你会注意到端口暴露在 Pod 上，而不是 http 容器本身。 Pause 容器负责为 Pod 定义网络。 Pod 中的其他容器共享相同的网络命名空间。允许多个容器通过同一网络接口进行通信，提高了网络性能。\n容器扩展 随着我们的部署运行，我们现在可以使用 kubectl 来扩展副本的数量。\n扩展部署将请求 Kubernetes 启动额外的 Pod。然后，这些 Pod 将使用开放服务自动进行负载平衡。\n任务 kubectl scale 命令能够为特定 Deployment 或 Replication Controller 调整运行的 Pod 数量。\n$ kubectl scale --replicas=3 deployment http deployment.apps/http scaled 列出所有 pod，能够看到三个正在运行的 http 部署。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE http-774bb756bb-m6tjp 1/1 Running 0 20m http-774bb756bb-q29v8 1/1 Running 0 45s http-774bb756bb-x2xxv 1/1 Running 0 45s httpexposed-68cb8c8d4-r7qtt 1/1 Running 0 9m3s 一旦每个 Pod 启动，就会被添加到负载均衡器服务中。通过 describe，可以查看包含的端点和与之关联的 Pod。\n$ kubectl describe svc http Name: http Namespace: default Labels: run=http Annotations: \u0026lt;none\u0026gt; Selector: run=http Type: ClusterIP IP: 10.97.103.237 External IPs: 172.17.0.44 Port: \u0026lt;unset\u0026gt; 8000/TCP TargetPort: 80/TCP Endpoints: 172.18.0.4:80,172.18.0.6:80,172.18.0.7:80 Session Affinity: None Events: \u0026lt;none\u0026gt; 向服务发出多次请求，可以通过返回信息看出请求将在不同的节点处理。\n$ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-x2xxv\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-q29v8\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-q29v8\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-x2xxv\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-q29v8\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-q29v8\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-m6tjp\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-x2xxv\u0026lt;/h1\u0026gt; ","date":"2021-07-20T11:27:00+08:00","image":"https://www.catfish.top/p/k8s-basic-3/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-3/","title":"Kubernetes初探（三）"},{"content":" Katacoda在线课：Launch a multi-node cluster using Kubeadm\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，您将学习如何使用 Kubeadm 启动 Kubernetes 集群。\nKubeadm 解决了TLS 加密配置、 Kubernetes 核心组件部署和额外节点集群加入的问题。启动的集群通过 RBAC 等机制开箱即用。\n关于 Kubeadm 的更多信息可以参考： https://github.com/kubernetes/kubeadm\n初始化 Master Kubeadm 已经安装在节点上。软件包适用于 Ubuntu 16.04+、CentOS 7 或 HypriotOS v1.0.1+。\n初始化集群的第一步是启动 Master节点 。 Master节点 负责运行控制平面组件、etcd 和 API 服务器。客户端能够与 API 通信，能够完成工作负载的调度和集群状态的管理。\n任务 下面的命令将使用已知的 Token 简化初始化集群的步骤。\ncontrolplane $ kubeadm init --token=102952.1a7dd4cc8d1f4cc5 --kubernetes-version $(kubeadm version -o short) [init] Using Kubernetes version: v1.14.0 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [controlplane localhost] and IPs [172.17.0.86 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [controlplane localhost] and IPs [172.17.0.86 127.0.0.1 ::1] [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [controlplane kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.17.0.86] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 17.503676 seconds [upload-config] storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config-1.14\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --experimental-upload-certs [mark-control-plane] Marking the node controlplane as control-plane by adding the label \u0026#34;node-role.kubernetes.io/master=\u0026#39;\u0026#39;\u0026#34; [mark-control-plane] Marking the node controlplane as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: 102952.1a7dd4cc8d1f4cc5 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.17.0.86:6443 --token 102952.1a7dd4cc8d1f4cc5 \\  --discovery-token-ca-cert-hash sha256:ab56a643a2d683bc1deeb483f0f946d4a774c4 在生产环境中，建议排除使用 kubeadm 生成的令牌。\n需要客户端配置和证书来管理 Kubernetes 集群。这个配置是在 kubeadm 初始化集群时创建的。该命令将配置复制到用户主目录并设置用于 CLI 的环境变量。\ncontrolplane $ sudo cp /etc/kubernetes/admin.conf $HOME/ controlplane $ sudo chown $(id -u):$(id -g) $HOME/admin.conf controlplane $ export KUBECONFIG=$HOME/admin.conf 部署容器网络接口 Container Networking Interface (CNI) 容器网络接口 ( CNI ) 定义了不同节点及其工作负载是如何通信的。有多个网络提供商可用，其中一些在 here 中列出。\n任务 在这个场景中，我们将使用 WeaveWorks 的 CNI 。\n/opt/weave-kube.yaml 可以通过 cat /opt/weave-kube.yaml 命令查看部署定义。\napiVersion:v1kind:Listitems:- apiVersion:v1kind:ServiceAccountmetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netnamespace:kube-system- apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netrules:- apiGroups:- \u0026#39;\u0026#39;resources:- pods- namespaces- nodesverbs:- get- list- watch- apiGroups:- networking.k8s.ioresources:- networkpoliciesverbs:- get- list- watch- apiGroups:- \u0026#39;\u0026#39;resources:- nodes/statusverbs:- patch- update- apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netroleRef:kind:ClusterRolename:weave-netapiGroup:rbac.authorization.k8s.iosubjects:- kind:ServiceAccountname:weave-netnamespace:kube-system- apiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netnamespace:kube-systemrules:- apiGroups:- \u0026#39;\u0026#39;resourceNames:- weave-netresources:- configmapsverbs:- get- update- apiGroups:- \u0026#39;\u0026#39;resources:- configmapsverbs:- create- apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netnamespace:kube-systemroleRef:kind:Rolename:weave-netapiGroup:rbac.authorization.k8s.iosubjects:- kind:ServiceAccountname:weave-netnamespace:kube-system- apiVersion:apps/v1kind:DaemonSetmetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netnamespace:kube-systemspec:minReadySeconds:5selector:matchLabels:name:weave-nettemplate:metadata:labels:name:weave-netspec:containers:- name:weavecommand:- /home/weave/launch.shenv:- name:IPALLOC_RANGEvalue:10.32.0.0/24- name:HOSTNAMEvalueFrom:fieldRef:apiVersion:v1fieldPath:spec.nodeNameimage:\u0026#39;docker.io/weaveworks/weave-kube:2.6.0\u0026#39;readinessProbe:httpGet:host:127.0.0.1path:/statusport:6784resources:requests:cpu:10msecurityContext:privileged:truevolumeMounts:- name:weavedbmountPath:/weavedb- name:cni-binmountPath:/host/opt- name:cni-bin2mountPath:/host/home- name:cni-confmountPath:/host/etc- name:dbusmountPath:/host/var/lib/dbus- name:lib-modulesmountPath:/lib/modules- name:xtables-lockmountPath:/run/xtables.lock- name:weave-npcenv:- name:HOSTNAMEvalueFrom:fieldRef:apiVersion:v1fieldPath:spec.nodeNameimage:\u0026#39;docker.io/weaveworks/weave-npc:2.6.0\u0026#39;resources:requests:cpu:10msecurityContext:privileged:truevolumeMounts:- name:xtables-lockmountPath:/run/xtables.lockhostNetwork:truehostPID:truerestartPolicy:AlwayssecurityContext:seLinuxOptions:{}serviceAccountName:weave-nettolerations:- effect:NoScheduleoperator:Existsvolumes:- name:weavedbhostPath:path:/var/lib/weave- name:cni-binhostPath:path:/opt- name:cni-bin2hostPath:path:/home- name:cni-confhostPath:path:/etc- name:dbushostPath:path:/var/lib/dbus- name:lib-moduleshostPath:path:/lib/modules- name:xtables-lockhostPath:path:/run/xtables.locktype:FileOrCreateupdateStrategy:type:RollingUpdate使用 kubectl apply 命令来完成部署工作。.\ncontrolplane $ kubectl apply -f /opt/weave-kube.yaml serviceaccount/weave-net created clusterrole.rbac.authorization.k8s.io/weave-net created clusterrolebinding.rbac.authorization.k8s.io/weave-net created role.rbac.authorization.k8s.io/weave-net created rolebinding.rbac.authorization.k8s.io/weave-net created daemonset.apps/weave-net created Weave 现在将在集群上部署为一系列的 Pod。 可以通过 kubectl get pod -n kube-system 命令查看状态。\ncontrolplane $ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-fb8b8dccf-gw8z6 1/1 Running 0 12m coredns-fb8b8dccf-qzd49 1/1 Running 0 12m etcd-controlplane 1/1 Running 0 12m kube-apiserver-controlplane 1/1 Running 0 11m kube-controller-manager-controlplane 1/1 Running 0 11m kube-proxy-2kjsl 1/1 Running 0 13m kube-scheduler-controlplane 1/1 Running 1 11m weave-net-2dtbs 2/2 Running 0 82s 需要安装 Weave 在你的集群中时，可以在 https://www.weave.works/docs/net/latest/kube-addon/ 找到更多详细信息。\n加入集群 一旦 Master 和 CNI 初始化完成，其他节点只要拥有正确的令牌就可以加入集群。令牌可以通过 kubeadm token 进行管理，例如 kubeadm token list。\ncontrolplane $ kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 102952.1a7dd4cc8d1f4cc5 23h 2021-07-21T10:00:20Z authentication,signing The default bootstrap token generated by \u0026#39;kubeadm init\u0026#39;. system:bootstrappers:kubeadm:default-node-token 任务 在第二个节点上，使用主节点的 IP 地址，运行命令加入集群。\nnode01 $ kubeadm join --discovery-token-unsafe-skip-ca-verification --token=102952.1a7dd4cc8d1f4cc5 172.17.0.86:6443 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.14\u0026quot; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. Node01节点与 Master 节点初始化提供的命令相同。\n--discovery-token-unsafe-skip-ca-verification 标签用于绕过 Discovery Token 验证。由于此令牌是动态生成的，因此我们无法将其包含在步骤中。在生产环境中，使用 kubeadm init 提供的令牌。\n查看节点 集群现已初始化。主节点将负责管理集群，另一个工作节点将负责运行我们的容器工作负载。\n任务 Kubernetes CLI，称为 kubectl，现在可以使用配置访问集群。例如，下面的命令将返回我们集群中的两个节点信息。\ncontrolplane $ kubectl get nodes NAME STATUS ROLES AGE VERSION controlplane Ready master 25m v1.14.0 node01 Ready \u0026lt;none\u0026gt; 2m v1.14.0 部署 Pod 集群中两个节点的状态现在应该是 Ready。这表示接下来可以进行调度和启动部署。\n使用 Kubectl，可以部署 Pod。命令总是由 Master 发出，每个节点只负责运行工作负载。\n下面的命令是基于 Docker 镜像 katacoda/docker-http-server 创建一个 Pod。\ncontrolplane $ kubectl create deployment http --image=katacoda/docker-http-server:latest deployment.apps/http created 可以使用kubectl get pods查看Pod创建的状态\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE http-7f8cbdf584-jcdrj 1/1 Running 0 70s 运行后，可以看到节点上Docker 容器的运行状态。\nnode01 $ docker ps | grep docker-http-server d874d8c3151b katacoda/docker-http-server \u0026quot;/app\u0026quot; About a minute ago Up About a minute k8s_docker-http-server_http-7f8cbdf584-jcdrj_default_2894ce10-e945-11eb-b87f-0242ac110056_0 部署仪表盘 Kubernetes 有一个基于 Web 的仪表板应用，提供对 Kubernetes 集群的查看与管理能力。\n任务 使用 kubectl apply -f dashboard.yaml 命令部署仪表板。\ncontrolplane $ kubectl apply -f dashboard.yaml secret/kubernetes-dashboard-certs created serviceaccount/kubernetes-dashboard created role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created deployment.apps/kubernetes-dashboard created service/kubernetes-dashboard created 仪表板将部署到 kube-system 命名空间中。使用 kubectl get pods -n kube-system命令 查看部署状态。\ncontrolplane $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-fb8b8dccf-rjzxp 1/1 Running 1 88s coredns-fb8b8dccf-tw8cm 1/1 Running 1 88s etcd-controlplane 1/1 Running 0 16s kube-apiserver-controlplane 1/1 Running 0 23s kube-controller-manager-controlplane 1/1 Running 1 61s kube-proxy-6xv6k 1/1 Running 0 84s kube-proxy-fn84g 1/1 Running 0 88s kube-scheduler-controlplane 1/1 Running 2 61s kubernetes-dashboard-5f57845f9d-jblx7 1/1 Running 0 5s weave-net-6gcbd 2/2 Running 1 88s weave-net-8s6bt 2/2 Running 1 84s 需要 ServiceAccount 才能登录。 ClusterRoleBinding 用于为新的 ServiceAccount (admin-user) 分配集群上的 cluster-admin 角色。\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system EOF Dashboard可以控制 Kubernetes 的所有方面。通过 ClusterRoleBinding 和 RBAC，可以根据安全要求来定义不同级别的权限。有关为仪表板创建用户的更多信息，请参考 Dashboard documentation。\n创建 ServiceAccount 后，可以通过以下方式获取登录令牌：\ncontrolplane $ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \u0026#39;{print $1}\u0026#39;) Name: attachdetach-controller-token-bltlp Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: attachdetach-controller kubernetes.io/service-account.uid: 0ced9bf4-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhdHRhY2hkZXRhY2gtY29udHJvbGxlci10b2tlbi1ibHRscCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhdHRhY2hkZXRhY2gtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBjZWQ5YmY0LWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphdHRhY2hkZXRhY2gtY29udHJvbGxlciJ9.x0a2m2SpBrwj8sARrvRfuct2ghrDydxYzyFmDL93hATdS_59zaueB4SrgHner8gOu_zrx9PLWcSoZNBxRblZuxJj8Di9TKUTjyDBE6txc4_0H8nseuQzljvRTbjUjEcL7fp8H1j4MrJgT4GrYU-n1gAOl6NIfk8FmpFpuUUS6G_IfIDfS60YpZRlqZGp14NuaL0RC71PsERnP6ZYnuRKTbYNfVNeURqVR4pY7XwCcQZFALaJcTf9SgwJLAAqLQFgKd9c2MYnHdnU5cnqjrYxi_b5kGBKUcRzACrHjh0uGG6ErHeo6-GbA-0NDdIMI7qKa3If0oh_GOQKmyB5cyx-Rw Name: bootstrap-signer-token-cstqm Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: bootstrap-signer kubernetes.io/service-account.uid: 0dc1b223-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJib290c3RyYXAtc2lnbmVyLXRva2VuLWNzdHFtIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImJvb3RzdHJhcC1zaWduZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZGMxYjIyMy1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Ym9vdHN0cmFwLXNpZ25lciJ9.hkciM9KpWYYPyi5MJbm67oqzuskjDB_Rj4Fz5THhtN_jJAO6go4oxz-NPparOi7kxaEU0yBLb_xLY42jOXvG_Q2Dd6gR2u9yc7nfwr67koW3lck4S0PCYSuGo-FqUndXS1a3BpT5SFAHhb52FdwEYiedevLz9C5wUDT8sVgMYSnHvEX_jwBmjrhhOohGxTNs_-_HyGMNNLfEOSbjaEtzkDFos0n3qPAJ95uaGKJ2laDsctVpHCeRx5hNCv8E7dvvR1Qp4XXDVDQrodkzkuG4LSKviYLzMLX1DM2XES3HQ7Q2uTtAICqxIDBjRlhfAPv_z02tqAbCf4fF6_CFlYK1ag Name: bootstrap-token-102952 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: bootstrap.kubernetes.io/token Data ==== token-secret: 16 bytes usage-bootstrap-authentication: 4 bytes usage-bootstrap-signing: 4 bytes auth-extra-groups: 47 bytes description: 56 bytes expiration: 20 bytes token-id: 6 bytes Name: certificate-controller-token-mw9rl Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: certificate-controller kubernetes.io/service-account.uid: 0cf891ba-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjZXJ0aWZpY2F0ZS1jb250cm9sbGVyLXRva2VuLW13OXJsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNlcnRpZmljYXRlLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwY2Y4OTFiYS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y2VydGlmaWNhdGUtY29udHJvbGxlciJ9.NIZpVmhuJUcrpbZJApUt7ODgqIPU9RN4UZcr6he83xA_66-4ORQVddj1oQQg4G0p4fSPckWfP0n7pSxahZ90VZ-NCxPeCVLThLZ563PUOIljsjfROgFLQ4eeWfRML0OHyhAgoTtfEdcHi4_IzyhSrvCOweM9oPFfdx9MdTNAG9znyQsCi4g1YNHwn7t4r8h-BouW4yRYMEOM9HVZkcDVWhmF9PDv9s235INvIAjco5JvCAeDKvw48hdSrm4RWQPmZ7yE71iBDymDF_Ntr9w12H_4FPOYcARfFOmIj1k5binKiHaOlWtfbKYGYFM4tAxqI-ErFx4lshkLNJm3ICMJSQ Name: clusterrole-aggregation-controller-token-xh52b Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: clusterrole-aggregation-controller kubernetes.io/service-account.uid: 0acea7db-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjbHVzdGVycm9sZS1hZ2dyZWdhdGlvbi1jb250cm9sbGVyLXRva2VuLXhoNTJiIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNsdXN0ZXJyb2xlLWFnZ3JlZ2F0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYWNlYTdkYi1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y2x1c3RlcnJvbGUtYWdncmVnYXRpb24tY29udHJvbGxlciJ9.SgSxj_9uNLbAU9zYK4sTUYL4STx9CSYflGe0GwqGz_f3K7Db5NhpCuCoCcLYVJ7I2xogXICDJV016ZaNEAKJHWNJNeO-Ye378zjgddYmLAxX0uNw9OfSSiHKq-ksHgkKmoxwixgzVw2Df6PZGeYrKZZMSGuW96hkzd5JJx9Jhg2c4k8Fpqcpg5t-N4F5ZC8Ix4THpOmBueB8BaKox6OS5kUxqBAmdkJxZur1Wl8BvJV3Pnl6qbzYa0oPqun6N-WckGjVLrOlll1E6moABT1i8udZeRSrsX7YjjWgnbVyuyP0vzY7T1OQz-jcS0Lw3shMSIeANpwfFFB5XNvacsIV9A Name: coredns-token-z5tnd Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: coredns kubernetes.io/service-account.uid: 0ce34e5b-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjb3JlZG5zLXRva2VuLXo1dG5kIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNvcmVkbnMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwY2UzNGU1Yi1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y29yZWRucyJ9.vTVtFW7YZ-_dxjyiHjrkJnxyw3Q1c2oy43wEfs71tnD5GIDcXjioGXRn4OFvGJYzUzSM7fujZy6e6lXUDZBx-PkiRZzaqbxZTuY1skfTKdAm31h6xbOESSvuFcR97eASVMv5WNdFNSXZik4P3_pEoF7XUKnOnY-o3nk-MZNWRhmdcOvj-BhBmTqXY9CQDvaW-P2MSQhaGvP-yVYBFHqNvOH_XgaBVdW5c35AuLGz3pVAiBl9s_Ghrkc5h0KrhU_eko5YAMGcXHUQPask-EVy4MYes_3ube_PbQLTvTgVZ6XwMjPNIOkYvrGsZrRZbRCDXKqc0KBFi2YT_dQWaoiNgg ca.crt: 1025 bytes Name: cronjob-controller-token-6hfqf Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: cronjob-controller kubernetes.io/service-account.uid: 0e4b090e-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjcm9uam9iLWNvbnRyb2xsZXItdG9rZW4tNmhmcWYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiY3JvbmpvYi1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGU0YjA5MGUtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmNyb25qb2ItY29udHJvbGxlciJ9.SKICsFaSIGnrn9pfqWg_Vrw1jBvYz_bc95tJ53FL_M9b6YQKgHRQZZH1DQciGdSd27nYSWnoG9L-ftKEZhaU7ABAAga3lNzyDJhgPdYtCy8OlxzY2nY2RdDbCsFRjaK76aIJQf_u0z8K-JSz6CjELBj294WWbXoAreCtZiQGRrrupgtUIPjUloaqx07BO2Y29N2ZMkELj_Ye5rSeffnz_djSFdwTIt6B3Jtxjp55DZjMzaj_coqmVXfGq4K90wkKlWLIbbAGYlRqEfJCpIK2OeCy5WF2EahlAUECHEQi5NqEQ1DoiVUj1LtdwQOGKXlQYT6h6GSLY_E1Q1Xv3vWFjQ Name: daemon-set-controller-token-6ftrt Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: daemon-set-controller kubernetes.io/service-account.uid: 0e251630-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYWVtb24tc2V0LWNvbnRyb2xsZXItdG9rZW4tNmZ0cnQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFlbW9uLXNldC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGUyNTE2MzAtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhZW1vbi1zZXQtY29udHJvbGxlciJ9.OTdB75fb-wdKbbcZJ3mS08JuJsq3urdovVUgiADbpAZnL4pVZWdAmrLadYVKFNqfXOlCaSqQefLNgSWsGcJee-YSQT6snP87Wplfo7VH05h9Xwnoz21uWpkjhIlioWbcKL1asBuBsvpSczqg4Bt3cyByxdrIJjBmBvoJpFhwf04IqyJ6ugxDLPTH-8CC5r5Xn6etm8Ey-p1rwQJb1u2FEq8K97nihv87TByrHLQv-CMn1wxfGHDYOEAneis2s0pWotvxrHWkFg1WnriDAq4hryGPDA9GRLqveFdCL76XTuT9plYFYMxLRF-8_EuwOwTa0F-LYm98o3iDhbvwkTDhiA ca.crt: 1025 bytes Name: default-token-jw2ql Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 0fdb30ea-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLWp3MnFsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZmRiMzBlYS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.ijnMq7u2xRTNKXov89YF67O27Jn7uLqnRYzK4V95pWt22dN3Qayf7U-jlLm6uWec79FLKNM_08IjnobzVxnzrFhRqLqOT55mkyLfXdNdR9EE__CrPOs-PiSIsHNbAG64bUSDOwbGaFjKROFBz1dwHj1O_XQZfLNaH3_LhLzOHtxJ4_FyOOf-hhjEWgjOptptTMkU3N_z7TRU4zzX9BUWTDg5s6JBpFkDSbZ-_yu-uzl4JiEBUwf0MY3YC79caRp9uFgwuaMCoFJYV7gSqWA5IpqgQe225LBTBS2HHLPJE67WLlSrkiEe4gvngnGtO_1RkO2MBCXgVp5icSFw4zxEzw Name: deployment-controller-token-fctz8 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: deployment-controller kubernetes.io/service-account.uid: 0cd7cffc-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4tZmN0ejgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGNkN2NmZmMtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.uobekKRLj02hr-qsNZCPNZWER2mYiZj8NCygiXz5c_r9B7AjhHkq3MlcfPe1RoU89P3hofrNWQpTY1Kikl9nspKQ_Yve2ykZidGGGoOtuaqI2h-SfNMjeqHGXM0CmQAEigrmkQtDrkwt-lz3pXEacRbh5pmS8BOWvpgfc5e8qt39YebFTKygNvCYvMgYMy5MsufSDeifFcb7J12hhXczxvYLDcbgDewhEnpxA887KaQlCML8soNmM36bC5qEU3nrGnKg1a5kvjjxaxres7VRb4U9fs0xogLAzcAsuOd40Re7QY8wNdY7dcUg2qivFSm7P-3vbiZ9Twr1kftazuR6Gg Name: disruption-controller-token-x8n2f Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: disruption-controller kubernetes.io/service-account.uid: 0cf3a17c-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkaXNydXB0aW9uLWNvbnRyb2xsZXItdG9rZW4teDhuMmYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGlzcnVwdGlvbi1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGNmM2ExN2MtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRpc3J1cHRpb24tY29udHJvbGxlciJ9.mu_3XUNoOGZwHlTyhKygDwAFjIWY0Uf0foIiS4vGNsnBBe5AUz3bN7gD0f7EpRZoI7KeIL-OWYGLvjtsCgC-1lYCRm9DNLmNxYnoDiNcbpCFx-xHF4E4yl4v51oJtXG1Bc-Xva-S15US673Gzv-soVAfpVKOzYQVklM1cbim__Eb_vXEZ_i0r-KD1DRNERMZWjvJ159DuiKMjd6CCzkgXCSQV1K4jS2jd2taiARKUcGNw0rQBeHh1_IN460jnMGCLM0lzbcZZAA4YKhdMfsU2AjIlsvK4k6VS5G-w4lo3PA1JW4Ve-Z-Im94Lqd67A2MSqUhWLPw1cn4PGGyWTaiJA ca.crt: 1025 bytes Name: endpoint-controller-token-qbz7h Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: endpoint-controller kubernetes.io/service-account.uid: 0ab80371-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJlbmRwb2ludC1jb250cm9sbGVyLXRva2VuLXFiejdoIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImVuZHBvaW50LWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYWI4MDM3MS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZW5kcG9pbnQtY29udHJvbGxlciJ9.z4jN9B3eHKiw7RTSz-UM0RqGiPXX2-7bV9d6N-6hkfPDCfOj3BwjApxqc3FoDiPeazVSNT-cmORpgAP3b5UMYYnN5wNw2R4XfysUXfY46i1clQJqZ14o9olfX1KegC1T-yZOC8MoEHMFvngem5gEfQquW6cIM6hYQ-kgWHaYRZwH00OqPg9YchBpmmm7erx5PZXmlWmZ76lobAyi-nAzShqJA5mKl0amSwzfQfd7ErZJhsHJrWynqpJi_SSTRsDL2zYY79aiNEMaXe4nXa0rlyVNZmRRiaA3_ca665S840KTMjvLfub4fShxpRib8EEnRJOAJft6kyfCn6bf2Uf1Og Name: expand-controller-token-q7k8m Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: expand-controller kubernetes.io/service-account.uid: 0fc0ecea-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJleHBhbmQtY29udHJvbGxlci10b2tlbi1xN2s4bSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJleHBhbmQtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBmYzBlY2VhLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpleHBhbmQtY29udHJvbGxlciJ9.w0CjgPX7fKK1huV-Lt0ZRGLo5JgMiAR9yn11R0CrsIyDyPGRlwOrUoDSRxNn0t__Bq6YSjblPDAjz0hh2PToc44g4abe1aSfF2cXtps1y87Jee78jQ3rF9vGzinqPCJGmiXCZNvAunEAT88KkiaND74X3rHgPU-E0XjlNVyjlwMRUQWpA4Z7rqMTSb448L9vjFETbT4n7jdFib3iijeP8MgcKDxiZYpkKhQcsvvO-r-r6ndHRw7_eO06Yyl-nW20NALbc3CB8gl-1JLuFkB0CsVE2J8D3s8vkQ1rd0fMPo67cgqnc5jCULRgKWYoKOaGtcCc_UdxOg_P_9gS3zfhjA ca.crt: 1025 bytes Name: generic-garbage-collector-token-qkjc7 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: generic-garbage-collector kubernetes.io/service-account.uid: 0f46f27b-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJnZW5lcmljLWdhcmJhZ2UtY29sbGVjdG9yLXRva2VuLXFramM3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImdlbmVyaWMtZ2FyYmFnZS1jb2xsZWN0b3IiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZjQ2ZjI3Yi1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Z2VuZXJpYy1nYXJiYWdlLWNvbGxlY3RvciJ9.cVdg68sCtTFnHtZenBGyzJx5RRxPekXORa8L4et-5F5WVoAyvVLIc4g1kPEWgdrGQwAbXsgjowYEGZiZZM7VjHLTKpiHaYuYyFjPVtK-bBc4EuB4bPhX-5h0w5A5e1npDTqRHohIel9Q4Tx9bCfMGtzoP9C33Dog5kIFbNA1h45YaT5DUSIe9lnD8MbDbf6JoLLitB-jJdv9B7oscm7b-azrZpoU6ffe6KifzkZmlfbjwuQJtXDzgL4fD0wzTCfNP1Iun69u5NdejbEhWA2lS0Gt4KNgMX3wSQgL_4_htOs2__PwlH3d1F-VmZFweVJD6CSBqpttsgBEO3y1dwkM8w ca.crt: 1025 bytes namespace: 11 bytes Name: horizontal-pod-autoscaler-token-v46ss Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: horizontal-pod-autoscaler kubernetes.io/service-account.uid: 0d30b7d4-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJob3Jpem9udGFsLXBvZC1hdXRvc2NhbGVyLXRva2VuLXY0NnNzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Imhvcml6b250YWwtcG9kLWF1dG9zY2FsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZDMwYjdkNC1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06aG9yaXpvbnRhbC1wb2QtYXV0b3NjYWxlciJ9.0nwfw8R2M_qtziNugziYCoGFn-aXOHUrCWea6akqBGXBaIXVW0OCeLip3oW2WO2Il4LMHAFL_xSeqCkrIXgCO_LYGGbnQ0yS_QfLsDTDUzPOZzx4_o9COOWF-rfz_7OK8T01k5xns5_Gjxhh7sWpXv5IrrGeFhIWAS10d-CPXAOx-pt2r0aG4Vg-Pn3bPVi7DZvgED5LURx-kFSTsRRN8HX32jNmLKCcQ_mn3MRrBeOohf2tvCISqVzkMZfSN-fHiemZLXqQQlD25fC1zkLLQzDHoQn_A1VKH53Ac12xso0Z1r1tK4F38L85QLhV7QE-471kYjgJMJsSWv3SbcmnRg Name: job-controller-token-cmwfj Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: job-controller kubernetes.io/service-account.uid: 0ac95588-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJqb2ItY29udHJvbGxlci10b2tlbi1jbXdmaiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJqb2ItY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBhYzk1NTg4LWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpqb2ItY29udHJvbGxlciJ9.JFVVRYktyZFtJTwkepgJ5xFLJZXJReBN11ebo242b7LmembqxN3dw6qaNqd9gKJpaekzcUSJaCQDlHBivbW637YSfBsR2zov3US1_lVfm1jRCJ9Li8787-V8YcQmUCyM-gxbqwER15Sx3fGYhQicgDDVLuQ1JDfz3-9RTScKKTHOoFdpU0cmfB4kfgi4OaSl5hRvW33mi4psxLem-TjXV6EmYgGvfduVm6TGoewT-3uU2K7b9_rMLDLz6B85uolAJ_V0wajv8ZZjerTon3NyLFo5Uta_zt-gauEzaxGTVk4GHYIMhQ6-PTPZ2KpQVN8MI3j7kF9KJ9-nZ5gsvyEI4A ca.crt: 1025 bytes Name: kube-proxy-token-ssgzc Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: kube-proxy kubernetes.io/service-account.uid: 0cea9f15-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlLXByb3h5LXRva2VuLXNzZ3pjIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Imt1YmUtcHJveHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwY2VhOWYxNS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06a3ViZS1wcm94eSJ9.IzyixUsktQs4oQAQ-r1PTbtRrBrVf7elWCypxBkUHDhBjNQOZIjt0VnIjRoTdJenIOCH6tAmeMddJMWs8nYRMte_Pi-_XylesQypzAeiuE4oT8bxGZAC2H6JJV1D2OIrYaABWJzVejqakkrtsk5RQBbMMyXlkFK1_R1YP-5XmhzyNyfCaKzi9cprbQNX3IV72I9R9DUo6YwO3j0r_5cnKMfgPAk95ACrwqglYMpLWCFFRDjS7vWSW_ue65Yohvs85xBvYiG_ybkN75eegQYt-1AS6T0S-TBY2V3lyfO4k52VomAm34d1WgPjUyWUEun5ywH9ucDU7T_tGH4rzf66Kw Name: kubernetes-dashboard-certs Namespace: kube-system Labels: k8s-app=kubernetes-dashboard Annotations: Type: Opaque Data ==== Name: kubernetes-dashboard-key-holder Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== priv: 1679 bytes pub: 459 bytes Name: kubernetes-dashboard-token-njndv Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: kubernetes-dashboard kubernetes.io/service-account.uid: 41afcd0e-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1uam5kdiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjQxYWZjZDBlLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.LirJ7cHBUyF30S6KKjKMNI27WLL2MC2gPPhJc1GovzWgTwUWVPYImNcXYoUOVUZ-ZJwqT9ukOH_0vxF2kblKW3xSmB_rHQ3cvs6daVNA5-HPeBT8kq0EhiRlFALYezgKpCgkpiXvTE6MGNjMhHPJ0C8yi8jEY6OC9Y4DZQXmPQV3B2Xpk7ZoaMXoHFHfPTDSpNdeWSUJN9JErrmVBFpWeTuGI6m9A2wInQY1zjLiCm-iH9y0AbpwYg1QKGWNvKVSwYkS4ViDA6qKEzGWQxWSbCdsCIGhjGNz_nxMoVm1yPeU4W1yn-Psk6LLiIzSpizZTl-3VfIR0dl5mNHq7N3KPg Name: namespace-controller-token-smw7m Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: namespace-controller kubernetes.io/service-account.uid: 0f20a4d0-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi1zbXc3bSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBmMjBhNGQwLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.pQGalvSM_lh41fNKsY2ddZghUe_lJuz9Y9rmbXY0V-RjmF0BagSobz9SxucHAAOttIraamgTKksbxLEoAV509Yfxf62ybE4cW9KPggozc8iluwGLkdWYdGZxVe4midlnYcIpYTv4Zueav8f-9W2cGWyzHustTZf3R9b2A5N02uT91aZ38QWllsy_WKMVAaDLCLel71koJ_HjjDqrZ_ObO-YAtBeA9zslY0nhH-mpj0WsjW76Af4pVTotFBGdqtO9eV3Oe_H6sy6y2BAjxItGDTU9xvq7f3-taa-SB9j5XbEPJ1I8ObxcOLhYtvyCcHf2ZbxR9vVJ1CLR4ymry1mD2w Name: node-controller-token-9ppdt Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: node-controller kubernetes.io/service-account.uid: 0d0b0ff7-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJub2RlLWNvbnRyb2xsZXItdG9rZW4tOXBwZHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibm9kZS1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGQwYjBmZjctZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOm5vZGUtY29udHJvbGxlciJ9.Hg-v6PYzmoSmC-FVhE07OVOtLSGb7eIDPeY77k9l4tlCT8wDPNSo3zF8LKlsKCehDeKVfQL6bdcG596ECF_Vh0zHjswZrZMCsOWO0sBArDLzOu6Zo2wgvbkPZHYA0X7nt5jc10W_q-Nw1Ud3WLNtW7v4iWpbeGZGz7EgCrqW5XqYxA9P8CA4jdMBmdit-zwjaJUu5jjIYRbeYQhKF8hrC00vMXyYMRZp2dzGSMAmI4yF2OO-QdKc1Mieq-w4i8pOl7gO4nYRLQCAcdY2TjCO5VbcigZ6IWzzQb120WgC2Iqd5mhSzcmMFcrT1IbzOm91bFkgwGAs1oQw8Cy8iCV9SQ Name: persistent-volume-binder-token-9hjlk Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: persistent-volume-binder kubernetes.io/service-account.uid: 0fb0da85-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwZXJzaXN0ZW50LXZvbHVtZS1iaW5kZXItdG9rZW4tOWhqbGsiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicGVyc2lzdGVudC12b2x1bWUtYmluZGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGZiMGRhODUtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnBlcnNpc3RlbnQtdm9sdW1lLWJpbmRlciJ9.MbBQ48J2l7rgogx8YWgbX5ys_oyNBLBOn0xsuhO5_8jIKLwV5hup5X2JdjwTCw0uGZ0lkYgYANXnnK2Q39mlsJLMYCudaVWGZm7IumjyLuu9CkvhUmEHhYmqY2if-PAAXxaO_hKSbEpsmBEQKWGnU69wUdsEoQdBwqlHg4jZ69dXvrMsBgJ96ic_511e9B8R_GPKS1IXocTJtFcbCp-rpbk9REBGYQB-Fjh2UBpPvmeFQcfZ39yvBRMmi1LWW7sTQtwEiTErgYcIPpr7klSguSApm9un75P4tCjLPNFbIjUbf3lCqonwtEwADD1sKg2f9TSxdverIhoXX90IBJBTUA Name: pod-garbage-collector-token-l42r5 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: pod-garbage-collector kubernetes.io/service-account.uid: 0dfebd7d-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwb2QtZ2FyYmFnZS1jb2xsZWN0b3ItdG9rZW4tbDQycjUiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicG9kLWdhcmJhZ2UtY29sbGVjdG9yIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGRmZWJkN2QtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnBvZC1nYXJiYWdlLWNvbGxlY3RvciJ9.VaMklkb7hlu7MRkn0rE2Tlu-lJoeXQI4aB7qVGGHisp3nGVOjSodrldgQUxFnO3oJ9invV6RcYIS-yCaBfL8c0tak7yrGSWI2gjIrM-nS16PgLDYPjbZqJk9wXKIdzqt63Nt8HvQfBdi0IKmyM8eszOXywpHX9wyJFBGH5PYRdBs3HbpzNg5_GdjPO2IGbQGwZyyonb0o1_xk4GH-bAfbe3fCsHN5N1zY39DNo-qJ6xWC89Xfufgw95-_JxR_Pd7IiuUyqFN9KxqrbvdqdtML2ZS53fvpNqZ62_a9nK6xefGIgfxBXov_eqsjlBKh7R0vXyFAHpn3yTLUKZP-zHtGg Name: pv-protection-controller-token-qp2xr Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: pv-protection-controller kubernetes.io/service-account.uid: 0ed4594d-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwdi1wcm90ZWN0aW9uLWNvbnRyb2xsZXItdG9rZW4tcXAyeHIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicHYtcHJvdGVjdGlvbi1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGVkNDU5NGQtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnB2LXByb3RlY3Rpb24tY29udHJvbGxlciJ9.e52Sqc6aGtwNO4ecQRqddvzWsqkVOvj0MtMT8DmLf21sxv3W0yDDJYYTz6rzljqJPOkP7JBoNVoZL80e0NCXArmA27B4vCPTzCnSCrSSquoljMvxFmalnnJke6TQZKhplDtMy16orAM00Dr4KAUcphDP0uwO2Lsu6uSyFxiMQ53nRxzRt8fIf2HJZecHnvxA3qYKDpJ9ceZK0EgtzSBCSY8qNW9hVvAlUUjCkoJHeSzBeIrJTrI9GzYkkeqWfIdLy9fBpYHiCSxScKg7IVQ7iPqyZjYoo9BFKeXiIS87fD9qCh_xOAIKtmjAWmgaabEqcUx-kefbVTNKLhWn1AUyGw Name: pvc-protection-controller-token-b6q9s Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: pvc-protection-controller kubernetes.io/service-account.uid: 0ad1b174-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwdmMtcHJvdGVjdGlvbi1jb250cm9sbGVyLXRva2VuLWI2cTlzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InB2Yy1wcm90ZWN0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYWQxYjE3NC1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06cHZjLXByb3RlY3Rpb24tY29udHJvbGxlciJ9.TeJQ1uCU_Jx5c7L6eZ2Is20BT9SmwvadmZdK7fF4W0jGzlohLbS-ACQQJTMszWxti9BiAxke_bijrUoKZElDlbNpgXqiqnxaIAUdJIkVPm8IQpVN7d0GfYmEeHcUl5kUHO5Oc9AFqFcQeknxcTo8RI839SXp595DHTq0SPT3gaMfSqpV3dHIlSHixBke9oG9bTYo4OoAOljfq2OT30JQSk_Z-6-_ftvCxyVtgGZGs9jyC462ic5oxQ8U4BwJoPxmTCXn_aQPXygmlONMsAo4HIRzCmv2eey44oKvGBR5cInhGaoP-SUWuc0JG1PZFDLIllU2lod-DBSdxm7err6cTQ Name: replicaset-controller-token-9mx8t Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: replicaset-controller kubernetes.io/service-account.uid: 0cf1ad30-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXBsaWNhc2V0LWNvbnRyb2xsZXItdG9rZW4tOW14OHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicmVwbGljYXNldC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGNmMWFkMzAtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnJlcGxpY2FzZXQtY29udHJvbGxlciJ9.c5PbPe1CGfhMX1LzxQcQKPm3bNIFjwf9HXtSHs-wzF3caLtwNqiqczQid6F2Nf2EWbgG-ZarSJ1ESMdoPEvNprF-prVNXxeoTAuX4C9re-0zcCuZTGpxRvLq_IPZRJUHj12d3Lr8JAXdgJFBRvwQZhgDe0Tyinh5JJnNwlWvZ07mMBOVbAgcq2Ut_oAwu9DCiNv-gZHqBI-BISH6a7uF6YIdBOKIJxkTs3D3aaPYtyw-a0qBp2bAlRa3Un0IgjDUe9eiCGAc1cYRYuWUV-Ro2zrOBxwvVCh6r7iIWIuHb7y8Lc9NkbTXnxOSLwOyaCCkuioiSa_LytFlhCJlC09ubA Name: replication-controller-token-99sjd Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: replication-controller kubernetes.io/service-account.uid: 0efa8a2d-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXBsaWNhdGlvbi1jb250cm9sbGVyLXRva2VuLTk5c2pkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InJlcGxpY2F0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZWZhOGEyZC1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06cmVwbGljYXRpb24tY29udHJvbGxlciJ9.d157wtS1_qaJdrOFJsmZR37qe7XwvIypzUOJHU4UZpvynL2Na2Wim1xH9-n5AlS-fO_VdwrkBSG7Ef8XV_sziJ9jJmLU2vXo_ZH6CVohviJyxkAob465QCVjjuvKwCgxS7vYCpJ78Y5Vdr7PDFu28X_kd3tNAIuhPMlhj5aeL5NWUmsRSo5aXl4DhoossqF81GkcmKe-kuMSnm9BOFNxokTDA2zFWbOtT6oK7ZSz2oKVN8YoIu5534nmH7-ydokDrcNgUnqy9ByTA8BAZp7ueMlIc5xktJlvt6sQ8a3gNg90j_3H2RCA1wQZJcIaB8IVVXe2bt0ZaQI9wrYBBb7vEg Name: resourcequota-controller-token-hdch8 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: resourcequota-controller kubernetes.io/service-account.uid: 0e881d99-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXNvdXJjZXF1b3RhLWNvbnRyb2xsZXItdG9rZW4taGRjaDgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicmVzb3VyY2VxdW90YS1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGU4ODFkOTktZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnJlc291cmNlcXVvdGEtY29udHJvbGxlciJ9.omLEn20jTg0sn6P8Gihen5l3IuxFNIxK79wv_q00aOsGsAK1BbFQlIOWKHqaO81yvS3T_7UXSyCr01HTVhXabsJPS82t1Gr7jzzN4AKM230nHW_VGRjgU2gN4GxgAYByOTXBootTINz37J9Kmz0HSdTkIuqylXR6pUVrLKYJU2YqPqWe1fcoOi8P66d5qmpXfj806hw2SJgvxrf36v9cGMeoK5JDa1rI7VFAVLR-kWy9LgLQpVzDfuNqHAznay1wBcT3_Kb6l4JOkd_QdNZRl0xsP1NZMVaPqrbsxxO5us3_idNtZvo9fSsQU9itxodtnqBeSSlnvZH1Kr4tVa-7eg Name: service-account-controller-token-n268k Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: service-account-controller kubernetes.io/service-account.uid: 0ad49e7f-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtY29udHJvbGxlci10b2tlbi1uMjY4ayIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBhZDQ5ZTdmLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpzZXJ2aWNlLWFjY291bnQtY29udHJvbGxlciJ9.gVr-OJa0nVuUTFjsUwdAeiQQHEFVztiXUQfV7AgRmi850C1QiwpiBAejT8wOK6lwhOrhzT5EQ77ITqmTQZz1BC_PfZRgdNdZz6Ytw01TW4AeWe3A723yiqFDnasmFB7XhbKDQxFLd0dw5VjlDsdaEu0IIU0rdQKul8EUiccJolP2s-5V4FvvPfcPqnuFTlbbTiv509DE9G1ZTcZ4TxcBx4-0aiIe_vdUjuE2ECaOIPry0rN1yg3JNA5mJhOM6hAfmaz-DzH7DNh134RiWdHxQxQqeoRrlby2aS7IayT_JSiSZecoYGvL-CFteJdzMEWqWzIzMuDb9uzHJ1CBzOezbQ Name: service-controller-token-dgqch Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: service-controller kubernetes.io/service-account.uid: 0ce95734-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJzZXJ2aWNlLWNvbnRyb2xsZXItdG9rZW4tZGdxY2giLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoic2VydmljZS1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGNlOTU3MzQtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnNlcnZpY2UtY29udHJvbGxlciJ9.b7Y5beL-iAl64nYe_gTiD8ogwl5GTLpeE9GBxaEMKy3KbLiags3p-0ZqXLoDD_e91BaPxpmUyRfavWV5lZ85uzzBdH-q4aD6FnwHBmoTwbw3TK1znxCeri9xObQNmxDPH7CjVQLVHbksMFGum1L1xWCf3dw0p2ZrPbgzRzq93uUYjiW-w2H40Ub0q9TFTp9ic-T6wRq4DqT8XgBKa__nNblCHiM8hlZ6ufHFYeE4aAFcLLc1RaKvFxH7oO10AJ0fB45NwaMPa1iC0O5dTl57jQbh9mxusLtKAysAMMugObnCJ9yYSA65n9p9OsYTYZVp0OOCRGu7P4lO3XR3ZLr1-Q Name: statefulset-controller-token-j7858 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: statefulset-controller kubernetes.io/service-account.uid: 0ce28a94-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJzdGF0ZWZ1bHNldC1jb250cm9sbGVyLXRva2VuLWo3ODU4Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InN0YXRlZnVsc2V0LWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwY2UyOGE5NC1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06c3RhdGVmdWxzZXQtY29udHJvbGxlciJ9.ju-pOwheRoeE61w_NiQ8jLXFFhIb0SA6uaOFcKR5SmaN4bhQYIdlK0IywWQwu6RzCzIgCr4XrGrWsxblNpBMQC2-dNIuELWIMAFR5gjKQZh4v2Zmh4ysXGhbj7repAOLYvcseN-tGW3hax4IywN0GI5Pw257xoHPXZToA9lnqnIPiVdzlSOe8WBfhv6omdeGmtrgNGkgAjh2LiXViwPW7V3DVPjIY5d3k0v3pqzHSuxNYTRdjqcg53TMNTUFe2LKTVbzmgLngdKufOPmtabtnxU21r1ua887BCxl9RO7FUm01CTBlofKF5wkaGr5_w6zXRVgLE6Fo4YrhILf1z1BaQ Name: token-cleaner-token-9fvhw Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: token-cleaner kubernetes.io/service-account.uid: 0eb7ce45-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0b2tlbi1jbGVhbmVyLXRva2VuLTlmdmh3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InRva2VuLWNsZWFuZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZWI3Y2U0NS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06dG9rZW4tY2xlYW5lciJ9.QDNslXVqy5OPVb2mKpgsBDpWbTIaSgaZtjnvwwlsxyARYyLDuBHHpik2_IcmVI94riv3LF-WKI2m3fw9uAlkQRmebQCNCDYKWiJEttvEXIgb6vcwEIO3Bp2Q-nbwPjzxvGHmxluidzKZ4EIusyxqsxoMCPevMmsYHQWhegIvezE8tL_7oUPIx_rGW1lqB6ohZwjSPHTXvnmzqvP1zQDmwPMDN4Neqy3W-ahOAmBdlGx1cnPtEWY7z4cN7oMqY_l4CXwbZbq54Dh6M9ODZmiwd5TarLGv2fqaNpiG5YZwtlnRz9cAIC8GfGz_OAsj9bMLkt3yC_SHHdXeQ3UYQijYkA Name: ttl-controller-token-wpd5f Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: ttl-controller kubernetes.io/service-account.uid: 0d9b87be-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0dGwtY29udHJvbGxlci10b2tlbi13cGQ1ZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0dGwtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBkOWI4N2JlLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp0dGwtY29udHJvbGxlciJ9.jSUdoCKEBQYkAL5GtVQWCzFmH4FQ6GaenbDm9Cu5psOGhAd7_fpPejjmTinedpCmv0KgYIfOpzWyjQ-b1lJciZ9OFBapVDh--51kYR-enbhXpa5EP8T5O0PcnzXRojRZPMGPgQBWbn4JgTrfplzLRlrjFInTJVXA08a6B5rYXXmdABMCD08eaba9M4XeFuCIYoStmNhYw-fBPbtBegTif2ToT47-EojkMJnL8qI-oOL1nAbFHwaHSPC-czpf7VgSRBpnfTzbGrtrRwUX3vbYrrSTGo2RK87P9Qqb7HPWLnM1LDKHlgwkNmUpp9Dt8Tmun8XYWOpfQwaAqn6zRpbsPA Name: weave-net-token-fn28h Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: weave-net kubernetes.io/service-account.uid: 0d455a77-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ3ZWF2ZS1uZXQtdG9rZW4tZm4yOGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoid2VhdmUtbmV0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGQ0NTVhNzctZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOndlYXZlLW5ldCJ9.PpqFn31z-GiFAdE-Nd9-R8xuW8ESiisX53Vn_niAvFQ2DPII8q4150iNJDedLZ6nZiXBgV4e7b3NcZzNGhwRVAUyRgLrIqjdgNk-04YJ40bdP4d_5hSUylEn_QH1dgON6tMV9cK6skC0weFZzpPptGhA0bhyFdNl3DvGKGGmMgfyQsf3tEjZjEyqOuPLPtW0TPF2h98UmSsBzWbn-tCM_GEZTVY-3uxzfn2AfkuSI-e9ri1W4X8_W1z_cFIXk4Pwg0q7RWcJi44BCBJE079ezRLIWbowjXOXpqrZwe_jw_F-fJI_7ddxWDx68-wBwhjVIdNqnnMvzB-RKfgaIcWEiw 部署仪表板时，它使用 externalIPs 将服务绑定到端口 8443。这使得仪表板可供集群外部使用，并可在 https://2886795309-8443-elsy05.environments.katacoda.com/ 中查看\n使用 admin-user 令牌访问仪表板。\n对于生产环境，建议使用 kubectl proxy 来访问仪表板，而不是 externalIP。在 https://github.com/kubernetes/dashboard 上查看更多详细信息。\n","date":"2021-07-20T11:05:00+08:00","image":"https://www.catfish.top/p/k8s-basic-2/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-2/","title":"Kubernetes初探（二）"},{"content":" Katacoda在线课：Launch A Single Node Cluster\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n Minikube 是一个可以轻松在本地运行 Kubernetes 的工具。 Minikube 是一个在本地上计算机的虚拟机内运行一个单节点 Kubernetes 集群，便于用户能够完成日常开发工作，同时也能够让新用户快速了解 Kubernetes 。\n详情见： https://github.com/kubernetes/minikube\n步骤 1 - 启动 Minikube Minikube 已经安装并配置到环境中。通过运行 minikube version 命令检查它是否已正确安装。\n$ minikube version minikube version: v1.8.1 commit: cbda04cf6bbe65e987ae52bb393c10099ab62014 通过运行 minikube start 命令启动集群。\n$ minikube start --wait=false * minikube v1.8.1 on Ubuntu 18.04 * Using the none driver based on user configuration * Running on localhost (CPUs=2, Memory=2460MB, Disk=145651MB) ... * OS release is Ubuntu 18.04.4 LTS * Preparing Kubernetes v1.17.3 on Docker 19.03.6 ... - kubelet.resolv-conf=/run/systemd/resolve/resolv.conf * Launching Kubernetes ... * Enabling addons: default-storageclass, storage-provisioner * Configuring local host environment ... * Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; 现在，您的在线终端中有一个正在运行的 Kubernetes 集群。 Minikube 会启动一个虚拟机为 K8S 集群提供运行环境。\n步骤 2 - 集群信息 用户使用 kubectl 客户端与集群交互。该工具用于管理 Kubernetes 和在集群上运行的应用程序.\n通过 kubectl cluster-info 命令查看集群详情信息和健康状态。\n$ kubectl cluster-info Kubernetes master is running at https://172.17.0.10:8443 KubeDNS is running at https://172.17.0.10:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 通过 kubectl get nodes 查看集群中的各节点信息。\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 9m15s v1.17.3 如果节点被标记为 NotReady 则它仍在启动组件阶段。\n此命令显示可用于托管我们的应用程序的所有节点。现在我们只有一个节点，可以看到它的状态是 Ready。\n步骤 3 - 部署容器 现在可以通过 Kubernetes 集群来部署容器。\nkubectl run 命令能够将容器部署到集群中。\n$ kubectl create deployment first-deployment --image=katacoda/docker-http-server deployment.apps/first-deployment created 部署状态可以通过 Pods 的运行状态得知。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE first-deployment-666c48b44-n7qjz 1/1 Running 0 60s 容器运行后，可以根据需求使用不同的网络选项公开暴露接口。其中的一种解决方案是 NodePort ，它为容器提供动态端口。\n$ kubectl expose deployment first-deployment --port=80 --type=NodePort service/first-deployment exposed 下面的命令能够查询到绑定的端口，并且能够发送HTTP请求进行测试。\n$ export PORT=$(kubectl get svc first-deployment -o go-template=\u0026#39;{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{\u0026#34;\\n\u0026#34;}}{{end}}{{end}}\u0026#39;) $ echo \u0026#34;Accessing host01:$PORT\u0026#34; Accessing host01:32492 $ curl host01:$PORT \u0026lt;h1\u0026gt;This request was processed by host: first-deployment-666c48b44-n7qjz\u0026lt;/h1\u0026gt; 结果显示的是处理请求的容器。\n步骤 4 - 仪表盘 使用 Minikube 命令启用仪表板\n$ minikube addons enable dashboard * The \u0026#39;dashboard\u0026#39; addon is enabled 通过使用 yaml 文件来定义部署 Kubernetes Dashboard 。该配置仅适用于Katacoda\n/opt/kubernetes-dashboard.yaml\napiVersion:v1kind:Namespacemetadata:labels:addonmanager.kubernetes.io/mode:Reconcilekubernetes.io/minikube-addons:dashboardname:kubernetes-dashboardselfLink:/api/v1/namespaces/kubernetes-dashboardspec:finalizers:- kubernetesstatus:phase:Active---apiVersion:v1kind:Servicemetadata:labels:app:kubernetes-dashboardname:kubernetes-dashboard-katacodanamespace:kubernetes-dashboardspec:ports:- port:80protocol:TCPtargetPort:9090nodePort:30000selector:k8s-app:kubernetes-dashboardtype:NodePort$ kubectl apply -f /opt/kubernetes-dashboard.yaml namespace/kubernetes-dashboard configured service/kubernetes-dashboard-katacoda created 可以使用 Kubernetes 仪表板查看部署到集群中的应用程序。仪表板将部署在 30000 端口，但需要一段时间才能完成启动。\n要查看 Dashboard 启动的进度，请使用 kubectl get pods -n kubernetes-dashboard -w 查看 kube-system 命名空间中的 Pod\n$ kubectl get pods -n kubernetes-dashboard -w NAME READY STATUS RESTARTS AGE dashboard-metrics-scraper-7b64584c5c-jmkls 1/1 Running 0 6m15s kubernetes-dashboard-79d9cd965-fcb4t 1/1 Running 0 6m14s 启动好后，仪表盘的在线访问地址为： https://2886795306-30000-ollie07.environments.katacoda.com/\n","date":"2021-07-20T10:21:00+08:00","image":"https://www.catfish.top/p/k8s-basic-1/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-1/","title":"Kubernetes初探（一）"},{"content":"介绍 Katacoda 是一个面向软件工程师的交互式学习和培训平台，可在浏览器中使用真实环境学习和测试新技术，帮助开发人员学习，并掌握最佳实践。\nKatacoda 的目标是消除新技术和技能的障碍。\nKatacoda 提供了一个平台来构建实时交互式演示和培训环境。运行环境可以根据应用要求进行定制。分步指导路径旨在确保用户以最佳方式学习。用户可以根据设计好的引导步骤，通过浏览器上的终端界面操作一套完整的环境，一步步的学习和实践。Katacoda 的出现很好的解决了这些问题。课程设计者可以定制应用程序所需环境，并设计循序渐进的指导路径，旨在确保用户以最佳方式学习。\nKatacoda 同样也是Kubernetes官网的学习工具，能够快速帮助开发者掌握K8s知识与应用，同时能够节省大量搭建部署环境的时间与精力，能够让开发者专注于学习掌握K8s。\nPlayground功能更是为开发者带来一种新的开发体验。借助于Katacoda平台，能够快速构建满足应用要求的各类环境，开发者能够在这之中，专注于完成创造、验证等工作。\n使用体验 课程 Playground 中文翻译-目录 Kubernetes Basic 原文：https://www.katacoda.com/courses/kubernetes\nLaunch A Single Node Cluster 原文：https://www.katacoda.com/courses/kubernetes/launch-single-node-cluster\n","date":"2021-07-19T17:57:00+08:00","image":"https://www.catfish.top/p/katacoda/katacoda_huf47240e26664e9c6e2f7130233d16c2d_114448_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.catfish.top/p/katacoda/","title":"Katacoda 在线学习神器"},{"content":"概述 Google工程师Jeffrey Dean 和 Sanjay Ghemawat在2004年发表了一篇论文MapReduce: Simplified Data Processing on Large Clusters，MapReduce作为大规模数据处理的需求下孕育而生的产物，被创造的初衷是为了解决Google公司内部搜索引擎中大规模数据的并行化处理。\n引用维基百科中对MapReduce的介绍：\n MapReduce是Google提出的一个软件架构，用于大规模数据集（大于1TB）的并行运算。\n 概念“Map（映射）”和“Reduce（归纳）”，及他们的主要思想，都是从函数式编程语言借来的，还有从矢量编程语言借来的特性。当前的软件实现是指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归纳）函数，用来保证所有映射的键值对中的每一个共享相同的键组。\n本系列将根据MIT6.824课程进行学习 MapReduce\n 简单看来，MapReduce的架构正如其名，分为Map和Reduce两部分。\n MapReduce Overview \n作为一个计算框架，其最大的核心便在于计算二字，以往处理计算的模式为单机运行，在大数据的情况下只能使用具有更强算力的计算机来完成计算工作，而算力的提升是需要花费极大的成本，这样在成本上是极为不划算的。在这一背景下MapReduce孕育而生，有个这样一个框架，就可以将数台不同算力的计算机组成一个集群，和适度的调度下并行计算提高效率降低成本。\n根据上图，MapReduce中存在3中角色，Master，Worker（Map），Worker(Reduce）,Master负责Map，Reduce两层的调度管理,Worker(Map)负责进行Map操作，Worker（Reduce）负责进行Reduce操作\n现在以该课程lab1为例来进行细致的学习，整套课程将由Go语言来实现。 lab1主要是对MapReduce模型进行初步的学习，实现一个本机的MapReduce模型，完成对多个文件的词频统计。\n示例程序流程 入口由上层程序控制，这个地方从master调度开始，预先定义Reduce任务个数m，再根据文本文件输入数量n。\nmaster将n传递给doMap也就是Map调度层，告诉Map调度层执行n次Map计算，每个Map计算层对应输入各个文本文件的数据。\nMap调度层将输出m*n个文件作为Map和Reduce的中间数据传递媒介，举例假设现在m为2、n为3，输出文件mid-0-0 mid-0-1 mid-1-0 mid-1-1 mid-2-0 mid-2-1这六个文件,其中mid-0-0 mid-0-1为第一个文本Map操作后的到的切分开的两个中间数据文件，剩下的以此类推。\n在Reduce调度层中便会将这六个文件交由Reduce计算层处理，将件mid-0-0 mid-1-0 mid-2-0交由编号为0的Reduce计算任务处理，显然，编号为1的任务则负责剩下3个文件的规约操作。Reduce调度层中仍然会将每个Reduce计算层任务得到的数据分别存入文件，根据Reduce任务的数量m为2，则文件编号分别为res-0 res-1，这样Map、Reduce两种操作完成，但整个任务还为完成。\nMerge操作则是最后一个步骤通常由master来完成，通过merge操作将上述的res-0 res-1合并，将所有结果存入到一个文件中，这样，整个MapReduce实现的多文本词频统计程序执行完毕。\n 数据结构 type KeyValue struct { Key string Value string } 实现 Master调度 master.go\nfunc (mr *Master) run(jobName string, files []string, nreduce int, schedule func(phase jobPhase), finish func(), ) { mr.jobName = jobName mr.files = files mr.nReduce = nreduce fmt.Printf(\u0026#34;%s: Starting Map/Reduce task %s\\n\u0026#34;, mr.address, mr.jobName) schedule(mapPhase) //map层进行操作 \tschedule(reducePhase) //reduce层进行操作 \tfinish() //结束所有worker \tmr.merge() //合并  fmt.Printf(\u0026#34;%s: Map/Reduce task completed\\n\u0026#34;, mr.address) mr.doneChannel \u0026lt;- true } 这部分展示了master如何对map、reduce进行调度的过程，首先执行所有的map操作，执行完毕后执行所有的reduce操作进行规约，最后merge合并所有reduce之后的结果，把所有的结果汇总并存储。\n Map调度层 common_map.go\nfunc doMap( jobName string, // MapReduce任务名 \tmapTaskNumber int, // Map任务序号 \tinFile string, nReduce int, // Reduce Worker数量 (\u0026#34;R\u0026#34; in the paper) \tmapF func(file string, contents string) []KeyValue, ) { dat, err := ioutil.ReadFile(inFile) //读文本文件 \tif err != nil { panic(err) } res := mapF(inFile, string(dat)) //根据文件中的内容进行单文件Map操作 \tm := make([]map[string]string, nReduce) //对于每个文本文件，将使用KeyValue的slice来存放，之后会对这部分数据进行切分，根据nReduce（reduce层worker数量）来决定划分数目 \tfor _, kv := range res { index := ihash(kv.Key) % nReduce //序号划分，根据Key做hash处理，对结果模你Reduce得到划分序号 \tif m[index] == nil{ m[index] = make(map[string]string) } m[index][kv.Key] = kv.Value } for i := 0; i \u0026lt; nReduce; i++ { filename := reduceName(jobName, mapTaskNumber, i) jsonObj, err := json.Marshal(m[i]) //使用json作为中间数据 \tif err != nil { panic(err) } ioutil.WriteFile(filename, jsonObj, 0644) //将数据写入到文件中 //将json数据写入到文件中，供Reduce操作使用 \t} Map调度层提供数据的输入与输出，不关心计算过程\n Map计算层 wc.go\n//filename 为输入文件文件名 //contents 为文本内容 func mapF(filename string, contents string) []mapreduce.KeyValue { var res []mapreduce.KeyValue m := make(map[string] int) reg := regexp.MustCompile(\u0026#34;\\n|\\r|\\t|[ ]+|[\\\\-]+|\\\\(|\\\\)\u0026#34;) contents = reg.ReplaceAllString(contents,\u0026#34; \u0026#34;) reg = regexp.MustCompile(\u0026#34;[^(a-zA-Z )]\u0026#34;) contents = reg.ReplaceAllString(contents,\u0026#34;\u0026#34;) //fmt.Println(contents) \ts := strings.Split(contents,\u0026#34; \u0026#34;) for i := 0;i \u0026lt; len(s);i++ { str := strings.TrimSpace(s[i]) m[strings.ToLower(str)]++ } for k,v := range m{ val := strconv.Itoa(v) res = append(res, mapreduce.KeyValue{k, val }) } fmt.Println(res) return res } Map计算层根据输入的文本内容完成下列步骤：\n 使用正则表达式将\\n \\t \\r替换为空格 使用正则表达式将无关字符删除 使用strings.Split方法根据空格进行划分 取出划分后的每个词，使用strings.TrimSpace方法去除两端空格，使用map[string] int结构的m来完成统计 将map[string] int转换为[]KeyValue返回  Map计算层和Map调度层共同组成的Map的结构，一个负责计算，一个负责IO，分工明确\n Reduce调度层 common_reduce.go\nfunc doReduce( jobName string, // MapReduce任务名 \treduceTaskNumber int, // Reduce任务序号 \toutFile string, // 输出文件路径 \tnMap int, // Map任务数 (\u0026#34;M\u0026#34; in the paper) \treduceF func(key string, values []string) string, ) { m := make(map[string][]string) for i := 0; i \u0026lt; nMap; i++ { filename := reduceName(jobName, i, reduceTaskNumber) fmt.Println(filename) data, err := ioutil.ReadFile(filename) if err != nil { panic(err) } var tm map[string]string err = json.Unmarshal(data, \u0026amp;tm) if err != nil { panic(err) } for k, v := range tm { if err != nil { panic(err) } m[k] = append(m[k], v) } } dataM := make([]KeyValue,0) for k, v := range m { ans := reduceF(k, v) dataM = append(dataM, KeyValue{k,ans}) } jsonData, err := json.Marshal(dataM) if err != nil { panic(err) } ioutil.WriteFile(outFile, jsonData, 0644) } Reduce调度层完成的工作是将Map执行后的文件读入为数据交有Reduce来规约，同样也不负责Reduce具体的过程，仅负责数据的读入（文件读入）和数据的保存（文件写出）\n Reduce计算层 wc.go\n//key为文本中的单词 //values为各个Map处理后得到的单词的个数 func reduceF(key string, values []string) string { sum :=0 for _,v := range values{ num,err := strconv.Atoi(v) //string转int \tif err!=nil{ panic(err) } sum+=num //累加 \t} return strconv.Itoa(sum) //将int转string返回 } Reduce计算层则仅仅负责数据的规约，步骤如下：\n 遍历整个values切片 将values的数值累加 将累加总和作为结果返回   Merge func (mr *Master) merge() { kvs := make(map[string]string) //总的数据存放 \tfor i := 0; i \u0026lt; mr.nReduce; i++ { p := mergeName(mr.jobName, i) //merge文件名生成 \tfile,err :=ioutil.ReadFile(p) if err != nil { log.Fatal(\u0026#34;Merge: \u0026#34;, err) } var jsonObj []KeyValue err = json.Unmarshal(file,\u0026amp;jsonObj) //json解码 \tif err != nil{ log.Fatal(\u0026#34;Merge: \u0026#34;, err) } for _,v:=range jsonObj{ kvs[v.Key] = v.Value } } } Merge操作便将Reduce操作得到的数据进行合并，完成最后一步工作。\n 总结 通过这几天短暂的学习，MapReduce模型确实能够极大的利用现有计算资源来打造一个算力强劲的计算机集群，但是本实例中也存在几个局限性：\n 本实例仅在单机中运行 操作间数据交换使用文件，在性能上可能会有一定影响 任务划分的科学性也应该是性能上应该考虑的问题，集群中计算机算力各不相同的时候，能者多劳，任务的划分应更加合理，才可能避免水桶原理造成的性能上的损失  如有不足之处，恳请提出批评！\n","date":"2017-09-07T01:21:00+08:00","image":"https://www.catfish.top/p/mit6.824-1/hadoop_hu7a934a9f3335148cadbaca544cf45563_202913_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/mit6.824-1/","title":"MIT 6.824 分布式系统初探（一）"},{"content":"字符编码 字集码是把字符集中的字符编码为指定集合中某一对象（例如：比特模式、自然数序列、8位组或者电脉冲），以便文本在计算机中存储和通过通信网络的传递。\n常见的例子包括将拉丁字母表编码成摩斯电码和ASCII。其中，ASCII将字母、数字和其它符号编号，并用7比特的二进制来表示这个整数。通常会额外使用一个扩充的比特，以便于以1个字节的方式存储。 常见的字符编码有： ASCII、UTF-8、Unicode、GBK等\n详见WikiPedia\n ASCII ASCII ( A merican S tandard C ode for I nformation I nterchange) 即美国信息交换标准代码。\nASCII第一次以规范标准的型态发表是在1967年，最后一次更新则是在1986年，至今为止共定义了128个字符；其中33个字符无法显示（一些终端提供了扩展，使得这些字符可显示为诸如笑脸、扑克牌花式等8-bit符号），且这33个字符多数都已是陈废的控制字符。控制字符的用途主要是用来操控已经处理过的文字。在33个字符之外的是95个可显示的字符，包含用键盘敲下空白键所产生的空白字符也算1个可显示字符（显示为空白）。\nASCII的局限在于只能显示26个基本拉丁字母、阿拉伯数目字和英式标点符号，因此只能用于显示现代美国英语（而且在处理英语当中的外来词如naïve、café、élite等等时，所有重音符号都不得不去掉，即使这样做会违反拼写规则）。而EASCII虽然解决了部分西欧语言的显示问题，但对更多其他语言依然无能为力。因此现在的软件系统大多采用Unicode。\n详见WikiPedia\n Unicode  Unicode 是为了解决传统的字符编码方案的局限而产生的，例如ISO 8859-1所定义的字符虽然在不同的国家中广泛地使用，可是在不同国家间却经常出现不兼容的情况。\n很多传统的编码方式都有一个共同的问题，即容许电脑处理双语环境（通常使用拉丁字母以及其本地语言），但却无法同时支持多语言环境（指可同时处理多种语言混合的情况）。\n目前，几乎所有电脑系统都支持基本拉丁字母，并各自支持不同的其他编码方式。Unicode为了和它们相互兼容，其首256字符保留给ISO 8859-1所定义的字符，使既有的西欧语系文字的转换不需特别考量；并且把大量相同的字符重复编到不同的字符码中去，使得旧有纷杂的编码方式得以和Unicode编码间互相直接转换，而不会丢失任何信息。举例来说，全角格式区块包含了主要的拉丁字母的全角格式，在中文、日文、以及韩文字形当中，这些字符以全角的方式来呈现，而不以常见的半角形式显示，这对竖排文字和等宽排列文字有重要作用。\n详见WikiPedia\n UTF-8  UTF-8 （8-bit Unicode Transformation Format）是一种针对Unicode的可变长度字符编码，也是一种前缀码。\n它可以用来表示Unicode标准中的任何字符，且其编码中的第一个字节仍与ASCII兼容，这使得原来处理ASCII字符的软件无须或只须做少部分修改，即可继续使用。因此，它逐渐成为电子邮件、网页及其他存储或发送文字的应用中，优先采用的编码。\nUTF-8 现已经作为通用的字符编码，应用于各中网页编码，数据编码，数据库字符编码等。编码的统一能够写出的程序或网页在中文环境下大大减少乱码的出现。dddddddddddddddddd\n详见WikiPedia\n GBK  汉字内码扩展规范 ，称GBK，全名为《汉字内码扩展规范(GBK)》1.0版，由中华人民共和国全国信息技术标准化技术委员会1995年12月1日制订，国家技术监督局标准化司和电子工业部科技与质量监督司1995年12月15日联合以《技术标函[1995]229号》文件的形式公布。\nGBK的K为汉语拼音Kuo Zhan（扩展）中“扩”字的声母。英文全称Chinese Internal Code Extension Specification。\nGBK 只为“技术规范指导性文件”，不属于国家标准。国家质量技术监督局于2000年3月17日推出了GB 18030-2000标准，以取代GBK。\n BOM ( Byte Order Mark )  这个才是重点，BOM头。\n在UTF-8编码文件中BOM在文件头部，占用三个字节，用来标示该文件属于UTF-8编码，现在已经有很多软件识别BOM头，但是还有些不能识别BOM头，比如Windows自带的记事本软件，这也是用记事本编辑UTF-8编码后执行就会出错的原因了。\nWindows下\n  支持BOM的编辑器 Notepad++\n  不支持BOM的编辑器 Windows自带的记事本 （坑）\n  正因有BOM头的存在，使我在本地Jekyll的调试环境中，页面不能正常显示。\n请使用Windows的童鞋一定注意该问题\n目前已经切换为 Hugo + GitHub Action 的方式来完成静态网站的生成\n","date":"2016-05-18T00:36:00+08:00","image":"https://www.catfish.top/p/bom/bom_hu2fca0881760928b759bf10a8d0d84e1c_194611_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.catfish.top/p/bom/","title":"Blog 第一帖 - 字符坑 BOM"}]