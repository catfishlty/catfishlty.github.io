[{"content":" Katacoda在线课：Helm Package Manager\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 这个场景教你如何使用 Kubernetes 的包管理器 Helm 来部署 Redis。 Helm 简化了服务发现和部署到 Kubernetes 集群的步骤。。\n \u0026ldquo;Helm is the best way to find, share, and use software built for Kubernetes.\u0026quot;\nHelm 是查找、共享和使用为 Kubernetes 构建的软件的最佳方式\n 更多细节可以前往官网：http://www.helm.sh/\n安装 Helm Helm 是一个单独的二进制文件，用于管理将 Charts 部署到 Kubernetes。 Chart 是 kubernetes 应用的一个打包单元。Helm 可以从 https://github.com/kubernetes/helm/releases 下载。\ncontrolplane $ curl -LO https://storage.googleapis.com/kubernetes-helm/helm-v2.8.2-linux-amd64.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 14.2M 100 14.2M 0 0 19.4M 0 --:--:-- --:--:-- --:--:-- 19.4M controlplane $ tar -xvf helm-v2.8.2-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/README.md linux-amd64/LICENSE controlplane $ mv linux-amd64/helm /usr/local/bin/ 安装后，初始化更新本地缓存将最新的包与本地安装环境同步。\ncontrolplane $ helm init --stable-repo-url https://charts.helm.sh/stable Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://charts.helm.sh/stable Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure \u0026#39;allow unauthenticated users\u0026#39; policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! controlplane $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Skip local chart repository ...Successfully got an update from the \u0026#34;stable\u0026#34; chart repository Update Complete. ⎈ Happy Helming!⎈ 搜索 Chart 您现在可以开始部署软件。可以使用搜索命令查找可用图表 Chart 。\n例如，要部署 Redis，我们需要找到一个 Redis 的 Chart 。\ncontrolplane $ helm search redis NAME CHART VERSION APP VERSION DESCRIPTION stable/prometheus-redis-exporter 3.5.1 1.3.4 DEPRECATED Prometheus exporter for Redis metrics stable/redis 10.5.7 5.0.7 DEPRECATED Open source, advanced key-value stor... stable/redis-ha 4.4.6 5.0.6 DEPRECATED - Highly available Kubernetes implem... stable/sensu 0.2.5 0.28 DEPRECATED Sensu monitoring framework backed by.. 通过 inspect 命令，我们可以查看更多关于 stable/redis 的信息，信息量很大。\ncontrolplane $ helm inspect stable/redis apiVersion: v1 appVersion: 5.0.7 deprecated: true description: DEPRECATED Open source, advanced key-value store. It is often referred to as a data structure server since keys can contain strings, hashes, lists, sets and sorted sets. engine: gotpl home: http://redis.io/ icon: https://bitnami.com/assets/stacks/redis/img/redis-stack-220x234.png keywords: - redis - keyvalue - database name: redis sources: - https://github.com/bitnami/bitnami-docker-redis version: 10.5.7 --- ## Global Docker image parameters ## Please, note that this will override the image parameters, including dependencies, configured to use the global value ## Current available global Docker image parameters: imageRegistry and imagePullSecrets ## global: # imageRegistry: myRegistryName # imagePullSecrets: # - myRegistryKeySecretName # storageClass: myStorageClass redis: {} ## Bitnami Redis image version ## ref: https://hub.docker.com/r/bitnami/redis/tags/ ## image: registry: docker.io repository: bitnami/redis ## Bitnami Redis image tag ## ref: https://github.com/bitnami/bitnami-docker-redis#supported-tags-and-respective-dockerfile-links ## tag: 5.0.7-debian-10-r32 ## Specify a imagePullPolicy ## Defaults to \u0026#39;Always\u0026#39; if image tag is \u0026#39;latest\u0026#39;, else set to \u0026#39;IfNotPresent\u0026#39; ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images ## pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName ## String to partially override redis.fullname template (will maintain the release name) ## # nameOverride: ## String to fully override redis.fullname template ## # fullnameOverride: ## Cluster settings cluster: enabled: true slaveCount: 2 ## Use redis sentinel in the redis pod. This will disable the master and slave services and ## create one redis service with ports to the sentinel and the redis instances sentinel: enabled: false ## Require password authentication on the sentinel itself ## ref: https://redis.io/topics/sentinel usePassword: true ## Bitnami Redis Sentintel image version ## ref: https://hub.docker.com/r/bitnami/redis-sentinel/tags/ ## image: registry: docker.io repository: bitnami/redis-sentinel ## Bitnami Redis image tag ## ref: https://github.com/bitnami/bitnami-docker-redis-sentinel#supported-tags-and-respective-dockerfile-links ## tag: 5.0.7-debian-10-r27 ## Specify a imagePullPolicy ## Defaults to \u0026#39;Always\u0026#39; if image tag is \u0026#39;latest\u0026#39;, else set to \u0026#39;IfNotPresent\u0026#39; ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images ## pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName masterSet: mymaster initialCheckTimeout: 5 quorum: 2 downAfterMilliseconds: 60000 failoverTimeout: 18000 parallelSyncs: 1 port: 26379 ## Additional Redis configuration for the sentinel nodes ## ref: https://redis.io/topics/config ## configmap: ## Enable or disable static sentinel IDs for each replicas ## If disabled each sentinel will generate a random id at startup ## If enabled, each replicas will have a constant ID on each start-up ## staticID: false ## Configure extra options for Redis Sentinel liveness and readiness probes ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes) ## livenessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 1 successThreshold: 1 failureThreshold: 5 ## Redis Sentinel resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ # resources: # requests: # memory: 256Mi # cpu: 100m ## Redis Sentinel Service properties service: ## Redis Sentinel Service type type: ClusterIP sentinelPort: 26379 redisPort: 6379 ## Specify the nodePort value for the LoadBalancer and NodePort service types. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport ## # sentinelNodePort: # redisNodePort: ## Provide any additional annotations which may be required. This can be used to ## set the LoadBalancer service type to internal only. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer ## annotations: {} labels: {} loadBalancerIP: ## Specifies the Kubernetes Cluster\u0026#39;s Domain Name. ## clusterDomain: cluster.local networkPolicy: ## Specifies whether a NetworkPolicy should be created ## enabled: false ## The Policy model to apply. When set to false, only pods with the correct ## client label will have network access to the port Redis is listening ## on. When true, Redis will accept connections from any source ## (with the correct destination port). ## # allowExternal: true ## Allow connections from other namespacess. Just set label for namespace and set label for pods (optional). ## ingressNSMatchLabels: {} ingressNSPodMatchLabels: {} serviceAccount: ## Specifies whether a ServiceAccount should be created ## create: false ## The name of the ServiceAccount to use. ## If not set and create is true, a name is generated using the fullname template name: rbac: ## Specifies whether RBAC resources should be created ## create: false role: ## Rules to create. It follows the role specification # rules: # - apiGroups: # - extensions # resources: # - podsecuritypolicies # verbs: # - use # resourceNames: # - gce.unprivileged rules: [] ## Redis pod Security Context securityContext: enabled: true fsGroup: 1001 runAsUser: 1001 ## sysctl settings for master and slave pods ## ## Uncomment the setting below to increase the net.core.somaxconn value ## # sysctls: # - name: net.core.somaxconn # value: \u0026#34;10000\u0026#34; ## Use password authentication usePassword: true ## Redis password (both master and slave) ## Defaults to a random 10-character alphanumeric string if not set and usePassword is true ## ref: https://github.com/bitnami/bitnami-docker-redis#setting-the-server-password-on-first-run ## password: \u0026#34;\u0026#34; ## Use existing secret (ignores previous password) # existingSecret: ## Password key to be retrieved from Redis secret ## # existingSecretPasswordKey: ## Mount secrets as files instead of environment variables usePasswordFile: false ## Persist data to a persistent volume (Redis Master) persistence: {} ## A manually managed Persistent Volume and Claim ## Requires persistence.enabled: true ## If defined, PVC must be created manually before volume will be bound # existingClaim: # Redis port redisPort: 6379 ## ## Redis Master parameters ## master: ## Redis command arguments ## ## Can be used to specify command line arguments, for example: ## command: \u0026#34;/run.sh\u0026#34; ## Additional Redis configuration for the master nodes ## ref: https://redis.io/topics/config ## configmap: ## Redis additional command line flags ## ## Can be used to specify command line flags, for example: ## ## extraFlags: ## - \u0026#34;--maxmemory-policy volatile-ttl\u0026#34; ## - \u0026#34;--repl-backlog-size 1024mb\u0026#34; extraFlags: [] ## Comma-separated list of Redis commands to disable ## ## Can be used to disable Redis commands for security reasons. ## Commands will be completely disabled by renaming each to an empty string. ## ref: https://redis.io/topics/security#disabling-of-specific-commands ## disableCommands: - FLUSHDB - FLUSHALL ## Redis Master additional pod labels and annotations ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/ podLabels: {} podAnnotations: {} ## Redis Master resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ # resources: # requests: # memory: 256Mi # cpu: 100m ## Use an alternate scheduler, e.g. \u0026#34;stork\u0026#34;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: ## Configure extra options for Redis Master liveness and readiness probes ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes) ## livenessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 1 successThreshold: 1 failureThreshold: 5 ## Redis Master Node selectors and tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature ## # nodeSelector: {\u0026#34;beta.kubernetes.io/arch\u0026#34;: \u0026#34;amd64\u0026#34;} # tolerations: [] ## Redis Master pod/node affinity/anti-affinity ## affinity: {} ## Redis Master Service properties service: ## Redis Master Service type type: ClusterIP port: 6379 ## Specify the nodePort value for the LoadBalancer and NodePort service types. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport ## # nodePort: ## Provide any additional annotations which may be required. This can be used to ## set the LoadBalancer service type to internal only. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer ## annotations: {} labels: {} loadBalancerIP: # loadBalancerSourceRanges: [\u0026#34;10.0.0.0/8\u0026#34;] ## Enable persistence using Persistent Volume Claims ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## persistence: enabled: true ## The path the volume will be mounted at, useful when using different ## Redis images. path: /data ## The subdirectory of the volume to mount to, useful in dev environments ## and one PV for multiple services. subPath: \u0026#34;\u0026#34; ## redis data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026#34;-\u0026#34;, storageClassName: \u0026#34;\u0026#34;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## # storageClass: \u0026#34;-\u0026#34; accessModes: - ReadWriteOnce size: 8Gi ## Persistent Volume selectors ## https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector matchLabels: {} matchExpressions: {} ## Update strategy, can be set to RollingUpdate or onDelete by default. ## https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets statefulset: updateStrategy: RollingUpdate ## Partition update strategy ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions # rollingUpdatePartition: ## Redis Master pod priorityClassName # priorityClassName: {} ## ## Redis Slave properties ## Note: service.type is a mandatory parameter ## The rest of the parameters are either optional or, if undefined, will inherit those declared in Redis Master ## slave: ## Slave Service properties service: ## Redis Slave Service type type: ClusterIP ## Redis port port: 6379 ## Specify the nodePort value for the LoadBalancer and NodePort service types. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport ## # nodePort: ## Provide any additional annotations which may be required. This can be used to ## set the LoadBalancer service type to internal only. ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer ## annotations: {} labels: {} loadBalancerIP: # loadBalancerSourceRanges: [\u0026#34;10.0.0.0/8\u0026#34;] ## Redis slave port port: 6379 ## Can be used to specify command line arguments, for example: ## command: \u0026#34;/run.sh\u0026#34; ## Additional Redis configuration for the slave nodes ## ref: https://redis.io/topics/config ## configmap: ## Redis extra flags extraFlags: [] ## List of Redis commands to disable disableCommands: - FLUSHDB - FLUSHALL ## Redis Slave pod/node affinity/anti-affinity ## affinity: {} ## Configure extra options for Redis Slave liveness and readiness probes ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes) ## livenessProbe: enabled: true initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: enabled: true initialDelaySeconds: 5 periodSeconds: 10 timeoutSeconds: 10 successThreshold: 1 failureThreshold: 5 ## Redis slave Resource # resources: # requests: # memory: 256Mi # cpu: 100m ## Redis slave selectors and tolerations for pod assignment # nodeSelector: {\u0026#34;beta.kubernetes.io/arch\u0026#34;: \u0026#34;amd64\u0026#34;} # tolerations: [] ## Use an alternate scheduler, e.g. \u0026#34;stork\u0026#34;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: ## Redis slave pod Annotation and Labels podLabels: {} podAnnotations: {} ## Redis slave pod priorityClassName # priorityClassName: {} ## Enable persistence using Persistent Volume Claims ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## persistence: enabled: true ## The path the volume will be mounted at, useful when using different ## Redis images. path: /data ## The subdirectory of the volume to mount to, useful in dev environments ## and one PV for multiple services. subPath: \u0026#34;\u0026#34; ## redis data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026#34;-\u0026#34;, storageClassName: \u0026#34;\u0026#34;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## # storageClass: \u0026#34;-\u0026#34; accessModes: - ReadWriteOnce size: 8Gi ## Persistent Volume selectors ## https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector matchLabels: {} matchExpressions: {} ## Update strategy, can be set to RollingUpdate or onDelete by default. ## https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets statefulset: updateStrategy: RollingUpdate ## Partition update strategy ## https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#partitions # rollingUpdatePartition: ## Prometheus Exporter / Metrics ## metrics: enabled: false image: registry: docker.io repository: bitnami/redis-exporter tag: 1.4.0-debian-10-r3 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName ## Metrics exporter resource requests and limits ## ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## # resources: {} ## Extra arguments for Metrics exporter, for example: ## extraArgs: ## check-keys: myKey,myOtherKey # extraArgs: {} ## Metrics exporter pod Annotation and Labels podAnnotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;9121\u0026#34; # podLabels: {} # Enable this if you\u0026#39;re using https://github.com/coreos/prometheus-operator serviceMonitor: enabled: false ## Specify a namespace if needed # namespace: monitoring # fallback to the prometheus default unless specified # interval: 10s ## Defaults to what\u0026#39;s used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr) ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1) ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters) selector: prometheus: kube-prometheus ## Custom PrometheusRule to be defined ## The value is evaluated as a template, so, for example, the value can depend on .Release or .Chart ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions prometheusRule: enabled: false additionalLabels: {} namespace: \u0026#34;\u0026#34; rules: [] ## These are just examples rules, please adapt them to your needs. ## Make sure to constraint the rules to the current postgresql service. # - alert: RedisDown # expr: redis_up{service=\u0026#34;{{ template \u0026#34;redis.fullname\u0026#34; . }}-metrics\u0026#34;} == 0 # for: 2m # labels: # severity: error # annotations: # summary: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} down # description: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} is down. # - alert: RedisMemoryHigh # expr: \u0026gt; # redis_memory_used_bytes{service=\u0026#34;{{ template \u0026#34;redis.fullname\u0026#34; . }}-metrics\u0026#34;} * 100 # / # redis_memory_max_bytes{service=\u0026#34;{{ template \u0026#34;redis.fullname\u0026#34; . }}-metrics\u0026#34;} # \u0026gt; 90 =\u0026lt; 100 # for: 2m # labels: # severity: error # annotations: # summary: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} is using too much memory # description: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} is using {{ \u0026#34;{{ $value }}\u0026#34; }}% of its available memory. # - alert: RedisKeyEviction # expr: increase(redis_evicted_keys_total{service=\u0026#34;{{ template \u0026#34;redis.fullname\u0026#34; . }}-metrics\u0026#34;}[5m]) \u0026gt; 0 # for: 1s # labels: # severity: error # annotations: # summary: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} has evicted keys # description: Redis instance {{ \u0026#34;{{ $instance }}\u0026#34; }} has evicted {{ \u0026#34;{{ $value }}\u0026#34; }} keys in the last 5 minutes. ## Metrics exporter pod priorityClassName # priorityClassName: {} service: type: ClusterIP ## Use serviceLoadBalancerIP to request a specific static IP, ## otherwise leave blank # loadBalancerIP: annotations: {} labels: {} ## ## Init containers parameters: ## volumePermissions: Change the owner of the persist volume mountpoint to RunAsUser:fsGroup ## volumePermissions: enabled: false image: registry: docker.io repository: bitnami/minideb tag: buster pullPolicy: Always ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName resources: {} # resources: # requests: # memory: 128Mi # cpu: 100m ## Redis config file ## ref: https://redis.io/topics/config ## configmap: |- # Enable AOF https://redis.io/topics/persistence#append-only-file appendonly yes # Disable RDB persistence, AOF persistence already enabled. save \u0026#34;\u0026#34; ## Sysctl InitContainer ## used to perform sysctl operation to modify Kernel settings (needed sometimes to avoid warnings) sysctlImage: enabled: false command: [] registry: docker.io repository: bitnami/minideb tag: buster pullPolicy: Always ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistryKeySecretName mountHostSys: false resources: {} # resources: # requests: # memory: 128Mi # cpu: 100m ## PodSecurityPolicy configuration ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ ## podSecurityPolicy: ## Specifies whether a PodSecurityPolicy should be created ## create: false 部署 Redis 使用 install 命令部署 Chart 至集群中。\ncontrolplane $ helm install stable/redis NAME: iced-ibis LAST DEPLOYED: Mon Jul 26 05:44:02 2021 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE iced-ibis-redis-headless ClusterIP None \u0026lt;none\u0026gt; 6379/TCP 1s iced-ibis-redis-master ClusterIP 10.105.234.211 \u0026lt;none\u0026gt; 6379/TCP 0s iced-ibis-redis-slave ClusterIP 10.107.214.148 \u0026lt;none\u0026gt; 6379/TCP 0s ==\u0026gt; v1/StatefulSet NAME DESIRED CURRENT AGE iced-ibis-redis-master 1 1 0s iced-ibis-redis-slave 2 1 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE iced-ibis-redis-master-0 0/1 Pending 0 0s iced-ibis-redis-slave-0 0/1 Pending 0 0s ==\u0026gt; v1/Secret NAME TYPE DATA AGE iced-ibis-redis Opaque 1 1s ==\u0026gt; v1/ConfigMap NAME DATA AGE iced-ibis-redis 3 1s iced-ibis-redis-health 6 1s NOTES: This Helm chart is deprecated Given the `stable` deprecation timeline (https://github.com/helm/charts#deprecation-timeline), the Bitnami maintained Redis Helm chart is now located at bitnami/charts (https://github.com/bitnami/charts/). The Bitnami repository is already included in the Hubs and we will continue providing the same cadence of updates, support, etc that we\u0026#39;ve been keeping here these years. Installation instructions are very similar, just adding the _bitnami_ repo and using it during the installation (`bitnami/\u0026lt;chart\u0026gt;` instead of `stable/\u0026lt;chart\u0026gt;`) \\\\`\\\\`\\\\`bash $ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm install my-release bitnami/\u0026lt;chart\u0026gt; # Helm 3 $ helm install --name my-release bitnami/\u0026lt;chart\u0026gt; # Helm 2 \\\\`\\\\`\\\\` To update an exisiting _stable_ deployment with a chart hosted in the bitnami repository you can execute \\\\`\\\\`\\\\`bash $ helm repo add bitnami https://charts.bitnami.com/bitnami $ helm upgrade my-release bitnami/\u0026lt;chart\u0026gt; \\\\`\\\\`\\\\` Issues and PRs related to the chart itself will be redirected to `bitnami/charts` GitHub repository. In the same way, we\u0026#39;ll be happy to answer questions related to this migration process in this issue (https://github.com/helm/charts/issues/20969) created as a common place for discussion. ** Please be patient while the chart is being deployed ** Redis can be accessed via port 6379 on the following DNS names from within your cluster: iced-ibis-redis-master.default.svc.cluster.local for read/write operations iced-ibis-redis-slave.default.svc.cluster.local for read-only operations To get your password run: export REDIS_PASSWORD=$(kubectl get secret --namespace default iced-ibis-redis -o jsonpath=\u0026#34;{.data.redis-password}\u0026#34; | base64 --decode) To connect to your Redis server: 1. Run a Redis pod that you can use as a client: kubectl run --namespace default iced-ibis-redis-client --rm --tty -i --restart=\u0026#39;Never\u0026#39; \\  --env REDIS_PASSWORD=$REDIS_PASSWORD \\  --image docker.io/bitnami/redis:5.0.7-debian-10-r32 -- bash 2. Connect using the Redis CLI: redis-cli -h iced-ibis-redis-master -a $REDIS_PASSWORD redis-cli -h iced-ibis-redis-slave -a $REDIS_PASSWORD To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace default svc/iced-ibis-redis-master 6379:6379 \u0026amp; redis-cli -h 127.0.0.1 -p 6379 -a $REDIS_PASSWORD Helm 现在将启动所需的 Pod。您可以使用 helm ls 查看所有包。\ncontrolplane $ helm ls NAME REVISION UPDATED STATUS CHART NAMESPACE iced-ibis 1 Mon Jul 26 05:44:02 2021 DEPLOYED redis-10.5.7 default 如果您收到 Helm 无法找到准备好的 Pod 的错误消息，则表示 helm 仍在部署中。稍等片刻，直到 Docker Image 完成下载。\n在下一步中，我们将验证部署状态。\n查看结果 Helm 部署了所有 Pod、Replication Controller 和 Controller 。使用 kubectl 找出部署的内容。\ncontrolplane $ kubectl get all NAME READY STATUS RESTARTS AGE pod/iced-ibis-redis-master-0 0/1 Pending 0 12m pod/iced-ibis-redis-slave-0 0/1 Pending 0 12m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/iced-ibis-redis-headless ClusterIP None \u0026lt;none\u0026gt; 6379/TCP 12m service/iced-ibis-redis-master ClusterIP 10.105.234.211 \u0026lt;none\u0026gt; 6379/TCP 12m service/iced-ibis-redis-slave ClusterIP 10.107.214.148 \u0026lt;none\u0026gt; 6379/TCP 12m service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 53m NAME READY AGE statefulset.apps/iced-ibis-redis-master 0/1 12m statefulset.apps/iced-ibis-redis-slave 0/2 12m 在下载 Docker 镜像时，Pod 将处于 pending 状态，直到 Persistent Volume 可用。\ncontrolplane $ kubectl apply -f pv.yaml persistentvolume/pv-volume1 created persistentvolume/pv-volume2 created persistentvolume/pv-volume3 created pv.yaml\nkind:PersistentVolumeapiVersion:v1metadata:name:pv-volume1labels:type:localspec:capacity:storage:10GiaccessModes:- ReadWriteOncehostPath:path:\u0026#34;/mnt/data1\u0026#34;---kind:PersistentVolumeapiVersion:v1metadata:name:pv-volume2labels:type:localspec:capacity:storage:10GiaccessModes:- ReadWriteOncehostPath:path:\u0026#34;/mnt/data2\u0026#34;---kind:PersistentVolumeapiVersion:v1metadata:name:pv-volume3labels:type:localspec:capacity:storage:10GiaccessModes:- ReadWriteOncehostPath:path:\u0026#34;/mnt/data3\u0026#34;Redis 需要写入权限。\ncontrolplane $ chmod 777 -R /mnt/data* controlplane $ kubectl get all NAME READY STATUS RESTARTS AGE pod/iced-ibis-redis-master-0 1/1 Running 3 18m pod/iced-ibis-redis-slave-0 0/1 CrashLoopBackOff 3 18m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/iced-ibis-redis-headless ClusterIP None \u0026lt;none\u0026gt; 6379/TCP 18m service/iced-ibis-redis-master ClusterIP 10.105.234.211 \u0026lt;none\u0026gt; 6379/TCP 18m service/iced-ibis-redis-slave ClusterIP 10.107.214.148 \u0026lt;none\u0026gt; 6379/TCP 18m service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 58m NAME READY AGE statefulset.apps/iced-ibis-redis-master 1/1 18m statefulset.apps/my-release-redis-master 1/1 36s 一旦完成，它将进入running 状态。您现在将拥有一个在 Kubernetes 之上运行的 Redis 集群。\nHelm 能够为部署的组件提供自定义的名称，例如：\nhelm install --name my-release stable/redis ","date":"2021-07-23T14:54:00+08:00","image":"https://www.catfish.top/p/k8s-basic-11/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-11/","title":"Kubernetes初探（十一）"},{"content":" Katacoda在线课：Use Kubernetes To Manage Secrets And Passwords\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，您将了解如何使用 Kubernetes 管理 Secrets 。 Kubernetes 允许您创建通过环境变量或作为卷挂载到 Pod 的 Secrets 。\n只允许 Secrets （例如 SSL 证书或密码）通过基础架构团队以安全的方式进行管理，而不是将密码存储在应用程序的部署工件中。\n启动 Kubernetes 首先，我们需要启动一个 Kubernetes 集群。\n执行以下命令启动集群组件并下载 Kubectl CLI。\ncontrolplane $ launch.sh Waiting for Kubernetes to start... Kubernetes started 创建 Secrets Kubernetes 要求将 Secrets 编码为 Base64 的字符串。\n使用命令行工具，我们可以创建 Base64 字符串并将它们存储为变量在文件中使用。\ncontrolplane $ username=$(echo -n \u0026#34;admin\u0026#34; | base64) controlplane $ password=$(echo -n \u0026#34;a62fjbd37942dcs\u0026#34; | base64) Secrets 是使用 YAML 定义的。下面我们将使用上面定义的变量，并为它们提供我们的应用程序可以使用的标签。这将创建一个可以通过名称访问的键/值秘密的集合。\necho \u0026#34;apiVersion: v1 kind: Secret metadata: name: test-secret type: Opaque data: username: $usernamepassword: $password\u0026#34; \u0026gt;\u0026gt; secret.yaml secret.yaml\napiVersion:v1kind:Secretmetadata:name:test-secrettype:Opaquedata:username:YWRtaW4=password:YTYyZmpiZDM3OTQyZGNz这个 YAML 文件中定义可以与 Kubectl 一起使用来创建我们的 Secrets 。在启动需要访问密钥的 Pod 时，我们将通过名称引用集合。\n任务: 创建 Secret 使用 kubectl 创建 Secret\ncontrolplane $ kubectl create -f secret.yaml secret/test-secret created 以下命令允许您查看定义的所有Secret集合。\ncontrolplane $ kubectl get secrets NAME TYPE DATA AGE default-token-z8shh kubernetes.io/service-account-token 3 14m test-secret Opaque 2 26s 在下一步中，我们将通过 Pod 使用这些 Secret 。\n通过环境变量使用 Secret 在文件 secret-env.yaml 中，我们定义了一个有先前创建的 Secret 为环境变量的 Pod 。\n使用 cat secret-env.yaml 查看文件\nsecret-env.yaml\napiVersion:v1kind:Podmetadata:name:secret-env-podspec:containers:- name:mycontainerimage:alpine:latestcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;9999\u0026#34;]env:- name:SECRET_USERNAMEvalueFrom:secretKeyRef:name:test-secretkey:username- name:SECRET_PASSWORDvalueFrom:secretKeyRef:name:test-secretkey:passwordrestartPolicy:Never为了填充环境变量，我们定义了名称，在本例中为 SECRET_USERNAME，以及 Secret 集合的名称和包含数据的密钥。\n结构如下所示：\n- name:SECRET_USERNAMEvalueFrom:secretKeyRef:name:test-secretkey:username任务 使用 kubectl create -f secret-env.yaml 启动 Pod 。\ncontrolplane $ kubectl create -f secret-env.yaml pod/secret-env-pod created Pod 启动后，将输出填充的环境变量。\ncontrolplane $ kubectl exec -it secret-env-pod env | grep SECRET_ SECRET_USERNAME=admin SECRET_PASSWORD=a62fjbd37942dcs Kubernetes 在填充环境变量时解码 base64 值。您应该会看到我们定义的原始用户名/密码组合。这些变量现在可用于访问 API、数据库等。\n您可以使用 kubectl get pods 检查 Pod 的状态。\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE secret-env-pod 1/1 Running 0 34s 在下一步中，我们将把 Secret 挂载为文件。\n通过文件使用 Secret 使用环境变量在内存中存储 Secret 可能会导致它们意外泄漏。推荐的方法是将它们作为卷安装。\nPod 定义可以使用 cat secret-pod.yaml 查看。\nsecret-pod.yaml\napiVersion:v1kind:Podmetadata:name:secret-vol-podspec:volumes:- name:secret-volumesecret:secretName:test-secretcontainers:- name:test-containerimage:alpine:latestcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;9999\u0026#34;]volumeMounts:- name:secret-volumemountPath:/etc/secret-volume要将 Secret 安装为卷，我们首先定义一个具有名称的卷，在本例中为 Secret 卷，并为其提供我们存储的 Secret 。\nvolumes:- name:secret-volumesecret:secretName:test-secret当我们定义容器时，我们将创建的卷挂载到特定目录。应用程序将从该路径读取Secret 作为文件。\nvolumeMounts:- name:secret-volumemountPath:/etc/secret-volume任务 使用 kubectl create -f secret-pod.yaml 创建我们的新 Pod 。\ncontrolplane $ kubectl create -f secret-pod.yaml pod/secret-vol-pod created 启动后，您可以与安装的Secret 进行交互。例如，您可以列出所有可用的 Secret ，就好像它们是常规数据一样。\ncontrolplane $ kubectl exec -it secret-vol-pod ls /etc/secret-volume password username 读取文件允许我们访问解码的 Secret 值。要访问我们使用的用户名\ncontrolplane $ kubectl exec -it secret-vol-pod cat /etc/secret-volume/username admin 对于密码，我们会读取密码文件。\nadmincontrolplane $ kubectl exec -it secret-vol-pod cat /etc/secret-volume/password a62fjbd37942dcs ","date":"2021-07-23T14:54:00+08:00","image":"https://www.catfish.top/p/k8s-basic-10/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-10/","title":"Kubernetes初探（十）"},{"content":" Katacoda在线课：Running Stateful Services on Kubernetes\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 部署 NFS 服务器 NFS 是一种允许节点通过网络读 / 写数据的协议。该协议的工作原理是让主节点运行 NFS 守护程序并存储数据。此主节点使某些目录可通过网络使用。\n客户端访问通过驱动器挂载共享的主服务器。从应用程序的角度来看，它们正在写入本地磁盘。在背后，NFS 协议将其写入主服务器。\n任务 在此场景中，出于演示和学习目的，NFS 服务器的角色由自定义容器处理。容器通过 NFS 提供目录并将数据存储在容器内。在生产环境中，建议配置专用的 NFS Server。\n使用 docker run -d --net=host \\ --privileged --name nfs-server \\ katacoda/contained-nfs-server:centos7 \\ /exports/data-0001 /exports/data-0002 命令启动 NFS 服务器\ncontrolplane $ docker run -d --net=host \\ \u0026gt; --privileged --name nfs-server \\ \u0026gt; katacoda/contained-nfs-server:centos7 \\ \u0026gt; /exports/data-0001 /exports/data-0002 Unable to find image \u0026#39;katacoda/contained-nfs-server:centos7\u0026#39; locally centos7: Pulling from katacoda/contained-nfs-server 8d30e94188e7: Pull complete 2b2b27f1f462: Pull complete 133e63cf95fe: Pull complete Digest: sha256:5f2ea4737fe27f26be5b5cabaa23e24180079a4dce8d5db235492ec48c5552d1 Status: Downloaded newer image for katacoda/contained-nfs-server:centos7 65699a0a96bfc489fe2141a815ef12b03917f9bb667340b1be68dfe838d14bf3 NFS 服务器公开两个目录，data-0001 和 data-0002。在接下来的步骤中，这将用于存储数据。\n部署 Persistent Volume 为了让 Kubernetes 了解可用的 NFS 共享，它需要一个 PersistentVolume 配置。 PersistentVolume 支持不同的数据存储协议，例如 AWS EBS 卷、GCE 存储、OpenStack Cinder、Glusterfs 和 NFS。该配置提供了存储和 API 之间的抽象，从而实现了一致的体验。\n在使用 NFS 的情况下，一个 PersistentVolume 与一个 NFS 目录相关联。当容器使用完卷后，数据可以 保留 以备将来使用，也可以 回收，这意味着所有数据都将被删除。该策略由 persistentVolumeReclaimPolicy 选项定义。\nYAML 文件结构：\napiVersion:v1kind:PersistentVolumemetadata:name:\u0026lt;friendly-name\u0026gt;spec:capacity:storage:1GiaccessModes:- ReadWriteOnce- ReadWriteManypersistentVolumeReclaimPolicy:Recyclenfs:server:\u0026lt;server-name\u0026gt;path:\u0026lt;shared-path\u0026gt;该规范定义了关于 PersistentVolume 的额外元数据，包括有多少可用空间以及它是否具有读 / 写访问权限。\n任务 使用 cat nfs-0001.yaml nfs-0002.yaml 查看 YAML 文件的内容。\nnfs-0001.yaml\napiVersion:v1kind:PersistentVolumemetadata:name:nfs-0001spec:capacity:storage:2GiaccessModes:- ReadWriteOnce- ReadWriteManypersistentVolumeReclaimPolicy:Recyclenfs:server:172.17.0.45path:/exports/data-0001nfs-0002.yaml\napiVersion:v1kind:PersistentVolumemetadata:name:nfs-0002spec:capacity:storage:5GiaccessModes:- ReadWriteOnce- ReadWriteManypersistentVolumeReclaimPolicy:Recyclenfs:server:172.17.0.45path:/exports/data-0002创建两个新的 PersistentVolume 定义并指向两个可用的 NFS 共享。\ncontrolplane $ kubectl create -f nfs-0001.yaml persistentvolume/nfs-0001 created controlplane $ kubectl create -f nfs-0002.yaml persistentvolume/nfs-0002 created 创建后，使用 kubectl get pv 命令查看集群中的所有 PersistentVolumes\ncontrolplane $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfs-0001 2Gi RWO,RWX Recycle Available 66s nfs-0002 5Gi RWO,RWX Recycle Available 61s 部署 Persistent Volume Claim 一旦 PersistentVolume 可用，应用程序就可以声明该卷供其使用。该声明旨在阻止应用程序意外写入同一卷并导致冲突和数据损坏。\nPersistent Volume Claim 指定了卷的要求。这包括所需的读/写访问和存储空间。一个例子如下：\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:claim-mysqlspec:accessModes:- ReadWriteOnceresources:requests:storage:3Gi任务 使用 cat pvc-mysql.yaml pvc-http.yaml 查看文件的内容\npvc-mysql.yaml\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:claim-mysqlspec:accessModes:- ReadWriteOnceresources:requests:storage:3Gipvc-http.yaml\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:claim-httpspec:accessModes:- ReadWriteOnceresources:requests:storage:1Gi为两个不同的应用程序创建两个声明。 MySQL Pod 将使用一个声明，另一个由 HTTP 服务器使用。\ncontrolplane $ kubectl create -f pvc-mysql.yaml persistentvolumeclaim/claim-mysql created controlplane $ kubectl create -f pvc-http.yaml persistentvolumeclaim/claim-http created 创建后，使用 kubectl get pvc 查看集群中的所有 PersistentVolumesClaims，将打印出声明映射到的卷。\ncontrolplane $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim-http Bound nfs-0001 2Gi RWO,RWX 53s claim-mysql Bound nfs-0002 5Gi RWO,RWX 56s Volume 的使用 定义部署后，它可以将自己分配给先前的声明。以下代码段定义了目录 /var/lib/mysql/data 的卷挂载，该目录映射到存储 mysql-persistent-storage。名为 mysql-persistent-storage 的存储映射到名为 claim-mysql 的声明中。\nspec:volumeMounts:- name:mysql-persistent-storagemountPath:/var/lib/mysql/datavolumes:- name:mysql-persistent-storagepersistentVolumeClaim:claimName:claim-mysql任务 使用以下命令查看 Pod 的定义。\ncontrolplane $ cat pod-mysql.yaml pod-www.yaml pod-mysql.yaml\napiVersion:v1kind:Podmetadata:name:mysqllabels:name:mysqlspec:containers:- name:mysqlimage:openshift/mysql-55-centos7env:- name:MYSQL_ROOT_PASSWORDvalue:yourpassword- name:MYSQL_USERvalue:wp_user- name:MYSQL_PASSWORDvalue:wp_pass- name:MYSQL_DATABASEvalue:wp_dbports:- containerPort:3306name:mysqlvolumeMounts:- name:mysql-persistent-storagemountPath:/var/lib/mysql/datavolumes:- name:mysql-persistent-storagepersistentVolumeClaim:claimName:claim-mysqlpod-http.yaml\napiVersion:v1kind:Podmetadata:name:wwwlabels:name:wwwspec:containers:- name:wwwimage:nginx:alpineports:- containerPort:80name:wwwvolumeMounts:- name:www-persistent-storagemountPath:/usr/share/nginx/htmlvolumes:- name:www-persistent-storagepersistentVolumeClaim:claimName:claim-http启动两个具有 Persistent Volume Claims 的新 Pod。当 Pod 开始允许应用程序像本地目录一样读/写时，卷被映射到正确的目录。\ncontrolplane $ kubectl create -f pod-mysql.yaml pod/mysql created controlplane $ kubectl create -f pod-www.yaml pod/www created 可以使用 kubectl get pods 开始查看 Pod 的状态\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE mysql 1/1 Running 0 27s www 1/1 Running 0 25s 如果 Persistent Volume Claim 未分配给 Persistent Volume，则 Pod 将处于 Pending 模式，直到它变为可用。在下一步中，我们将向卷读/写数据。\n数据读写 我们的 Pod 现在可以进行读/写。 MySQL 将所有数据库更改存储到 NFS 服务器，而 HTTP 服务器将从 NFS 驱动器提供静态服务。升级、重新启动或将容器移动到不同的机器时，数据仍然可以访问。\n要测试 HTTP 服务器，请编写一个“Hello World”index.html 主页。在这种情况下，我们知道 HTTP 目录将基于 data-0001，因为卷定义没有驱动足够的空间来满足 MySQL 大小要求。\ncontrolplane $ docker exec -it nfs-server bash -c \u0026#34;echo \u0026#39;Hello World\u0026#39; \u0026gt; /exports/data-0001/index.html\u0026#34; 根据 Pod 的 IP，访问 Pod 时，应该返回预期的响应。\ncontrolplane $ ip=$(kubectl get pod www -o yaml |grep podIP | awk \u0026#39;{split($0,a,\u0026#34;:\u0026#34;); print a[2]}\u0026#39;); echo $ip 10.32.0.6 controlplane $ curl $ip Hello World 更新数据 当 NFS 共享上的数据发生变化时，Pod 会读取新更新的数据。\ncontrolplane $ docker exec -it nfs-server bash -c \u0026#34;echo \u0026#39;Hello NFS World\u0026#39; \u0026gt; /exports/data-0001/index.html\u0026#34; controlplane $ curl $ip Hello NFS World 重建 Pod 因为使用远程 NFS 服务器存储数据，如果 Pod 或主机宕机，那么数据仍然可用。\n任务 删除 Pod 将导致它删除对任何持久卷的声明。新 Pod 可以获取并重新使用 NFS 共享。\npod-www2.yaml\napiVersion:v1kind:Podmetadata:name:www2labels:name:www2spec:containers:- name:www2image:nginx:alpineports:- containerPort:80name:www2volumeMounts:- name:www-persistent-storagemountPath:/usr/share/nginx/htmlvolumes:- name:www-persistent-storagepersistentVolumeClaim:claimName:claim-httpcontrolplane $ kubectl delete pod www pod \u0026#34;www\u0026#34; deleted controlplane $ kubectl create -f pod-www2.yaml pod/www2 created controlplane $ ip=$(kubectl get pod www2 -o yaml |grep podIP | awk \u0026#39;{split($0,a,\u0026#34;:\u0026#34;); print a[2]}\u0026#39;); curl $ip Hello NFS World 应用程序现在使用远程 NFS 进行数据存储。根据具体要求，这种相同的方法适用于其他存储引擎，例如 GlusterFS、AWS EBS、GCE 存储或 OpenStack Cinder。\n","date":"2021-07-23T14:23:00+08:00","image":"https://www.catfish.top/p/k8s-basic-9/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-9/","title":"Kubernetes初探（九）"},{"content":" Katacoda在线课：Liveness and Readiness Healthchecks\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，您将了解 Kubernetes 如何使用 Readiness and Liveness Probes 检查容器运行状况。\nReadiness Probe 检查应用是否准备好开始处理流量。此探针解决了容器已启动的问题，但该进程仍在预热和配置自身，这意味着它尚未准备好接收流量。\nLiveness Probe 确保应用程序健康并能够处理请求。\n启动集群 首先，我们需要启动一个 Kubernetes 集群。\n执行以下命令启动集群组件并下载 Kubectl CLI\ncontrolplane $ launch.sh Waiting for Kubernetes to start... Kubernetes started 集群启动后，使用 kubectl apply -f deploy.yaml 部署演示应用程序。\ndeploy.yaml\nkind:ListapiVersion:v1items:- kind:ReplicationControllerapiVersion:v1metadata:name:frontendlabels:name:frontendspec:replicas:1selector:name:frontendtemplate:metadata:labels:name:frontendspec:containers:- name:frontendimage:katacoda/docker-http-server:healthreadinessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1livenessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1- kind:ReplicationControllerapiVersion:v1metadata:name:bad-frontendlabels:name:bad-frontendspec:replicas:1selector:name:bad-frontendtemplate:metadata:labels:name:bad-frontendspec:containers:- name:bad-frontendimage:katacoda/docker-http-server:unhealthyreadinessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1livenessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1- kind:ServiceapiVersion:v1metadata:labels:app:frontendkubernetes.io/cluster-service:\u0026#34;true\u0026#34;name:frontendspec:type:NodePortports:- port:80nodePort:30080selector:app:frontendcontrolplane $ kubectl apply -f deploy.yaml replicationcontroller/frontend created replicationcontroller/bad-frontend created service/frontend created Readiness Probe 在部署集群时，还部署了两个 Pod 来演示健康检查。您可以使用 cat deploy.yaml 查看部署。\nlivenessProbe:httpGet:path:/port:80initialDelaySeconds:1timeoutSeconds:1可以根据您的应用程序更改设置来调用不同的端点，例如 /ping。\n获取状态 第一个 Pod bad-frontend 是一个 HTTP 服务，它总是返回 500 错误，表明它没有正确启动。您可以使用 kubectl get pods --selector=\u0026quot;name=bad-frontend\u0026quot; 命令查看 Pod 的状态。\ncontrolplane $ kubectl get pods --selector=\u0026#34;name=bad-frontend\u0026#34; NAME READY STATUS RESTARTS AGE bad-frontend-jk5z2 0/1 Running 2 78s Kubectl 将返回使用我们的特定标签部署的 Pod。因为健康检查失败，它会显示没有容器为准备就绪的状态，同时它还将显示容器的重启尝试次数。\n要了解有关失败原因的更多详细信息，请对该 Pod 使用描述命令。\ncontrolplane $ pod=$(kubectl get pods --selector=\u0026#34;name=bad-frontend\u0026#34; --output=jsonpath={.items..metadata.name}) controlplane $ kubectl describe pod $pod Name: bad-frontend-jk5z2 Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: controlplane/172.17.0.67 Start Time: Fri, 23 Jul 2021 05:54:59 +0000 Labels: name=bad-frontend Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.32.0.6 Controlled By: ReplicationController/bad-frontend Containers: bad-frontend: Container ID: docker://811c3fa6d76c13f4b5bc7a6b2d6f514292e9673be46572a79b8f2d3d5e36bc62 Image: katacoda/docker-http-server:unhealthy Image ID: docker-pullable://katacoda/docker-http-server@sha256:bea95c69c299c690103c39ebb3159c39c5061fee1dad13aa1b0625e0c6b52f22 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 2 Started: Fri, 23 Jul 2021 05:57:07 +0000 Finished: Fri, 23 Jul 2021 05:57:37 +0000 Ready: False Restart Count: 4 Liveness: http-get http://:80/ delay=1s timeout=1s period=10s #success=1 #failure=3 Readiness: http-get http://:80/ delay=1s timeout=1s period=10s #success=1 #failure=3 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-5n24z (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: default-token-5n24z: Type: Secret (a volume populated by a Secret) SecretName: default-token-5n24z Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m53s default-scheduler Successfully assigned default/bad-frontend-jk5z2 to controlplane Normal Pulling 2m52s kubelet, controlplane Pulling image \u0026#34;katacoda/docker-http-server:unhealthy\u0026#34; Normal Pulled 2m45s kubelet, controlplane Successfully pulled image \u0026#34;katacoda/docker-http-server:unhealthy\u0026#34; Normal Created 105s (x3 over 2m45s) kubelet, controlplane Created container bad-frontend Normal Started 105s (x3 over 2m45s) kubelet, controlplane Started container bad-frontend Warning Unhealthy 105s (x6 over 2m35s) kubelet, controlplane Liveness probe failed: HTTP probe failed with statuscode: 500 Normal Killing 105s (x2 over 2m15s) kubelet, controlplane Container bad-frontend failed liveness probe, will be restarted Normal Pulled 105s (x2 over 2m15s) kubelet, controlplane Container image \u0026#34;katacoda/docker-http-server:unhealthy\u0026#34; already present on machine Warning Unhealthy 103s (x7 over 2m43s) kubelet, controlplane Readiness probe failed: HTTP probe failed with statuscode: 500 Readiness 我们的第二个 Pod，frontend，在启动时返回 OK 状态。\ncontrolplane $ kubectl get pods --selector=\u0026#34;name=frontend\u0026#34; NAME READY STATUS RESTARTS AGE frontend-54czd 1/1 Running 0 4m9s Liveness Probe 由于我们的第二个 Pod 当前处于健康状态，我们可以模拟发生的故障。\n目前，该 Pod 应该没有发生崩溃。\ncontrolplane $ kubectl get pods --selector=\u0026#34;name=frontend\u0026#34; NAME READY STATUS RESTARTS AGE frontend-54czd 1/1 Running 0 7m17s 让服务崩溃 HTTP 服务器有一个额外的端点，这将导致它返回 500 错误。使用 kubectl exec 可以调用端点。\ncontrolplane $ pod=$(kubectl get pods --selector=\u0026#34;name=frontend\u0026#34; --output=jsonpath={.items..metadata.name}) controlplane $ kubectl exec $pod -- /usr/bin/curl -s localhost/unhealthy Liveness Kubernetes 将根据配置执行 Liveness Probe。如果探测器失败，Kubernetes 将销毁并重新创建失败的容器。执行上面的命令使服务崩溃并观察 Kubernetes 自动恢复它。\ncontrolplane $ kubectl get pods --selector=\u0026#34;name=frontend\u0026#34; NAME READY STATUS RESTARTS AGE frontend-54czd 1/1 Running 1 9m42s 检查可能需要一些时间才能检测到。\n","date":"2021-07-23T13:07:00+08:00","image":"https://www.catfish.top/p/k8s-basic-8/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-8/","title":"Kubernetes初探（八）"},{"content":" Katacoda在线课：Create Ingress Routing\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n Kubernetes 具有先进的网络功能，允许 Pod 和 Service 在集群网络内部进行通信。 Ingress 开启了到集群的入站连接，允许外部流量到达正确的 Pod。\nIngress 能够提供外部可访问的 URL、负载平衡流量、终止 SSL、为 Kubernetes 集群提供基于名称的虚拟主机。\n在此场景中，您将学习如何部署和配置 Ingress 规则来管理传入的 HTTP 请求。\n创建 Deployment 首先，部署一个示例 HTTP 服务器，它将成为我们请求的目标。部署中包含三个部署，分别为 webapp1， webapp2， webapp3，每个部署都有一个服务。\ndeployment.yaml\napiVersion:apps/v1kind:Deploymentmetadata:name:webapp1spec:replicas:1selector:matchLabels:app:webapp1template:metadata:labels:app:webapp1spec:containers:- name:webapp1image:katacoda/docker-http-server:latestports:- containerPort:80---apiVersion:apps/v1kind:Deploymentmetadata:name:webapp2spec:replicas:1selector:matchLabels:app:webapp2template:metadata:labels:app:webapp2spec:containers:- name:webapp2image:katacoda/docker-http-server:latestports:- containerPort:80---apiVersion:apps/v1kind:Deploymentmetadata:name:webapp3spec:replicas:1selector:matchLabels:app:webapp3template:metadata:labels:app:webapp3spec:containers:- name:webapp3image:katacoda/docker-http-server:latestports:- containerPort:80---apiVersion:v1kind:Servicemetadata:name:webapp1-svclabels:app:webapp1spec:ports:- port:80selector:app:webapp1---apiVersion:v1kind:Servicemetadata:name:webapp2-svclabels:app:webapp2spec:ports:- port:80selector:app:webapp2---apiVersion:v1kind:Servicemetadata:name:webapp3-svclabels:app:webapp3spec:ports:- port:80selector:app:webapp3任务 使用 kubectl apply -f deployment.yaml 命令部署 YAML 定义。\ncontrolplane $ kubectl apply -f deployment.yaml deployment.apps/webapp1 created deployment.apps/webapp2 created deployment.apps/webapp3 created service/webapp1-svc created service/webapp2-svc created service/webapp3-svc created 可以用 kubectl get deployment 查看状态。\ncontrolplane $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE webapp1 1/1 1 1 15s webapp2 1/1 1 1 15s webapp3 1/1 1 1 15s controlplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 7m41s webapp1-svc ClusterIP 10.98.236.192 \u0026lt;none\u0026gt; 80/TCP 41s webapp2-svc ClusterIP 10.105.201.139 \u0026lt;none\u0026gt; 80/TCP 41s webapp3-svc ClusterIP 10.103.159.185 \u0026lt;none\u0026gt; 80/TCP 41s 部署 Ingress ingress.yaml 文件中定义了一个基于 Nginx 的 Ingress 控制器以及一个 Service ，开放 80 端口用于使用 External IPs 的外部连接。如果 Kubernetes 集群在云服务商上运行，那么它将为 LoadBalancer 服务类型。\nServiceAccount 定义了带有权限的、连接集群以操作定义的入口规则的一组的帐户。默认服务器密钥是其他 Nginx 示例 SSL 连接的自签名证书，并且是Nginx Default Example 中所必须的。\ningress.yaml\napiVersion:v1kind:Namespacemetadata:name:nginx-ingress---apiVersion:v1kind:Secretmetadata:name:default-server-secretnamespace:nginx-ingresstype:kubernetes.io/tlsdata:tls.crt:LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN2akNDQWFZQ0NRREFPRjl0THNhWFhEQU5CZ2txaGtpRzl3MEJBUXNGQURBaE1SOHdIUVlEVlFRRERCWk8KUjBsT1dFbHVaM0psYzNORGIyNTBjbTlzYkdWeU1CNFhEVEU0TURreE1qRTRNRE16TlZvWERUSXpNRGt4TVRFNApNRE16TlZvd0lURWZNQjBHQTFVRUF3d1dUa2RKVGxoSmJtZHlaWE56UTI5dWRISnZiR3hsY2pDQ0FTSXdEUVlKCktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUwvN2hIUEtFWGRMdjNyaUM3QlBrMTNpWkt5eTlyQ08KR2xZUXYyK2EzUDF0azIrS3YwVGF5aGRCbDRrcnNUcTZzZm8vWUk1Y2Vhbkw4WGM3U1pyQkVRYm9EN2REbWs1Qgo4eDZLS2xHWU5IWlg0Rm5UZ0VPaStlM2ptTFFxRlBSY1kzVnNPazFFeUZBL0JnWlJVbkNHZUtGeERSN0tQdGhyCmtqSXVuektURXUyaDU4Tlp0S21ScUJHdDEwcTNRYzhZT3ExM2FnbmovUWRjc0ZYYTJnMjB1K1lYZDdoZ3krZksKWk4vVUkxQUQ0YzZyM1lma1ZWUmVHd1lxQVp1WXN2V0RKbW1GNWRwdEMzN011cDBPRUxVTExSakZJOTZXNXIwSAo1TmdPc25NWFJNV1hYVlpiNWRxT3R0SmRtS3FhZ25TZ1JQQVpQN2MwQjFQU2FqYzZjNGZRVXpNQ0F3RUFBVEFOCkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQWpLb2tRdGRPcEsrTzhibWVPc3lySmdJSXJycVFVY2ZOUitjb0hZVUoKdGhrYnhITFMzR3VBTWI5dm15VExPY2xxeC9aYzJPblEwMEJCLzlTb0swcitFZ1U2UlVrRWtWcitTTFA3NTdUWgozZWI4dmdPdEduMS9ienM3bzNBaS9kclkrcUI5Q2k1S3lPc3FHTG1US2xFaUtOYkcyR1ZyTWxjS0ZYQU80YTY3Cklnc1hzYktNbTQwV1U3cG9mcGltU1ZmaXFSdkV5YmN3N0NYODF6cFErUyt1eHRYK2VBZ3V0NHh3VlI5d2IyVXYKelhuZk9HbWhWNThDd1dIQnNKa0kxNXhaa2VUWXdSN0diaEFMSkZUUkk3dkhvQXprTWIzbjAxQjQyWjNrN3RXNQpJUDFmTlpIOFUvOWxiUHNoT21FRFZkdjF5ZytVRVJxbStGSis2R0oxeFJGcGZnPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=tls.key: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---apiVersion:v1kind:ServiceAccountmetadata:name:nginx-ingress namespace:nginx-ingress---kind:ConfigMapapiVersion:v1metadata:name:nginx-confignamespace:nginx-ingressdata:---# Described at: https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-manifests/# Source from: https://github.com/nginxinc/kubernetes-ingress/blob/master/deployments/common/ingress-class.yamlapiVersion:networking.k8s.io/v1beta1kind:IngressClassmetadata:name:nginx# annotations:# ingressclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34;spec:controller:nginx.org/ingress-controller---apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-ingressnamespace:nginx-ingressspec:replicas:1selector:matchLabels:app:nginx-ingresstemplate:metadata:labels:app:nginx-ingressspec:serviceAccountName:nginx-ingresscontainers:- image:nginx/nginx-ingress:edgeimagePullPolicy:Alwaysname:nginx-ingressports:- name:httpcontainerPort:80- name:httpscontainerPort:443env:- name:POD_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespace- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.nameargs:- -nginx-configmaps=$(POD_NAMESPACE)/nginx-config- -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret---apiVersion:v1kind:Servicemetadata:name:nginx-ingressnamespace:nginx-ingressspec:type:NodePort ports:- port:80targetPort:80protocol:TCPname:http- port:443targetPort:443protocol:TCPname:httpsselector:app:nginx-ingressexternalIPs:- 172.17.0.46任务 Ingress 控制器以同样的方式部署到 Kubernetes 集群，使用 kubectl create -f ingress.yaml命令操作。\ncontrolplane $ kubectl create -f ingress.yaml namespace/nginx-ingress created secret/default-server-secret created serviceaccount/nginx-ingress created configmap/nginx-config created ingressclass.networking.k8s.io/nginx created deployment.apps/nginx-ingress created service/nginx-ingress created 可以使用 kubectl get deployment -n nginx-ingress 查看状态。\ncontrolplane $ kubectl get deployment -n nginx-ingress NAME READY UP-TO-DATE AVAILABLE AGE nginx-ingress 1/1 1 1 35s 部署 Ingress Rules Ingress Rules 是 Kubernetes 的对象类型。规则可以基于请求主机（域），或请求的路径，或两者的组合。\ncat ingress-rules.yaml 中定义了一组示例规则。\ningress-rules.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: webapp-ingress spec: ingressClassName: nginx rules: - host: my.kubernetes.example http: paths: - path: /webapp1 backend: serviceName: webapp1-svc servicePort: 80 - path: /webapp2 backend: serviceName: webapp2-svc servicePort: 80 - backend: serviceName: webapp3-svc servicePort: 80 规则的重要部分定义如下。\n这些规则适用于对主机 my.kubernetes.example 的请求。基于路径请求定义了两个规则，并使用一个 catch all 定义。对路径 /webapp1 的请求被转发到服务 webapp1-svc。同样，对 /webapp2 的请求被转发到 webapp2-svc。如果没有规则适用，将使用 webapp3-svc。\n这演示了应用的 URL 结构如何独立于应用程序的部署方式。\n任务 与所有 Kubernetes 对象一样，它们可以通过 kubectl create -f ingress-rules.yaml 进行部署。\ncontrolplane $ kubectl create -f ingress-rules.yaml ingress.extensions/webapp-ingress created 部署后，可以通过 kubectl get ing 查看所有 Ingress 规则的状态\ncontrolplane $ kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE webapp-ingress nginx my.kubernetes.example 80 41s 测试 应用入口规则后，流量将被路由到定义的位置。\n第一个请求将由 webapp1 处理。\ncontrolplane $ curl -H \u0026#34;Host: my.kubernetes.example\u0026#34; 172.17.0.68/webapp1 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-7c456784b7-cg6sk\u0026lt;/h1\u0026gt; 第二个请求将由 webapp2 处理。\ncontrolplane $ curl -H \u0026#34;Host: my.kubernetes.example\u0026#34; 172.17.0.68/webapp2 \u0026lt;h1\u0026gt;This request was processed by host: webapp2-79f9947d45-25j6l\u0026lt;/h1\u0026gt; 最后，所有的其他请求将由 webapp3 处理。\ncontrolplane $ curl -H \u0026#34;Host: my.kubernetes.example\u0026#34; 172.17.0.68 \u0026lt;h1\u0026gt;This request was processed by host: webapp3-777f4dd675-n2pqs\u0026lt;/h1\u0026gt; ","date":"2021-07-20T15:30:00+08:00","image":"https://www.catfish.top/p/k8s-basic-7/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-7/","title":"Kubernetes初探（七）"},{"content":" Katacoda在线课：Networking Introduction\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n Kubernetes 具有先进的网络功能，允许 Pod 和 Service 在集群网络内部和外部进行通信。\n在此场景中，您将学习以下类型的 Kubernetes Service。\n Cluster IP Target Ports NodePort External IPs Load Balancer  Kubernetes 服务是一个抽象，它定义了如何访问一组 Pod 的策略和方法。可以通过 Service 访问基于标签选择器的 Pod 集合。\nCluster IP Cluster IP 是创建 Kubernetes 服务时的默认方法。该服务被分配了一个内部 IP，其他组件可以使用它来访问 Pod。\nService 能够通过单个 IP 地址在多个 Pod 之间进行负载平衡。\n通过 kubectl apply -f clusterip.yaml 命令部署服务。\n在 cat clusterip.yaml 可以查看相关定义。\nclusterip.yaml\napiVersion:v1kind:Servicemetadata:name:webapp1-clusterip-svclabels:app:webapp1-clusteripspec:ports:- port:80selector:app:webapp1-clusterip---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:webapp1-clusterip-deploymentspec:replicas:2template:metadata:labels:app:webapp1-clusteripspec:containers:- name:webapp1-clusterip-podimage:katacoda/docker-http-server:latestports:- containerPort:80---controlplane $ kubectl apply -f clusterip.yaml service/webapp1-clusterip-svc created deployment.extensions/webapp1-clusterip-deployment created 该文件定义了部署具有两个副本的 Web 应用程序，用来展示负载平衡和服务。 可以通过 kubectl get pods 命令查看 Pod 状态。\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE webapp1-clusterip-deployment-669c7c65c4-r6k2l 1/1 Running 0 15s webapp1-clusterip-deployment-669c7c65c4-xdsd4 1/1 Running 0 15s 同时也部署了一个 Service，可以通过 kubectl get svc 命令查看。\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 8m12s webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 39s 可以通过 kubectl describe svc/webapp1-clusterip-svc 命令查看有关服务配置和活动端点 (Pod) 的更多详细信息\ncontrolplane $ kubectl describe svc/webapp1-clusterip-svc Name: webapp1-clusterip-svc Namespace: default Labels: app=webapp1-clusterip Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-clusterip\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-clusterip-svc\u0026#34;,\u0026#34;name... Selector: app=webapp1-clusterip Type: ClusterIP IP: 10.108.32.169 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP Endpoints: 10.32.0.5:80,10.32.0.6:80 Session Affinity: None Events: \u0026lt;none\u0026gt; 部署完成后，可以通过分配的 Cluster IP 访问该服务。\ncontrolplane $ export CLUSTER_IP=$(kubectl get services/webapp1-clusterip-svc -o go-template=\u0026#39;{{(index .spec.clusterIP)}}\u0026#39;) controlplane $ echo CLUSTER_IP=$CLUSTER_IP CLUSTER_IP=10.108.32.169 controlplane $ curl $CLUSTER_IP:80 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-deployment-669c7c65c4-r6k2l\u0026lt;/h1\u0026gt; 将通过多个请求来展示基于公共标签选择器的跨多个* Pod* 的负载均衡器。\ncontrolplane $ curl $CLUSTER_IP:80 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-deployment-669c7c65c4-xdsd4\u0026lt;/h1\u0026gt; controlplane $ curl $CLUSTER_IP:80 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-deployment-669c7c65c4-r6k2l\u0026lt;/h1\u0026gt; Target Port TargetPort 允许我们将服务可用的端口与应用程序正在侦听的端口分开。 TargetPort 是应用配置为侦听的端口。端口是从外部访问应用的方式。\n与之前类似，Service 和额外的 Pod 是通过 kubectl apply -f clusterip-target.yaml 命令来部署的。\n**clusterip-target.yaml **\napiVersion:v1kind:Servicemetadata:name:webapp1-clusterip-targetport-svclabels:app:webapp1-clusterip-targetportspec:ports:- port:8080targetPort:80selector:app:webapp1-clusterip-targetport---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:webapp1-clusterip-targetport-deploymentspec:replicas:2template:metadata:labels:app:webapp1-clusterip-targetportspec:containers:- name:webapp1-clusterip-targetport-podimage:katacoda/docker-http-server:latestports:- containerPort:80---以下命令将创建服务。\ncontrolplane $ kubectl apply -f clusterip-target.yaml service/webapp1-clusterip-targetport-svc created deployment.extensions/webapp1-clusterip-targetport-deployment created controlplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 13m webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 5m29s webapp1-clusterip-targetport-svc ClusterIP 10.102.59.206 \u0026lt;none\u0026gt; 8080/TCP 40s controlplane $ kubectl describe svc/webapp1-clusterip-targetport-svc Name: webapp1-clusterip-targetport-svc Namespace: default Labels: app=webapp1-clusterip-targetport Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-clusterip-targetport\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-clusterip... Selector: app=webapp1-clusterip-targetport Type: ClusterIP IP: 10.102.59.206 Port: \u0026lt;unset\u0026gt; 8080/TCP TargetPort: 80/TCP Endpoints: 10.32.0.7:80,10.32.0.8:80 Session Affinity: None Events: \u0026lt;none\u0026gt; Service 和 Pod 部署完成后，可以像以前一样通过 Cluster IP 访问，但这次是在定义的端口 8080 上。\ncontrolplane $ export CLUSTER_IP=$(kubectl get services/webapp1-clusterip-targetport-svc -o go-template=\u0026#39;{{(index .spec.clusterIP)}}\u0026#39;) controlplane $ echo CLUSTER_IP=$CLUSTER_IP CLUSTER_IP=10.102.59.206 controlplane $ curl $CLUSTER_IP:8080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-targetport-deployment-5599945ff4-96fqh\u0026lt;/h1\u0026gt; controlplane $ curl $CLUSTER_IP:8080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-clusterip-targetport-deployment-5599945ff4-zwr8f\u0026lt;/h1\u0026gt; 应用程序本身仍然配置为侦听 80 端口。Kubernetes 服务管理着两者之间的转换。\nNodePort 虽然 TargetPort 和 ClusterIP 使其可用于集群内部，但 NodePort 通过定义的静态端口在每个节点的 IP 上公开服务。无论访问集群内的哪个节点，都可以通过定义的端口号直接访问该服务。\ncontrolplane $ kubectl apply -f nodeport.yaml service/webapp1-nodeport-svc created deployment.extensions/webapp1-nodeport-deployment created 查看服务定义时，注意定义的附加类型和 NodePort 属性。\nnodeport.yaml\napiVersion:v1kind:Servicemetadata:name:webapp1-nodeport-svclabels:app:webapp1-nodeportspec:type:NodePortports:- port:80nodePort:30080selector:app:webapp1-nodeport---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:webapp1-nodeport-deploymentspec:replicas:2template:metadata:labels:app:webapp1-nodeportspec:containers:- name:webapp1-nodeport-podimage:katacoda/docker-http-server:latestports:- containerPort:80---controlplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 22m webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 14m webapp1-clusterip-targetport-svc ClusterIP 10.102.59.206 \u0026lt;none\u0026gt; 8080/TCP 9m44s webapp1-nodeport-svc NodePort 10.102.135.99 \u0026lt;none\u0026gt; 80:30080/TCP 5m52s controlplane $ kubectl describe svc/webapp1-nodeport-svc Name: webapp1-nodeport-svc Namespace: default Labels: app=webapp1-nodeport Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-nodeport\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-nodeport-svc\u0026#34;,\u0026#34;namesp... Selector: app=webapp1-nodeport Type: NodePort IP: 10.102.135.99 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 30080/TCP Endpoints: 10.32.0.10:80,10.32.0.9:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; 可以通过 NodePort 定义的 Node 的 IP 地址访问到服务。\ncontrolplane $ curl 172.17.0.31:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-nodeport-deployment-677bd89b96-j256b\u0026lt;/h1\u0026gt; controlplane $ curl 172.17.0.31:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-nodeport-deployment-677bd89b96-88qj5\u0026lt;/h1\u0026gt; External IP 另一种方法是通过外部 IP 地址让服务在集群外可用。\n将定义中的 externalIPs 更新为当前集群的 IP 地址\ncontrolplane $ sed -i \u0026#39;s/HOSTIP/172.17.0.31/g\u0026#39; externalip.yaml externalip.yaml\napiVersion: v1 kind: Service metadata: name: webapp1-externalip-svc labels: app: webapp1-externalip spec: ports: - port: 80 externalIPs: - 172.17.0.30 selector: app: webapp1-externalip --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: webapp1-externalip-deployment spec: replicas: 2 template: metadata: labels: app: webapp1-externalip spec: containers: - name: webapp1-externalip-pod image: katacoda/docker-http-server:latest ports: - containerPort: 80 --- controlplane $ cat externalip.yaml apiVersion: v1 kind: Service metadata: name: webapp1-externalip-svc labels: app: webapp1-externalip spec: ports: - port: 80 externalIPs: - 172.17.0.30 selector: app: webapp1-externalip --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: webapp1-externalip-deployment spec: replicas: 2 template: metadata: labels: app: webapp1-externalip spec: containers: - name: webapp1-externalip-pod image: katacoda/docker-http-server:latest ports: - containerPort: 80 --- controlplane $ kubectl apply -f externalip.yaml service/webapp1-externalip-svc created deployment.extensions/webapp1-externalip-deployment created controlplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 28m webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 20m webapp1-clusterip-targetport-svc ClusterIP 10.102.59.206 \u0026lt;none\u0026gt; 8080/TCP 15m webapp1-externalip-svc ClusterIP 10.109.28.108 172.17.0.31 80/TCP 8s webapp1-nodeport-svc NodePort 10.102.135.99 \u0026lt;none\u0026gt; 80:30080/TCP 12m controlplane $ kubectl describe svc/webapp1-externalip-svc Name: webapp1-externalip-svc Namespace: default Labels: app=webapp1-externalip Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-externalip\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-externalip-svc\u0026#34;,\u0026#34;na... Selector: app=webapp1-externalip Type: ClusterIP IP: 10.109.28.108 External IPs: 172.17.0.31 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP Endpoints: 10.32.0.11:80,10.32.0.12:80 Session Affinity: None Events: \u0026lt;none\u0026gt; 该服务现在绑定到主节点的 IP 地址和 80端口 。\ncontrolplane $ curl 172.17.0.31 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-externalip-deployment-6446b488f8-dxdb5\u0026lt;/h1\u0026gt; controlplane $ curl 172.17.0.31 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-externalip-deployment-6446b488f8-2zw2p\u0026lt;/h1\u0026gt; Load Balancer 在例如 EC2 或 Azure 的云服务中运行时，可以通过云服务商配置和分配发布的公网 IP 地址。这将通过负载均衡器（例如 ELB）发出，允许将额外的公网 IP 地址分配给 Kubernetes 集群，而无需直接与云提供商交互。\n由于 Katacoda 不是云服务商，因此仍然可以为 LoadBalancer 类型的服务动态分配 IP 地址。这是通过使用 kubectl apply -f cloudprovider.yaml 部署 Cloud Provider 来完成的。并不是必需在云服务商提供的服务中运行的。\ncloudprovider.yaml\napiVersion:extensions/v1beta1kind:DaemonSetmetadata:name:kube-keepalived-vipnamespace:kube-systemspec:template:metadata:labels:name:kube-keepalived-vipspec:hostNetwork:truecontainers:- image:gcr.io/google_containers/kube-keepalived-vip:0.9name:kube-keepalived-vipimagePullPolicy:AlwayssecurityContext:privileged:truevolumeMounts:- mountPath:/lib/modulesname:modulesreadOnly:true- mountPath:/devname:dev# use downward APIenv:- name:POD_NAMEvalueFrom:fieldRef:fieldPath:metadata.name- name:POD_NAMESPACEvalueFrom:fieldRef:fieldPath:metadata.namespace# to use unicastargs:- --services-configmap=kube-system/vip-configmap# unicast uses the ip of the nodes instead of multicast# this is useful if running in cloud providers (like AWS)#- --use-unicast=truevolumes:- name:moduleshostPath:path:/lib/modules- name:devhostPath:path:/devnodeSelector:# type: worker # adjust this to match your worker nodes---## We also create an empty ConfigMap to hold our configapiVersion:v1kind:ConfigMapmetadata:name:vip-configmapnamespace:kube-systemdata:---apiVersion:apps/v1beta1kind:Deploymentmetadata:labels:app:keepalived-cloud-providername:keepalived-cloud-providernamespace:kube-systemspec:replicas:1revisionHistoryLimit:2selector:matchLabels:app:keepalived-cloud-providerstrategy:type:RollingUpdatetemplate:metadata:annotations:scheduler.alpha.kubernetes.io/critical-pod:\u0026#34;\u0026#34;scheduler.alpha.kubernetes.io/tolerations:\u0026#39;[{\u0026#34;key\u0026#34;:\u0026#34;CriticalAddonsOnly\u0026#34;, \u0026#34;operator\u0026#34;:\u0026#34;Exists\u0026#34;}]\u0026#39;labels:app:keepalived-cloud-providerspec:containers:- name:keepalived-cloud-providerimage:quay.io/munnerz/keepalived-cloud-provider:0.0.1imagePullPolicy:IfNotPresentenv:- name:KEEPALIVED_NAMESPACEvalue:kube-system- name:KEEPALIVED_CONFIG_MAPvalue:vip-configmap- name:KEEPALIVED_SERVICE_CIDRvalue:10.10.0.0/26# pick a CIDR that is explicitly reserved for keepalivedvolumeMounts:- name:certsmountPath:/etc/ssl/certsresources:requests:cpu:200mlivenessProbe:httpGet:path:/healthzport:10252host:127.0.0.1initialDelaySeconds:15timeoutSeconds:15failureThreshold:8volumes:- name:certshostPath:path:/etc/ssl/certs---controlplane $ kubectl apply -f cloudprovider.yaml daemonset.extensions/kube-keepalived-vip configured configmap/vip-configmap configured deployment.apps/keepalived-cloud-provider created 当服务请求负载均衡时，负载均衡的提供商将从配置中定义的 10.10.0.0/26 范围中分配一个 IP。\ncontrolplane $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-fb8b8dccf-6rvjq 1/1 Running 0 39m coredns-fb8b8dccf-p5lxm 1/1 Running 0 39m etcd-controlplane 1/1 Running 0 38m katacoda-cloud-provider-66d7758d5d-bn748 1/1 Running 0 39m keepalived-cloud-provider-78fc4468b-bk8wn 1/1 Running 0 6m16s kube-apiserver-controlplane 1/1 Running 0 39m kube-controller-manager-controlplane 1/1 Running 2 39m kube-keepalived-vip-42ql5 1/1 Running 0 38m kube-proxy-d5q4m 1/1 Running 0 39m kube-scheduler-controlplane 1/1 Running 3 39m weave-net-9rc2g 2/2 Running 1 39m 该服务是通过负载均衡配置的，如 cat loadbalancer.yaml 中所定义所示。\nloadbalancer.yaml\napiVersion:v1kind:Servicemetadata:name:webapp1-loadbalancer-svclabels:app:webapp1-loadbalancerspec:type:LoadBalancerports:- port:80selector:app:webapp1-loadbalancer---apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:webapp1-loadbalancer-deploymentspec:replicas:2template:metadata:labels:app:webapp1-loadbalancerspec:containers:- name:webapp1-loadbalancer-podimage:katacoda/docker-http-server:latestports:- containerPort:80controlplane $ kubectl apply -f loadbalancer.yaml service/webapp1-loadbalancer-svc created deployment.extensions/webapp1-loadbalancer-deployment created 在定义 IP 地址时，服务将显示 Pending。分配后，它将出现在服务列表中。\ncontrolplane $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 38m webapp1-clusterip-svc ClusterIP 10.108.32.169 \u0026lt;none\u0026gt; 80/TCP 31m webapp1-clusterip-targetport-svc ClusterIP 10.102.59.206 \u0026lt;none\u0026gt; 8080/TCP 26m webapp1-externalip-svc ClusterIP 10.109.28.108 172.17.0.31 80/TCP 10m webapp1-loadbalancer-svc LoadBalancer 10.105.60.108 172.17.0.31 80:30626/TCP 7s webapp1-nodeport-svc NodePort 10.102.135.99 \u0026lt;none\u0026gt; 80:30080/TCP 22m controlplane $ kubectl describe svc/webapp1-loadbalancer-svc Name: webapp1-loadbalancer-svc Namespace: default Labels: app=webapp1-loadbalancer Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;webapp1-loadbalancer\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;webapp1-loadbalancer-svc\u0026#34;... Selector: app=webapp1-loadbalancer Type: LoadBalancer IP: 10.105.60.108 LoadBalancer Ingress: 172.17.0.31 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 30626/TCP Endpoints: 10.32.0.14:80,10.32.0.15:80 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CreatingLoadBalancer 95s service-controller Creating load balancer Normal CreatedLoadBalancer 95s service-controller Created load balancer 现在可以通过分配的 IP 地址访问该服务，在本例中的范围是 10.10.0.0/26 。\ncontrolplane $ export LoadBalancerIP=$(kubectl get services/webapp1-loadbalancer-svc -o go-template=\u0026#39;{{(index .status.loadBalancer.ingress 0).ip}}\u0026#39;) controlplane $ echo LoadBalancerIP=$LoadBalancerIP LoadBalancerIP=172.17.0.31 controlplane $ curl $LoadBalancerIP \u0026lt;h1\u0026gt;This request was processed by host: webapp1-externalip-deployment-6446b488f8-2zw2p\u0026lt;/h1\u0026gt; controlplane $ curl $LoadBalancerIP \u0026lt;h1\u0026gt;This request was processed by host: webapp1-externalip-deployment-6446b488f8-dxdb5\u0026lt;/h1\u0026gt; ","date":"2021-07-20T13:36:00+08:00","image":"https://www.catfish.top/p/k8s-basic-6/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-6/","title":"Kubernetes初探（六）"},{"content":" Katacoda在线课：Deploy Guestbook Web App Example\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 本场景说明了如何使用 Kubernetes 和 Docker 启动简单的多层 Web 应用。留言簿示例应用程序通过调用* JavaScript API* 将访客的笔记存储在 *Redis* 中。 *Redis* 包含一个 master（用于存储）和一组 *slave* 复制集。\n核心概念 在此场景中将涵盖以下核心概念。这些是理解 Kubernetes 的基础。\n Pods Replication Controllers Services NodePorts  启动 Kubernetes 首先，我们需要一个正在运行的 Kubernetes 集群。详细信息在 Launch Kubernetes cluster\n任务 使用初始化程序脚本启动单节点集群。初始化脚本将启动 API、Master、Proxy 和 DNS Discovery。 Web App 使用 DNS Discovery 来查找 Redis slave 来存储数据。\nlaunch.sh 健康检查 使用 kubectl cluster-info 和 kubectl get nodes命令来检查部署的集群的节点健康信息。\ncontrolplane $ kubectl cluster-info Kubernetes master is running at https://172.17.0.81:6443 KubeDNS is running at https://172.17.0.81:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. controlplane $ kubectl get nodes NAME STATUS ROLES AGE VERSION controlplane Ready master 2m15s v1.14.0 node01 Ready \u0026lt;none\u0026gt; 114s v1.14.0 如果节点返回 NotReady，则它仍在等待。请等待几秒钟后再重新启动。\nRedis Master - Replication Controller 启动应用的第一步是启动Redis Master。 Kubernetes 服务部署至少有两个部分，分别为 Replication Controller 和 Service。\nReplication Controller 定义了运行的实例数量、要使用的 Docker 镜像以及服务标识名称。其他选项可用于配置和发现。使用上面的编辑器查看 YAML 定义。\n如果 Redis 出现故障，Replication Controller 将在活动节点上重新启动它。\n创建 Replication Controller redis-master-controller.yaml\napiVersion:v1kind:ReplicationControllermetadata:name:redis-masterlabels:name:redis-masterspec:replicas:1selector:name:redis-mastertemplate:metadata:labels:name:redis-masterspec:containers:- name:masterimage:redis:3.0.7-alpineports:- containerPort:6379在本示例中，YAML 使用官方 redis 设置端口 6379 运行了一个名为 redis-master 的 redis 服务器。\nkubectl create 命令采用 YAML 定义并指示 master 启动控制器。\ncontrolplane $ kubectl create -f redis-master-controller.yaml replicationcontroller/redis-master created 查看运行的组件 上面的命令创建了一个 Replication Controller 。\ncontrolplane $ kubectl get rc NAME DESIRED CURRENT READY AGE redis-master 1 1 1 2m9s 所有容器都被描述为 Pod。 Pod 是构成特定应用程序（例如 Redis）的容器集合。可以使用 kubectl 查看 Pod 信息。\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE redis-master-qkfxw 1/1 Running 0 2m54s Redis Master - Service 第二步是 Service。 Kubernetes Service 是一种命名负载均衡器，它将流量代理到一个或多个容器。即使容器位于不同的节点上，代理也能工作。\n服务代理在集群内通信，很少将端口暴露给外部接口。\n当启动服务时，似乎无法使用 curl 或 netcat 进行连接，除非=将其作为 Kubernetes 的一部分启动。推荐的方法是使用 LoadBalancer 服务来处理外部通信。\nredis-master-service.yaml\napiVersion:v1kind:Servicemetadata:name:redis-masterlabels:name:redis-masterspec:ports:# the port that this service should serve on- port:6379targetPort:6379selector:name:redis-master创建 Service YAML 定义了 Service 的名称redis-master，以及应该被代理的端口。\ncontrolplane $ kubectl create -f redis-master-service.yaml service/redis-master created 查看 Service 列表与详情 bash controlplane $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 10m redis-master ClusterIP 10.100.28.180 6379/TCP 43s\ncontrolplane $ kubectl describe services redis-master Name: redis-master Namespace: default Labels: name=redis-master Annotations: Selector: name=redis-master Type: ClusterIP IP: 10.100.28.180 Port: 6379/TCP TargetPort: 6379/TCP Endpoints: 10.32.0.193:6379 Session Affinity: None Events:  ## Redis Slave Pods 在本示例中，我们将运行 *Redis Slaves*，它会从 *master* 复制数据。有关 *Redis* 复制的更多详细信息，请访问 [http://redis.io/topics/replication](http://redis.io/topics/replication) 如前所述，*Controller* 定义了 *Service* 的运行方式。在这个例子中，我们需要确定 *Service* 是如何发现其他 *Pod*。 *YAML* 将 *GET_HOSTS_FROM* 属性表示为 *DNS*。您可以将其更改为在 *YAML* 中使用环境变量，但这会引入创建顺序依赖关系，因为需要运行服务才能定义环境变量。 **redis-slave-controller.yaml** ```yaml apiVersion: v1 kind: ReplicationController metadata: name: redis-slave labels: name: redis-slave spec: replicas: 2 selector: name: redis-slave template: metadata: labels: name: redis-slave spec: containers: - name: worker image: gcr.io/google_samples/gb-redisslave:v1 env: - name: GET_HOSTS_FROM value: dns # If your cluster config does not include a dns service, then to # instead access an environment variable to find the master # service's host, comment out the 'value: dns' line above, and # uncomment the line below. # value: env ports: - containerPort: 6379 启动 Redis Slave Controller 在这种情况下，我们将使用镜像 kubernetes/redis-slave:v2 启动 Pod 的两个实例。它将通过 DNS 链接到 redis-master。\ncontrolplane $ kubectl create -f redis-slave-controller.yaml replicationcontroller/redis-slave created 列出 Replication Controller NAME DESIRED CURRENT READY AGE redis-master 1 1 1 11m redis-slave 2 2 2 26s Redis Slave Service 和以前一样，我们需要启动一个与 redis-slave 通信的Service来让Slave能够接收传入的请求。\n因为我们有两个复制的 Pod，该服务还将在两个节点之间提供负载平衡。\nredis-slave-service.yaml\napiVersion:v1kind:Servicemetadata:name:redis-slavelabels:name:redis-slavespec:ports:# the port that this service should serve on- port:6379selector:name:redis-slaveStart Redis Slave Service controlplane $ kubectl create -f redis-slave-service.yaml service/redis-slave created controlplane $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2m20s redis-master ClusterIP 10.100.144.249 \u0026lt;none\u0026gt; 6379/TCP 46s redis-slave ClusterIP 10.105.27.32 \u0026lt;none\u0026gt; 6379/TCP 16s 前端 Replicated Pods 启动数据服务后，我们现在可以部署 Web 应用。部署 Web 应用的模式与我们之前部署的 Pod 相同。\n启动前端 YAML 定义了一个名为 frontend 的服务，该服务使用 gcr.io/google_samples/gb-frontend:v3 的镜像。Replication Controller 将确保三个 Pod 始终存在。\nfrontend-controller.yaml\napiVersion:v1kind:ReplicationControllermetadata:name:frontendlabels:name:frontendspec:replicas:3selector:name:frontendtemplate:metadata:labels:name:frontendspec:containers:- name:php-redisimage:gcr.io/google_samples/gb-frontend:v3env:- name:GET_HOSTS_FROMvalue:dns# If your cluster config does not include a dns service, then to# instead access environment variables to find service host# info, comment out the \u0026#39;value: dns\u0026#39; line above, and uncomment the# line below.# value: envports:- containerPort:80controlplane $ kubectl create -f frontend-controller.yaml replicationcontroller/frontend created 列出 Controllers 和 Pods controlplane $ kubectl get rc NAME DESIRED CURRENT READY AGE frontend 3 3 3 34s redis-master 1 1 1 8m20s redis-slave 2 2 2 8m6s controlplane $ kubectl get pods NAME READY STATUS RESTARTS AGE frontend-88lvf 1/1 Running 0 42s frontend-phhhv 1/1 Running 0 42s frontend-rq8tt 1/1 Running 0 42s redis-master-7h9t2 1/1 Running 0 8m28s redis-slave-gnhfh 1/1 Running 0 8m14s redis-slave-mtn2r 1/1 Running 0 8m14s PHP 代码 PHP 代码使用 HTTP 和 JSON 与 Redis 通信。当设置一个值时，请求转到 redis-master，而读取的数据来自 redis-slave 节点。\n用户手册前端 Service 为了能够访问前端，我们需要启动一个 Service 来配置代理。\n启动代理 YAML 将服务定义为 NodePort 。 NodePort 允许您设置在整个集群中共享的特点端口，这就像 Docker 中的 -p 80:80。\n在这种情况下，我们定义我们的 Web 应用在端口 80 上运行，在 30080 上开放服务。\nfrontend-service.yaml\napiVersion:v1kind:Servicemetadata:name:frontendlabels:name:frontendspec:# if your cluster supports it, uncomment the following to automatically create# an external load-balanced IP for the frontend service.# type: LoadBalancertype:NodePortports:# the port that this service should serve on- port:80nodePort:30080selector:name:frontendcontrolplane $ kubectl create -f frontend-service.yaml service/frontend created controlplane $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend NodePort 10.98.211.171 \u0026lt;none\u0026gt; 80:30080/TCP 3s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 13m redis-master ClusterIP 10.100.144.249 \u0026lt;none\u0026gt; 6379/TCP 11m redis-slave ClusterIP 10.105.27.32 \u0026lt;none\u0026gt; 6379/TCP 11m 我们将在未来的场景中讨论 NodePort 。\n访问用户手册前端 定义了所有 Controller 和 Service 后，Kubernetes 将开始将它们作为 Pod 启动。根据实际发生的情况，Pod 可以具有不同的状态。例如，如果 Docker 映像仍在下载无法启动，则 Pod 将处于 pending 状态。准备就绪后，状态将会变更为 running。\n查看 Pods 状态 可以使用下列命令查看所有 Pod 的状态。\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE frontend-88lvf 1/1 Running 0 9m3s frontend-phhhv 1/1 Running 0 9m3s frontend-rq8tt 1/1 Running 0 9m3s redis-master-7h9t2 1/1 Running 0 16m redis-slave-gnhfh 1/1 Running 0 16m redis-slave-mtn2r 1/1 Running 0 16m 查找 NodePort 如果您没有分配 众所周知（well-known） 的 NodePort，那么 Kubernetes 将随机分配一个可用端口。您可以使用 kubectl 找到分配的 NodePort。\ncontrolplane $ kubectl describe service frontend | grep NodePort Type: NodePort NodePort: \u0026lt;unset\u0026gt; 30080/TCP 查看 UI 一旦 Pod 处于 running 状态，您将能够通过端口 30080 查看 UI。使用 URL 查看页面 https://2886795308-30080-kitek05.environments.katacoda.com\n在背后（Under the covers） PHP 服务通过 DNS 发现 Redis 实例。您现在已经在 Kubernetes 上部署了一个有效的多层应用程序。\n","date":"2021-07-20T13:18:00+08:00","image":"https://www.catfish.top/p/k8s-basic-5/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-5/","title":"Kubernetes初探（五）"},{"content":" Katacoda在线课：Deploy Containers Using YAML\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，您将学习如何使用 Kubectl 创建和启动 Deployment、Replication Controller，并通过编写 yaml 定义使用服务开暴露它们。\nYAML 定义了计划部署的 Kubernetes 对象。可以以更新对象并将其重新部署到集群的方式来更改配置。\n创建 Deployment Deployment 对象是最常见的 Kubernetes 对象之一。Deployment 对象定义了所需的容器规范，以及 Kubernetes 其他部分用来发现和连接到应用程序的名称和标签。\n任务 将以下定义复制到编辑器的YAML文件中。该 YAML 定义了如何使用在端口 80 上运行的应用，该应用使用 Docker 映像 katacoda/docker-http-server ，启动名为 webapp1 。\ndeployment.yaml\napiVersion:apps/v1kind:Deploymentmetadata:name:webapp1spec:replicas:1selector:matchLabels:app:webapp1template:metadata:labels:app:webapp1spec:containers:- name:webapp1image:katacoda/docker-http-server:latestports:- containerPort:80使用 kubectl create -f deployment.yaml 命令向集群部署。\n$ kubectl create -f deployment.yaml deployment.apps/webapp1 created 由于它是一个 Deployment 对象，因此可以通过 kubectl get deployment 获取所有已部署的 Deployment 对象的列表。\n$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE webapp1 1/1 1 1 10s 可以使用 kubectl describe deployment webapp1 输出单个部署的详细信息。\n$ kubectl describe deployment webapp1 Name: webapp1 Namespace: default CreationTimestamp: Wed, 21 Jul 2021 09:31:47 +0000 Labels: \u0026lt;none\u0026gt; Annotations: deployment.kubernetes.io/revision: 1 Selector: app=webapp1 Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=webapp1 Containers: webapp1: Image: katacoda/docker-http-server:latest Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: webapp1-6b54fb89d9 (1/1 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 49s deployment-controller Scaled up replica set webapp1-6b54fb89d9 to 1 创建Service Kubernetes 具有强大的网络功能，可以控制应用程序的通信方式。这些网络配置也可以通过 YAML 定义。\n任务 将 Service 定义复制到编辑器。该 Service 选择带有标签 webapp1 的所有应用程序。当部署多个副本或实例时，它们将根据此公共标签自动进行负载平衡。该 Service 通过 NodePort 的网络连接方式部署。\nservice.yaml\napiVersion:v1kind:Servicemetadata:name:webapp1-svclabels:app:webapp1spec:type:NodePortports:- port:80nodePort:30080selector:app:webapp1所有 Kubernetes 对象都一致使用 kubectl 完成部署。\n使用 kubectl create -f service.yaml 命令部署 Service\n$ kubectl create -f service.yaml service/webapp1-svc created 和以往一样，使用 kubectl get svc 可以查看所有已部署的 Service 对象的信息。通过 kubectl describe svc webapp1-svc 命令可以查看该 Service 对象更多的配置信息。\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 6m38s webapp1-svc NodePort 10.111.56.111 \u0026lt;none\u0026gt; 80:30080/TCP 20s $ kubectl describe svc webapp1-svc Name: webapp1-svc Namespace: default Labels: app=webapp1 Annotations: \u0026lt;none\u0026gt; Selector: app=webapp1 Type: NodePort IP: 10.111.56.111 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 30080/TCP Endpoints: 172.18.0.4:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-5rzpv\u0026lt;/h1\u0026gt; 缩放 Deployment 由于 Deployment 需要不同的配置，因此可以更改 YAML 的详细信息。这遵循基础设施即代码的思维方式。清单应在源代码控制下，并且保证生产环境的配置和源代码控制中的配置一致。\n任务 更新 deployment.yaml 文件以增加运行的实例数。例如，该文件应如下所示：\nreplicas:1# --\u0026gt; replicas:4使用 kubectl apply 提交对现有定义的更新。要扩展副本数量，请使用 kubectl apply -f deployment.yaml 部署更新的 YAML 文件\n$ kubectl apply -f deployment.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply deployment.apps/webapp1 configured 集群的状态将立即更新，可通过 kubectl get deployment 命令 查看\n$ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE webapp1 4/4 4 4 15m 根据定义将有额外的 Pods 被调度到集群中， 使用 kubectl get pods 查看 Pod 信息\n$ kubectl get pods NAME READY STATUS RESTARTS AGE webapp1-6b54fb89d9-5rzpv 1/1 Running 0 16m webapp1-6b54fb89d9-ktvz4 1/1 Running 0 2m15s webapp1-6b54fb89d9-kzdjw 1/1 Running 0 2m15s webapp1-6b54fb89d9-x8vtv 1/1 Running 0 2m15s 由于所有 Pod 具有相同的标签选择器，因此将由已部署的 Service NodePort 为这些 Pod 提供负载平衡能力。\n向目标端口发出多个请求，可以通过返回信息发现，请求是由多个容器进行处理的。\n$ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-5rzpv\u0026lt;/h1\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-kzdjw\u0026lt;/h1\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-kzdjw\u0026lt;/h1\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-x8vtv\u0026lt;/h1\u0026gt; $ curl host01:30080 \u0026lt;h1\u0026gt;This request was processed by host: webapp1-6b54fb89d9-ktvz4\u0026lt;/h1\u0026gt; 其他 Kubernetes 网络细节和对象定义将在未来的其他场景中介绍。\n","date":"2021-07-20T11:39:00+08:00","image":"https://www.catfish.top/p/k8s-basic-4/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-4/","title":"Kubernetes初探（四）"},{"content":" Katacoda在线课：Deploy Containers Using Kubectl\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，将学习如何使用 Kubectl 创建和启动 Deployment 和 Replication Controllers 并通过 Services 对外暴露接口。本场景中无需编写 yaml 定义，便可以快速将容器启动到集群上。\n启动集群 首先，我们需要启动一个 Kubernetes 集群。\n执行以下命令启动集群并下载 Kubectl CLI。\n$ minikube start --wait=false * minikube v1.8.1 on Ubuntu 18.04 * Using the none driver based on user configuration * Running on localhost (CPUs=2, Memory=2460MB, Disk=145651MB) ... * OS release is Ubuntu 18.04.4 LTS * Preparing Kubernetes v1.17.3 on Docker 19.03.6 ... - kubelet.resolv-conf=/run/systemd/resolve/resolv.conf * Launching Kubernetes ... * Enabling addons: default-storageclass, storage-provisioner * Configuring local host environment ... * Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; 通过 kubectl get nodes 命令来检查节点是否准备就绪。\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 19s v1.17.3 Kubectl Run Run 命令根据指定的参数（例如映像或副本）创建部署。该部署发布给 Kubernetes 主节点，启动所需 Pod 和容器的 。 Kubectl run 类似于 docker run，但 Kubectl run 是在集群中运行的。\n命令的格式为 kubectl run \u0026lt;name of deployment\u0026gt; \u0026lt;properties\u0026gt;\n任务 以下命令将启动一个名为 http 的部署，它将启动一个基于 Docker 镜像 katacoda/docker-http-server:latest 的容器。\n$ kubectl run http --image=katacoda/docker-http-server:latest --replicas=1 kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. deployment.apps/http created 使用 kubectl 查看各部署的状态。\n$ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE http 1/1 1 1 2m7s 可以通过 describe 查看 Kubernetes 创建了哪些内容。\n$ kubectl describe deployment http Name: http Namespace: default CreationTimestamp: Wed, 21 Jul 2021 08:04:55 +0000 Labels: run=http Annotations: deployment.kubernetes.io/revision: 1 Selector: run=http Replicas: 1 desired | 1 updated | 1 total | 1 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: run=http Containers: http: Image: katacoda/docker-http-server:latest Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: http-774bb756bb (1/1 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 4m16s deployment-controller Scaled up replica set http-774bb756bb to 1 描述中包含有多少副本可用、指定的标签以及与部署相关的事件等信息。这些事件将突出显示可能发生的任何问题和错误。\n在下一步中，我们将开放正在运行的服务。\nKubectl Expose 创建部署后，我们可以使用 kubectl 创建一个服务，该服务在特定端口上开放 Pod。\n通过 kubectl expose 开放新部署的 http 部署。该命令允许定义服务的不同参数以及如何开放该部署。\n任务 使用以下命令将主机上的容器端口 80 公开 8000 绑定到主机的 external-ip。\n$ kubectl expose deployment http --external-ip=\u0026#34;172.17.0.44\u0026#34; --port=8000 --target-port=80 service/http exposed 接下来就能 ping 主机查看 HTTP 服务返回的结果。\n$ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-m6tjp\u0026lt;/h1\u0026gt; Kubectl Run and Expose 可以单独使用命令 kubectl run 来创建部署并将其开放。\n任务 使用命令创建在端口 8001 上开放的第二个 http 服务。\n$ kubectl run httpexposed --image=katacoda/docker-http-server:latest --replicas=1 --port=80 --hostport=8001 kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. deployment.apps/httpexposed created 接下来可以使用 curl http://172.17.0.41:8001 来访问该服务。\n$ curl http://172.17.0.44:8001 \u0026lt;h1\u0026gt;This request was processed by host: httpexposed-68cb8c8d4-r7qtt\u0026lt;/h1\u0026gt; 在服务内，通过 Docker 端口映射开放了 Pod。因此，您将看不到使用 kubectl get svc 列出的服务\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE http ClusterIP 10.97.103.237 172.17.0.44 8000/TCP 4m47s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 44m 可以通过 docker ps | grep httpexposed 命令查看详细信息。\n$ docker ps | grep httpexposed 1690c6682f93 katacoda/docker-http-server \u0026#34;/app\u0026#34; 2 minutes ago Up 2 minutes k8s_httpexposed_httpexposed-68cb8c8d4-r7qtt_default_6fb23c14-277e-4298-b325-c52ef4eb09a9_0 d9ae0076a0d4 k8s.gcr.io/pause:3.1 \u0026#34;/pause\u0026#34; 2 minutes ago Up 2 minutes 0.0.0.0:8001-\u0026gt;80/tcp k8s_POD_httpexposed-68cb8c8d4-r7qtt_default_6fb23c14-277e-4298-b325-c52ef4eb09a9_0 暂停容器 运行上面的命令，你会注意到端口暴露在 Pod 上，而不是 http 容器本身。 Pause 容器负责为 Pod 定义网络。 Pod 中的其他容器共享相同的网络命名空间。允许多个容器通过同一网络接口进行通信，提高了网络性能。\n容器扩展 随着我们的部署运行，我们现在可以使用 kubectl 来扩展副本的数量。\n扩展部署将请求 Kubernetes 启动额外的 Pod。然后，这些 Pod 将使用开放服务自动进行负载平衡。\n任务 kubectl scale 命令能够为特定 Deployment 或 Replication Controller 调整运行的 Pod 数量。\n$ kubectl scale --replicas=3 deployment http deployment.apps/http scaled 列出所有 pod，能够看到三个正在运行的 http 部署。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE http-774bb756bb-m6tjp 1/1 Running 0 20m http-774bb756bb-q29v8 1/1 Running 0 45s http-774bb756bb-x2xxv 1/1 Running 0 45s httpexposed-68cb8c8d4-r7qtt 1/1 Running 0 9m3s 一旦每个 Pod 启动，就会被添加到负载均衡器服务中。通过 describe，可以查看包含的端点和与之关联的 Pod。\n$ kubectl describe svc http Name: http Namespace: default Labels: run=http Annotations: \u0026lt;none\u0026gt; Selector: run=http Type: ClusterIP IP: 10.97.103.237 External IPs: 172.17.0.44 Port: \u0026lt;unset\u0026gt; 8000/TCP TargetPort: 80/TCP Endpoints: 172.18.0.4:80,172.18.0.6:80,172.18.0.7:80 Session Affinity: None Events: \u0026lt;none\u0026gt; 向服务发出多次请求，可以通过返回信息看出请求将在不同的节点处理。\n$ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-x2xxv\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-q29v8\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-q29v8\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-x2xxv\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-q29v8\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-q29v8\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-m6tjp\u0026lt;/h1\u0026gt; $ curl http://172.17.0.44:8000 \u0026lt;h1\u0026gt;This request was processed by host: http-774bb756bb-x2xxv\u0026lt;/h1\u0026gt; ","date":"2021-07-20T11:27:00+08:00","image":"https://www.catfish.top/p/k8s-basic-3/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-3/","title":"Kubernetes初探（三）"},{"content":" Katacoda在线课：Launch a multi-node cluster using Kubeadm\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n 在此场景中，您将学习如何使用 Kubeadm 启动 Kubernetes 集群。\nKubeadm 解决了TLS 加密配置、 Kubernetes 核心组件部署和额外节点集群加入的问题。启动的集群通过 RBAC 等机制开箱即用。\n关于 Kubeadm 的更多信息可以参考： https://github.com/kubernetes/kubeadm\n初始化 Master Kubeadm 已经安装在节点上。软件包适用于 Ubuntu 16.04+、CentOS 7 或 HypriotOS v1.0.1+。\n初始化集群的第一步是启动 Master节点 。 Master节点 负责运行控制平面组件、etcd 和 API 服务器。客户端能够与 API 通信，能够完成工作负载的调度和集群状态的管理。\n任务 下面的命令将使用已知的 Token 简化初始化集群的步骤。\ncontrolplane $ kubeadm init --token=102952.1a7dd4cc8d1f4cc5 --kubernetes-version $(kubeadm version -o short) [init] Using Kubernetes version: v1.14.0 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [controlplane localhost] and IPs [172.17.0.86 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [controlplane localhost] and IPs [172.17.0.86 127.0.0.1 ::1] [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [controlplane kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.17.0.86] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 17.503676 seconds [upload-config] storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config-1.14\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --experimental-upload-certs [mark-control-plane] Marking the node controlplane as control-plane by adding the label \u0026#34;node-role.kubernetes.io/master=\u0026#39;\u0026#39;\u0026#34; [mark-control-plane] Marking the node controlplane as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: 102952.1a7dd4cc8d1f4cc5 [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.17.0.86:6443 --token 102952.1a7dd4cc8d1f4cc5 \\  --discovery-token-ca-cert-hash sha256:ab56a643a2d683bc1deeb483f0f946d4a774c4 在生产环境中，建议排除使用 kubeadm 生成的令牌。\n需要客户端配置和证书来管理 Kubernetes 集群。这个配置是在 kubeadm 初始化集群时创建的。该命令将配置复制到用户主目录并设置用于 CLI 的环境变量。\ncontrolplane $ sudo cp /etc/kubernetes/admin.conf $HOME/ controlplane $ sudo chown $(id -u):$(id -g) $HOME/admin.conf controlplane $ export KUBECONFIG=$HOME/admin.conf 部署容器网络接口 Container Networking Interface (CNI) 容器网络接口 ( CNI ) 定义了不同节点及其工作负载是如何通信的。有多个网络提供商可用，其中一些在 here 中列出。\n任务 在这个场景中，我们将使用 WeaveWorks 的 CNI 。\n/opt/weave-kube.yaml 可以通过 cat /opt/weave-kube.yaml 命令查看部署定义。\napiVersion:v1kind:Listitems:- apiVersion:v1kind:ServiceAccountmetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netnamespace:kube-system- apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netrules:- apiGroups:- \u0026#39;\u0026#39;resources:- pods- namespaces- nodesverbs:- get- list- watch- apiGroups:- networking.k8s.ioresources:- networkpoliciesverbs:- get- list- watch- apiGroups:- \u0026#39;\u0026#39;resources:- nodes/statusverbs:- patch- update- apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netroleRef:kind:ClusterRolename:weave-netapiGroup:rbac.authorization.k8s.iosubjects:- kind:ServiceAccountname:weave-netnamespace:kube-system- apiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netnamespace:kube-systemrules:- apiGroups:- \u0026#39;\u0026#39;resourceNames:- weave-netresources:- configmapsverbs:- get- update- apiGroups:- \u0026#39;\u0026#39;resources:- configmapsverbs:- create- apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netnamespace:kube-systemroleRef:kind:Rolename:weave-netapiGroup:rbac.authorization.k8s.iosubjects:- kind:ServiceAccountname:weave-netnamespace:kube-system- apiVersion:apps/v1kind:DaemonSetmetadata:name:weave-netannotations:cloud.weave.works/launcher-info:|-{ \u0026#34;original-request\u0026#34;: { \u0026#34;url\u0026#34;: \u0026#34;/k8s/v1.10/net.yaml?k8s-version=v1.16.0\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;Mon Oct 28 2019 18:38:09 GMT+0000 (UTC)\u0026#34; }, \u0026#34;email-address\u0026#34;: \u0026#34;support@weave.works\u0026#34; }labels:name:weave-netnamespace:kube-systemspec:minReadySeconds:5selector:matchLabels:name:weave-nettemplate:metadata:labels:name:weave-netspec:containers:- name:weavecommand:- /home/weave/launch.shenv:- name:IPALLOC_RANGEvalue:10.32.0.0/24- name:HOSTNAMEvalueFrom:fieldRef:apiVersion:v1fieldPath:spec.nodeNameimage:\u0026#39;docker.io/weaveworks/weave-kube:2.6.0\u0026#39;readinessProbe:httpGet:host:127.0.0.1path:/statusport:6784resources:requests:cpu:10msecurityContext:privileged:truevolumeMounts:- name:weavedbmountPath:/weavedb- name:cni-binmountPath:/host/opt- name:cni-bin2mountPath:/host/home- name:cni-confmountPath:/host/etc- name:dbusmountPath:/host/var/lib/dbus- name:lib-modulesmountPath:/lib/modules- name:xtables-lockmountPath:/run/xtables.lock- name:weave-npcenv:- name:HOSTNAMEvalueFrom:fieldRef:apiVersion:v1fieldPath:spec.nodeNameimage:\u0026#39;docker.io/weaveworks/weave-npc:2.6.0\u0026#39;resources:requests:cpu:10msecurityContext:privileged:truevolumeMounts:- name:xtables-lockmountPath:/run/xtables.lockhostNetwork:truehostPID:truerestartPolicy:AlwayssecurityContext:seLinuxOptions:{}serviceAccountName:weave-nettolerations:- effect:NoScheduleoperator:Existsvolumes:- name:weavedbhostPath:path:/var/lib/weave- name:cni-binhostPath:path:/opt- name:cni-bin2hostPath:path:/home- name:cni-confhostPath:path:/etc- name:dbushostPath:path:/var/lib/dbus- name:lib-moduleshostPath:path:/lib/modules- name:xtables-lockhostPath:path:/run/xtables.locktype:FileOrCreateupdateStrategy:type:RollingUpdate使用 kubectl apply 命令来完成部署工作。.\ncontrolplane $ kubectl apply -f /opt/weave-kube.yaml serviceaccount/weave-net created clusterrole.rbac.authorization.k8s.io/weave-net created clusterrolebinding.rbac.authorization.k8s.io/weave-net created role.rbac.authorization.k8s.io/weave-net created rolebinding.rbac.authorization.k8s.io/weave-net created daemonset.apps/weave-net created Weave 现在将在集群上部署为一系列的 Pod。 可以通过 kubectl get pod -n kube-system 命令查看状态。\ncontrolplane $ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-fb8b8dccf-gw8z6 1/1 Running 0 12m coredns-fb8b8dccf-qzd49 1/1 Running 0 12m etcd-controlplane 1/1 Running 0 12m kube-apiserver-controlplane 1/1 Running 0 11m kube-controller-manager-controlplane 1/1 Running 0 11m kube-proxy-2kjsl 1/1 Running 0 13m kube-scheduler-controlplane 1/1 Running 1 11m weave-net-2dtbs 2/2 Running 0 82s 需要安装 Weave 在你的集群中时，可以在 https://www.weave.works/docs/net/latest/kube-addon/ 找到更多详细信息。\n加入集群 一旦 Master 和 CNI 初始化完成，其他节点只要拥有正确的令牌就可以加入集群。令牌可以通过 kubeadm token 进行管理，例如 kubeadm token list。\ncontrolplane $ kubeadm token list TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 102952.1a7dd4cc8d1f4cc5 23h 2021-07-21T10:00:20Z authentication,signing The default bootstrap token generated by \u0026#39;kubeadm init\u0026#39;. system:bootstrappers:kubeadm:default-node-token 任务 在第二个节点上，使用主节点的 IP 地址，运行命令加入集群。\nnode01 $ kubeadm join --discovery-token-unsafe-skip-ca-verification --token=102952.1a7dd4cc8d1f4cc5 172.17.0.86:6443 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.14\u0026quot; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. Node01节点与 Master 节点初始化提供的命令相同。\n--discovery-token-unsafe-skip-ca-verification 标签用于绕过 Discovery Token 验证。由于此令牌是动态生成的，因此我们无法将其包含在步骤中。在生产环境中，使用 kubeadm init 提供的令牌。\n查看节点 集群现已初始化。主节点将负责管理集群，另一个工作节点将负责运行我们的容器工作负载。\n任务 Kubernetes CLI，称为 kubectl，现在可以使用配置访问集群。例如，下面的命令将返回我们集群中的两个节点信息。\ncontrolplane $ kubectl get nodes NAME STATUS ROLES AGE VERSION controlplane Ready master 25m v1.14.0 node01 Ready \u0026lt;none\u0026gt; 2m v1.14.0 部署 Pod 集群中两个节点的状态现在应该是 Ready。这表示接下来可以进行调度和启动部署。\n使用 Kubectl，可以部署 Pod。命令总是由 Master 发出，每个节点只负责运行工作负载。\n下面的命令是基于 Docker 镜像 katacoda/docker-http-server 创建一个 Pod。\ncontrolplane $ kubectl create deployment http --image=katacoda/docker-http-server:latest deployment.apps/http created 可以使用kubectl get pods查看Pod创建的状态\ncontrolplane $ kubectl get pods NAME READY STATUS RESTARTS AGE http-7f8cbdf584-jcdrj 1/1 Running 0 70s 运行后，可以看到节点上Docker 容器的运行状态。\nnode01 $ docker ps | grep docker-http-server d874d8c3151b katacoda/docker-http-server \u0026quot;/app\u0026quot; About a minute ago Up About a minute k8s_docker-http-server_http-7f8cbdf584-jcdrj_default_2894ce10-e945-11eb-b87f-0242ac110056_0 部署仪表盘 Kubernetes 有一个基于 Web 的仪表板应用，提供对 Kubernetes 集群的查看与管理能力。\n任务 使用 kubectl apply -f dashboard.yaml 命令部署仪表板。\ncontrolplane $ kubectl apply -f dashboard.yaml secret/kubernetes-dashboard-certs created serviceaccount/kubernetes-dashboard created role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created deployment.apps/kubernetes-dashboard created service/kubernetes-dashboard created 仪表板将部署到 kube-system 命名空间中。使用 kubectl get pods -n kube-system命令 查看部署状态。\ncontrolplane $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-fb8b8dccf-rjzxp 1/1 Running 1 88s coredns-fb8b8dccf-tw8cm 1/1 Running 1 88s etcd-controlplane 1/1 Running 0 16s kube-apiserver-controlplane 1/1 Running 0 23s kube-controller-manager-controlplane 1/1 Running 1 61s kube-proxy-6xv6k 1/1 Running 0 84s kube-proxy-fn84g 1/1 Running 0 88s kube-scheduler-controlplane 1/1 Running 2 61s kubernetes-dashboard-5f57845f9d-jblx7 1/1 Running 0 5s weave-net-6gcbd 2/2 Running 1 88s weave-net-8s6bt 2/2 Running 1 84s 需要 ServiceAccount 才能登录。 ClusterRoleBinding 用于为新的 ServiceAccount (admin-user) 分配集群上的 cluster-admin 角色。\ncat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system EOF Dashboard可以控制 Kubernetes 的所有方面。通过 ClusterRoleBinding 和 RBAC，可以根据安全要求来定义不同级别的权限。有关为仪表板创建用户的更多信息，请参考 Dashboard documentation。\n创建 ServiceAccount 后，可以通过以下方式获取登录令牌：\ncontrolplane $ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \u0026#39;{print $1}\u0026#39;) Name: attachdetach-controller-token-bltlp Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: attachdetach-controller kubernetes.io/service-account.uid: 0ced9bf4-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhdHRhY2hkZXRhY2gtY29udHJvbGxlci10b2tlbi1ibHRscCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhdHRhY2hkZXRhY2gtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBjZWQ5YmY0LWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphdHRhY2hkZXRhY2gtY29udHJvbGxlciJ9.x0a2m2SpBrwj8sARrvRfuct2ghrDydxYzyFmDL93hATdS_59zaueB4SrgHner8gOu_zrx9PLWcSoZNBxRblZuxJj8Di9TKUTjyDBE6txc4_0H8nseuQzljvRTbjUjEcL7fp8H1j4MrJgT4GrYU-n1gAOl6NIfk8FmpFpuUUS6G_IfIDfS60YpZRlqZGp14NuaL0RC71PsERnP6ZYnuRKTbYNfVNeURqVR4pY7XwCcQZFALaJcTf9SgwJLAAqLQFgKd9c2MYnHdnU5cnqjrYxi_b5kGBKUcRzACrHjh0uGG6ErHeo6-GbA-0NDdIMI7qKa3If0oh_GOQKmyB5cyx-Rw Name: bootstrap-signer-token-cstqm Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: bootstrap-signer kubernetes.io/service-account.uid: 0dc1b223-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJib290c3RyYXAtc2lnbmVyLXRva2VuLWNzdHFtIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImJvb3RzdHJhcC1zaWduZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZGMxYjIyMy1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Ym9vdHN0cmFwLXNpZ25lciJ9.hkciM9KpWYYPyi5MJbm67oqzuskjDB_Rj4Fz5THhtN_jJAO6go4oxz-NPparOi7kxaEU0yBLb_xLY42jOXvG_Q2Dd6gR2u9yc7nfwr67koW3lck4S0PCYSuGo-FqUndXS1a3BpT5SFAHhb52FdwEYiedevLz9C5wUDT8sVgMYSnHvEX_jwBmjrhhOohGxTNs_-_HyGMNNLfEOSbjaEtzkDFos0n3qPAJ95uaGKJ2laDsctVpHCeRx5hNCv8E7dvvR1Qp4XXDVDQrodkzkuG4LSKviYLzMLX1DM2XES3HQ7Q2uTtAICqxIDBjRlhfAPv_z02tqAbCf4fF6_CFlYK1ag Name: bootstrap-token-102952 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: bootstrap.kubernetes.io/token Data ==== token-secret: 16 bytes usage-bootstrap-authentication: 4 bytes usage-bootstrap-signing: 4 bytes auth-extra-groups: 47 bytes description: 56 bytes expiration: 20 bytes token-id: 6 bytes Name: certificate-controller-token-mw9rl Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: certificate-controller kubernetes.io/service-account.uid: 0cf891ba-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjZXJ0aWZpY2F0ZS1jb250cm9sbGVyLXRva2VuLW13OXJsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNlcnRpZmljYXRlLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwY2Y4OTFiYS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y2VydGlmaWNhdGUtY29udHJvbGxlciJ9.NIZpVmhuJUcrpbZJApUt7ODgqIPU9RN4UZcr6he83xA_66-4ORQVddj1oQQg4G0p4fSPckWfP0n7pSxahZ90VZ-NCxPeCVLThLZ563PUOIljsjfROgFLQ4eeWfRML0OHyhAgoTtfEdcHi4_IzyhSrvCOweM9oPFfdx9MdTNAG9znyQsCi4g1YNHwn7t4r8h-BouW4yRYMEOM9HVZkcDVWhmF9PDv9s235INvIAjco5JvCAeDKvw48hdSrm4RWQPmZ7yE71iBDymDF_Ntr9w12H_4FPOYcARfFOmIj1k5binKiHaOlWtfbKYGYFM4tAxqI-ErFx4lshkLNJm3ICMJSQ Name: clusterrole-aggregation-controller-token-xh52b Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: clusterrole-aggregation-controller kubernetes.io/service-account.uid: 0acea7db-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjbHVzdGVycm9sZS1hZ2dyZWdhdGlvbi1jb250cm9sbGVyLXRva2VuLXhoNTJiIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNsdXN0ZXJyb2xlLWFnZ3JlZ2F0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYWNlYTdkYi1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y2x1c3RlcnJvbGUtYWdncmVnYXRpb24tY29udHJvbGxlciJ9.SgSxj_9uNLbAU9zYK4sTUYL4STx9CSYflGe0GwqGz_f3K7Db5NhpCuCoCcLYVJ7I2xogXICDJV016ZaNEAKJHWNJNeO-Ye378zjgddYmLAxX0uNw9OfSSiHKq-ksHgkKmoxwixgzVw2Df6PZGeYrKZZMSGuW96hkzd5JJx9Jhg2c4k8Fpqcpg5t-N4F5ZC8Ix4THpOmBueB8BaKox6OS5kUxqBAmdkJxZur1Wl8BvJV3Pnl6qbzYa0oPqun6N-WckGjVLrOlll1E6moABT1i8udZeRSrsX7YjjWgnbVyuyP0vzY7T1OQz-jcS0Lw3shMSIeANpwfFFB5XNvacsIV9A Name: coredns-token-z5tnd Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: coredns kubernetes.io/service-account.uid: 0ce34e5b-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjb3JlZG5zLXRva2VuLXo1dG5kIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNvcmVkbnMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwY2UzNGU1Yi1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y29yZWRucyJ9.vTVtFW7YZ-_dxjyiHjrkJnxyw3Q1c2oy43wEfs71tnD5GIDcXjioGXRn4OFvGJYzUzSM7fujZy6e6lXUDZBx-PkiRZzaqbxZTuY1skfTKdAm31h6xbOESSvuFcR97eASVMv5WNdFNSXZik4P3_pEoF7XUKnOnY-o3nk-MZNWRhmdcOvj-BhBmTqXY9CQDvaW-P2MSQhaGvP-yVYBFHqNvOH_XgaBVdW5c35AuLGz3pVAiBl9s_Ghrkc5h0KrhU_eko5YAMGcXHUQPask-EVy4MYes_3ube_PbQLTvTgVZ6XwMjPNIOkYvrGsZrRZbRCDXKqc0KBFi2YT_dQWaoiNgg ca.crt: 1025 bytes Name: cronjob-controller-token-6hfqf Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: cronjob-controller kubernetes.io/service-account.uid: 0e4b090e-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjcm9uam9iLWNvbnRyb2xsZXItdG9rZW4tNmhmcWYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiY3JvbmpvYi1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGU0YjA5MGUtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmNyb25qb2ItY29udHJvbGxlciJ9.SKICsFaSIGnrn9pfqWg_Vrw1jBvYz_bc95tJ53FL_M9b6YQKgHRQZZH1DQciGdSd27nYSWnoG9L-ftKEZhaU7ABAAga3lNzyDJhgPdYtCy8OlxzY2nY2RdDbCsFRjaK76aIJQf_u0z8K-JSz6CjELBj294WWbXoAreCtZiQGRrrupgtUIPjUloaqx07BO2Y29N2ZMkELj_Ye5rSeffnz_djSFdwTIt6B3Jtxjp55DZjMzaj_coqmVXfGq4K90wkKlWLIbbAGYlRqEfJCpIK2OeCy5WF2EahlAUECHEQi5NqEQ1DoiVUj1LtdwQOGKXlQYT6h6GSLY_E1Q1Xv3vWFjQ Name: daemon-set-controller-token-6ftrt Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: daemon-set-controller kubernetes.io/service-account.uid: 0e251630-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYWVtb24tc2V0LWNvbnRyb2xsZXItdG9rZW4tNmZ0cnQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFlbW9uLXNldC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGUyNTE2MzAtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhZW1vbi1zZXQtY29udHJvbGxlciJ9.OTdB75fb-wdKbbcZJ3mS08JuJsq3urdovVUgiADbpAZnL4pVZWdAmrLadYVKFNqfXOlCaSqQefLNgSWsGcJee-YSQT6snP87Wplfo7VH05h9Xwnoz21uWpkjhIlioWbcKL1asBuBsvpSczqg4Bt3cyByxdrIJjBmBvoJpFhwf04IqyJ6ugxDLPTH-8CC5r5Xn6etm8Ey-p1rwQJb1u2FEq8K97nihv87TByrHLQv-CMn1wxfGHDYOEAneis2s0pWotvxrHWkFg1WnriDAq4hryGPDA9GRLqveFdCL76XTuT9plYFYMxLRF-8_EuwOwTa0F-LYm98o3iDhbvwkTDhiA ca.crt: 1025 bytes Name: default-token-jw2ql Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: 0fdb30ea-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLWp3MnFsIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZmRiMzBlYS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.ijnMq7u2xRTNKXov89YF67O27Jn7uLqnRYzK4V95pWt22dN3Qayf7U-jlLm6uWec79FLKNM_08IjnobzVxnzrFhRqLqOT55mkyLfXdNdR9EE__CrPOs-PiSIsHNbAG64bUSDOwbGaFjKROFBz1dwHj1O_XQZfLNaH3_LhLzOHtxJ4_FyOOf-hhjEWgjOptptTMkU3N_z7TRU4zzX9BUWTDg5s6JBpFkDSbZ-_yu-uzl4JiEBUwf0MY3YC79caRp9uFgwuaMCoFJYV7gSqWA5IpqgQe225LBTBS2HHLPJE67WLlSrkiEe4gvngnGtO_1RkO2MBCXgVp5icSFw4zxEzw Name: deployment-controller-token-fctz8 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: deployment-controller kubernetes.io/service-account.uid: 0cd7cffc-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4tZmN0ejgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGNkN2NmZmMtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.uobekKRLj02hr-qsNZCPNZWER2mYiZj8NCygiXz5c_r9B7AjhHkq3MlcfPe1RoU89P3hofrNWQpTY1Kikl9nspKQ_Yve2ykZidGGGoOtuaqI2h-SfNMjeqHGXM0CmQAEigrmkQtDrkwt-lz3pXEacRbh5pmS8BOWvpgfc5e8qt39YebFTKygNvCYvMgYMy5MsufSDeifFcb7J12hhXczxvYLDcbgDewhEnpxA887KaQlCML8soNmM36bC5qEU3nrGnKg1a5kvjjxaxres7VRb4U9fs0xogLAzcAsuOd40Re7QY8wNdY7dcUg2qivFSm7P-3vbiZ9Twr1kftazuR6Gg Name: disruption-controller-token-x8n2f Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: disruption-controller kubernetes.io/service-account.uid: 0cf3a17c-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkaXNydXB0aW9uLWNvbnRyb2xsZXItdG9rZW4teDhuMmYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGlzcnVwdGlvbi1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGNmM2ExN2MtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRpc3J1cHRpb24tY29udHJvbGxlciJ9.mu_3XUNoOGZwHlTyhKygDwAFjIWY0Uf0foIiS4vGNsnBBe5AUz3bN7gD0f7EpRZoI7KeIL-OWYGLvjtsCgC-1lYCRm9DNLmNxYnoDiNcbpCFx-xHF4E4yl4v51oJtXG1Bc-Xva-S15US673Gzv-soVAfpVKOzYQVklM1cbim__Eb_vXEZ_i0r-KD1DRNERMZWjvJ159DuiKMjd6CCzkgXCSQV1K4jS2jd2taiARKUcGNw0rQBeHh1_IN460jnMGCLM0lzbcZZAA4YKhdMfsU2AjIlsvK4k6VS5G-w4lo3PA1JW4Ve-Z-Im94Lqd67A2MSqUhWLPw1cn4PGGyWTaiJA ca.crt: 1025 bytes Name: endpoint-controller-token-qbz7h Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: endpoint-controller kubernetes.io/service-account.uid: 0ab80371-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJlbmRwb2ludC1jb250cm9sbGVyLXRva2VuLXFiejdoIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImVuZHBvaW50LWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYWI4MDM3MS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZW5kcG9pbnQtY29udHJvbGxlciJ9.z4jN9B3eHKiw7RTSz-UM0RqGiPXX2-7bV9d6N-6hkfPDCfOj3BwjApxqc3FoDiPeazVSNT-cmORpgAP3b5UMYYnN5wNw2R4XfysUXfY46i1clQJqZ14o9olfX1KegC1T-yZOC8MoEHMFvngem5gEfQquW6cIM6hYQ-kgWHaYRZwH00OqPg9YchBpmmm7erx5PZXmlWmZ76lobAyi-nAzShqJA5mKl0amSwzfQfd7ErZJhsHJrWynqpJi_SSTRsDL2zYY79aiNEMaXe4nXa0rlyVNZmRRiaA3_ca665S840KTMjvLfub4fShxpRib8EEnRJOAJft6kyfCn6bf2Uf1Og Name: expand-controller-token-q7k8m Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: expand-controller kubernetes.io/service-account.uid: 0fc0ecea-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJleHBhbmQtY29udHJvbGxlci10b2tlbi1xN2s4bSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJleHBhbmQtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBmYzBlY2VhLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpleHBhbmQtY29udHJvbGxlciJ9.w0CjgPX7fKK1huV-Lt0ZRGLo5JgMiAR9yn11R0CrsIyDyPGRlwOrUoDSRxNn0t__Bq6YSjblPDAjz0hh2PToc44g4abe1aSfF2cXtps1y87Jee78jQ3rF9vGzinqPCJGmiXCZNvAunEAT88KkiaND74X3rHgPU-E0XjlNVyjlwMRUQWpA4Z7rqMTSb448L9vjFETbT4n7jdFib3iijeP8MgcKDxiZYpkKhQcsvvO-r-r6ndHRw7_eO06Yyl-nW20NALbc3CB8gl-1JLuFkB0CsVE2J8D3s8vkQ1rd0fMPo67cgqnc5jCULRgKWYoKOaGtcCc_UdxOg_P_9gS3zfhjA ca.crt: 1025 bytes Name: generic-garbage-collector-token-qkjc7 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: generic-garbage-collector kubernetes.io/service-account.uid: 0f46f27b-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJnZW5lcmljLWdhcmJhZ2UtY29sbGVjdG9yLXRva2VuLXFramM3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImdlbmVyaWMtZ2FyYmFnZS1jb2xsZWN0b3IiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZjQ2ZjI3Yi1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Z2VuZXJpYy1nYXJiYWdlLWNvbGxlY3RvciJ9.cVdg68sCtTFnHtZenBGyzJx5RRxPekXORa8L4et-5F5WVoAyvVLIc4g1kPEWgdrGQwAbXsgjowYEGZiZZM7VjHLTKpiHaYuYyFjPVtK-bBc4EuB4bPhX-5h0w5A5e1npDTqRHohIel9Q4Tx9bCfMGtzoP9C33Dog5kIFbNA1h45YaT5DUSIe9lnD8MbDbf6JoLLitB-jJdv9B7oscm7b-azrZpoU6ffe6KifzkZmlfbjwuQJtXDzgL4fD0wzTCfNP1Iun69u5NdejbEhWA2lS0Gt4KNgMX3wSQgL_4_htOs2__PwlH3d1F-VmZFweVJD6CSBqpttsgBEO3y1dwkM8w ca.crt: 1025 bytes namespace: 11 bytes Name: horizontal-pod-autoscaler-token-v46ss Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: horizontal-pod-autoscaler kubernetes.io/service-account.uid: 0d30b7d4-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJob3Jpem9udGFsLXBvZC1hdXRvc2NhbGVyLXRva2VuLXY0NnNzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Imhvcml6b250YWwtcG9kLWF1dG9zY2FsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZDMwYjdkNC1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06aG9yaXpvbnRhbC1wb2QtYXV0b3NjYWxlciJ9.0nwfw8R2M_qtziNugziYCoGFn-aXOHUrCWea6akqBGXBaIXVW0OCeLip3oW2WO2Il4LMHAFL_xSeqCkrIXgCO_LYGGbnQ0yS_QfLsDTDUzPOZzx4_o9COOWF-rfz_7OK8T01k5xns5_Gjxhh7sWpXv5IrrGeFhIWAS10d-CPXAOx-pt2r0aG4Vg-Pn3bPVi7DZvgED5LURx-kFSTsRRN8HX32jNmLKCcQ_mn3MRrBeOohf2tvCISqVzkMZfSN-fHiemZLXqQQlD25fC1zkLLQzDHoQn_A1VKH53Ac12xso0Z1r1tK4F38L85QLhV7QE-471kYjgJMJsSWv3SbcmnRg Name: job-controller-token-cmwfj Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: job-controller kubernetes.io/service-account.uid: 0ac95588-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJqb2ItY29udHJvbGxlci10b2tlbi1jbXdmaiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJqb2ItY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBhYzk1NTg4LWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpqb2ItY29udHJvbGxlciJ9.JFVVRYktyZFtJTwkepgJ5xFLJZXJReBN11ebo242b7LmembqxN3dw6qaNqd9gKJpaekzcUSJaCQDlHBivbW637YSfBsR2zov3US1_lVfm1jRCJ9Li8787-V8YcQmUCyM-gxbqwER15Sx3fGYhQicgDDVLuQ1JDfz3-9RTScKKTHOoFdpU0cmfB4kfgi4OaSl5hRvW33mi4psxLem-TjXV6EmYgGvfduVm6TGoewT-3uU2K7b9_rMLDLz6B85uolAJ_V0wajv8ZZjerTon3NyLFo5Uta_zt-gauEzaxGTVk4GHYIMhQ6-PTPZ2KpQVN8MI3j7kF9KJ9-nZ5gsvyEI4A ca.crt: 1025 bytes Name: kube-proxy-token-ssgzc Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: kube-proxy kubernetes.io/service-account.uid: 0cea9f15-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlLXByb3h5LXRva2VuLXNzZ3pjIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Imt1YmUtcHJveHkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwY2VhOWYxNS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06a3ViZS1wcm94eSJ9.IzyixUsktQs4oQAQ-r1PTbtRrBrVf7elWCypxBkUHDhBjNQOZIjt0VnIjRoTdJenIOCH6tAmeMddJMWs8nYRMte_Pi-_XylesQypzAeiuE4oT8bxGZAC2H6JJV1D2OIrYaABWJzVejqakkrtsk5RQBbMMyXlkFK1_R1YP-5XmhzyNyfCaKzi9cprbQNX3IV72I9R9DUo6YwO3j0r_5cnKMfgPAk95ACrwqglYMpLWCFFRDjS7vWSW_ue65Yohvs85xBvYiG_ybkN75eegQYt-1AS6T0S-TBY2V3lyfO4k52VomAm34d1WgPjUyWUEun5ywH9ucDU7T_tGH4rzf66Kw Name: kubernetes-dashboard-certs Namespace: kube-system Labels: k8s-app=kubernetes-dashboard Annotations: Type: Opaque Data ==== Name: kubernetes-dashboard-key-holder Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== priv: 1679 bytes pub: 459 bytes Name: kubernetes-dashboard-token-njndv Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: kubernetes-dashboard kubernetes.io/service-account.uid: 41afcd0e-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1uam5kdiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjQxYWZjZDBlLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.LirJ7cHBUyF30S6KKjKMNI27WLL2MC2gPPhJc1GovzWgTwUWVPYImNcXYoUOVUZ-ZJwqT9ukOH_0vxF2kblKW3xSmB_rHQ3cvs6daVNA5-HPeBT8kq0EhiRlFALYezgKpCgkpiXvTE6MGNjMhHPJ0C8yi8jEY6OC9Y4DZQXmPQV3B2Xpk7ZoaMXoHFHfPTDSpNdeWSUJN9JErrmVBFpWeTuGI6m9A2wInQY1zjLiCm-iH9y0AbpwYg1QKGWNvKVSwYkS4ViDA6qKEzGWQxWSbCdsCIGhjGNz_nxMoVm1yPeU4W1yn-Psk6LLiIzSpizZTl-3VfIR0dl5mNHq7N3KPg Name: namespace-controller-token-smw7m Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: namespace-controller kubernetes.io/service-account.uid: 0f20a4d0-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi1zbXc3bSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBmMjBhNGQwLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.pQGalvSM_lh41fNKsY2ddZghUe_lJuz9Y9rmbXY0V-RjmF0BagSobz9SxucHAAOttIraamgTKksbxLEoAV509Yfxf62ybE4cW9KPggozc8iluwGLkdWYdGZxVe4midlnYcIpYTv4Zueav8f-9W2cGWyzHustTZf3R9b2A5N02uT91aZ38QWllsy_WKMVAaDLCLel71koJ_HjjDqrZ_ObO-YAtBeA9zslY0nhH-mpj0WsjW76Af4pVTotFBGdqtO9eV3Oe_H6sy6y2BAjxItGDTU9xvq7f3-taa-SB9j5XbEPJ1I8ObxcOLhYtvyCcHf2ZbxR9vVJ1CLR4ymry1mD2w Name: node-controller-token-9ppdt Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: node-controller kubernetes.io/service-account.uid: 0d0b0ff7-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJub2RlLWNvbnRyb2xsZXItdG9rZW4tOXBwZHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibm9kZS1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGQwYjBmZjctZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOm5vZGUtY29udHJvbGxlciJ9.Hg-v6PYzmoSmC-FVhE07OVOtLSGb7eIDPeY77k9l4tlCT8wDPNSo3zF8LKlsKCehDeKVfQL6bdcG596ECF_Vh0zHjswZrZMCsOWO0sBArDLzOu6Zo2wgvbkPZHYA0X7nt5jc10W_q-Nw1Ud3WLNtW7v4iWpbeGZGz7EgCrqW5XqYxA9P8CA4jdMBmdit-zwjaJUu5jjIYRbeYQhKF8hrC00vMXyYMRZp2dzGSMAmI4yF2OO-QdKc1Mieq-w4i8pOl7gO4nYRLQCAcdY2TjCO5VbcigZ6IWzzQb120WgC2Iqd5mhSzcmMFcrT1IbzOm91bFkgwGAs1oQw8Cy8iCV9SQ Name: persistent-volume-binder-token-9hjlk Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: persistent-volume-binder kubernetes.io/service-account.uid: 0fb0da85-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwZXJzaXN0ZW50LXZvbHVtZS1iaW5kZXItdG9rZW4tOWhqbGsiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicGVyc2lzdGVudC12b2x1bWUtYmluZGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGZiMGRhODUtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnBlcnNpc3RlbnQtdm9sdW1lLWJpbmRlciJ9.MbBQ48J2l7rgogx8YWgbX5ys_oyNBLBOn0xsuhO5_8jIKLwV5hup5X2JdjwTCw0uGZ0lkYgYANXnnK2Q39mlsJLMYCudaVWGZm7IumjyLuu9CkvhUmEHhYmqY2if-PAAXxaO_hKSbEpsmBEQKWGnU69wUdsEoQdBwqlHg4jZ69dXvrMsBgJ96ic_511e9B8R_GPKS1IXocTJtFcbCp-rpbk9REBGYQB-Fjh2UBpPvmeFQcfZ39yvBRMmi1LWW7sTQtwEiTErgYcIPpr7klSguSApm9un75P4tCjLPNFbIjUbf3lCqonwtEwADD1sKg2f9TSxdverIhoXX90IBJBTUA Name: pod-garbage-collector-token-l42r5 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: pod-garbage-collector kubernetes.io/service-account.uid: 0dfebd7d-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwb2QtZ2FyYmFnZS1jb2xsZWN0b3ItdG9rZW4tbDQycjUiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicG9kLWdhcmJhZ2UtY29sbGVjdG9yIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGRmZWJkN2QtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnBvZC1nYXJiYWdlLWNvbGxlY3RvciJ9.VaMklkb7hlu7MRkn0rE2Tlu-lJoeXQI4aB7qVGGHisp3nGVOjSodrldgQUxFnO3oJ9invV6RcYIS-yCaBfL8c0tak7yrGSWI2gjIrM-nS16PgLDYPjbZqJk9wXKIdzqt63Nt8HvQfBdi0IKmyM8eszOXywpHX9wyJFBGH5PYRdBs3HbpzNg5_GdjPO2IGbQGwZyyonb0o1_xk4GH-bAfbe3fCsHN5N1zY39DNo-qJ6xWC89Xfufgw95-_JxR_Pd7IiuUyqFN9KxqrbvdqdtML2ZS53fvpNqZ62_a9nK6xefGIgfxBXov_eqsjlBKh7R0vXyFAHpn3yTLUKZP-zHtGg Name: pv-protection-controller-token-qp2xr Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: pv-protection-controller kubernetes.io/service-account.uid: 0ed4594d-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwdi1wcm90ZWN0aW9uLWNvbnRyb2xsZXItdG9rZW4tcXAyeHIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicHYtcHJvdGVjdGlvbi1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGVkNDU5NGQtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnB2LXByb3RlY3Rpb24tY29udHJvbGxlciJ9.e52Sqc6aGtwNO4ecQRqddvzWsqkVOvj0MtMT8DmLf21sxv3W0yDDJYYTz6rzljqJPOkP7JBoNVoZL80e0NCXArmA27B4vCPTzCnSCrSSquoljMvxFmalnnJke6TQZKhplDtMy16orAM00Dr4KAUcphDP0uwO2Lsu6uSyFxiMQ53nRxzRt8fIf2HJZecHnvxA3qYKDpJ9ceZK0EgtzSBCSY8qNW9hVvAlUUjCkoJHeSzBeIrJTrI9GzYkkeqWfIdLy9fBpYHiCSxScKg7IVQ7iPqyZjYoo9BFKeXiIS87fD9qCh_xOAIKtmjAWmgaabEqcUx-kefbVTNKLhWn1AUyGw Name: pvc-protection-controller-token-b6q9s Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: pvc-protection-controller kubernetes.io/service-account.uid: 0ad1b174-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJwdmMtcHJvdGVjdGlvbi1jb250cm9sbGVyLXRva2VuLWI2cTlzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InB2Yy1wcm90ZWN0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYWQxYjE3NC1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06cHZjLXByb3RlY3Rpb24tY29udHJvbGxlciJ9.TeJQ1uCU_Jx5c7L6eZ2Is20BT9SmwvadmZdK7fF4W0jGzlohLbS-ACQQJTMszWxti9BiAxke_bijrUoKZElDlbNpgXqiqnxaIAUdJIkVPm8IQpVN7d0GfYmEeHcUl5kUHO5Oc9AFqFcQeknxcTo8RI839SXp595DHTq0SPT3gaMfSqpV3dHIlSHixBke9oG9bTYo4OoAOljfq2OT30JQSk_Z-6-_ftvCxyVtgGZGs9jyC462ic5oxQ8U4BwJoPxmTCXn_aQPXygmlONMsAo4HIRzCmv2eey44oKvGBR5cInhGaoP-SUWuc0JG1PZFDLIllU2lod-DBSdxm7err6cTQ Name: replicaset-controller-token-9mx8t Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: replicaset-controller kubernetes.io/service-account.uid: 0cf1ad30-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXBsaWNhc2V0LWNvbnRyb2xsZXItdG9rZW4tOW14OHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicmVwbGljYXNldC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGNmMWFkMzAtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnJlcGxpY2FzZXQtY29udHJvbGxlciJ9.c5PbPe1CGfhMX1LzxQcQKPm3bNIFjwf9HXtSHs-wzF3caLtwNqiqczQid6F2Nf2EWbgG-ZarSJ1ESMdoPEvNprF-prVNXxeoTAuX4C9re-0zcCuZTGpxRvLq_IPZRJUHj12d3Lr8JAXdgJFBRvwQZhgDe0Tyinh5JJnNwlWvZ07mMBOVbAgcq2Ut_oAwu9DCiNv-gZHqBI-BISH6a7uF6YIdBOKIJxkTs3D3aaPYtyw-a0qBp2bAlRa3Un0IgjDUe9eiCGAc1cYRYuWUV-Ro2zrOBxwvVCh6r7iIWIuHb7y8Lc9NkbTXnxOSLwOyaCCkuioiSa_LytFlhCJlC09ubA Name: replication-controller-token-99sjd Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: replication-controller kubernetes.io/service-account.uid: 0efa8a2d-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXBsaWNhdGlvbi1jb250cm9sbGVyLXRva2VuLTk5c2pkIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InJlcGxpY2F0aW9uLWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZWZhOGEyZC1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06cmVwbGljYXRpb24tY29udHJvbGxlciJ9.d157wtS1_qaJdrOFJsmZR37qe7XwvIypzUOJHU4UZpvynL2Na2Wim1xH9-n5AlS-fO_VdwrkBSG7Ef8XV_sziJ9jJmLU2vXo_ZH6CVohviJyxkAob465QCVjjuvKwCgxS7vYCpJ78Y5Vdr7PDFu28X_kd3tNAIuhPMlhj5aeL5NWUmsRSo5aXl4DhoossqF81GkcmKe-kuMSnm9BOFNxokTDA2zFWbOtT6oK7ZSz2oKVN8YoIu5534nmH7-ydokDrcNgUnqy9ByTA8BAZp7ueMlIc5xktJlvt6sQ8a3gNg90j_3H2RCA1wQZJcIaB8IVVXe2bt0ZaQI9wrYBBb7vEg Name: resourcequota-controller-token-hdch8 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: resourcequota-controller kubernetes.io/service-account.uid: 0e881d99-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJyZXNvdXJjZXF1b3RhLWNvbnRyb2xsZXItdG9rZW4taGRjaDgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoicmVzb3VyY2VxdW90YS1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGU4ODFkOTktZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnJlc291cmNlcXVvdGEtY29udHJvbGxlciJ9.omLEn20jTg0sn6P8Gihen5l3IuxFNIxK79wv_q00aOsGsAK1BbFQlIOWKHqaO81yvS3T_7UXSyCr01HTVhXabsJPS82t1Gr7jzzN4AKM230nHW_VGRjgU2gN4GxgAYByOTXBootTINz37J9Kmz0HSdTkIuqylXR6pUVrLKYJU2YqPqWe1fcoOi8P66d5qmpXfj806hw2SJgvxrf36v9cGMeoK5JDa1rI7VFAVLR-kWy9LgLQpVzDfuNqHAznay1wBcT3_Kb6l4JOkd_QdNZRl0xsP1NZMVaPqrbsxxO5us3_idNtZvo9fSsQU9itxodtnqBeSSlnvZH1Kr4tVa-7eg Name: service-account-controller-token-n268k Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: service-account-controller kubernetes.io/service-account.uid: 0ad49e7f-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtY29udHJvbGxlci10b2tlbi1uMjY4ayIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJzZXJ2aWNlLWFjY291bnQtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBhZDQ5ZTdmLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpzZXJ2aWNlLWFjY291bnQtY29udHJvbGxlciJ9.gVr-OJa0nVuUTFjsUwdAeiQQHEFVztiXUQfV7AgRmi850C1QiwpiBAejT8wOK6lwhOrhzT5EQ77ITqmTQZz1BC_PfZRgdNdZz6Ytw01TW4AeWe3A723yiqFDnasmFB7XhbKDQxFLd0dw5VjlDsdaEu0IIU0rdQKul8EUiccJolP2s-5V4FvvPfcPqnuFTlbbTiv509DE9G1ZTcZ4TxcBx4-0aiIe_vdUjuE2ECaOIPry0rN1yg3JNA5mJhOM6hAfmaz-DzH7DNh134RiWdHxQxQqeoRrlby2aS7IayT_JSiSZecoYGvL-CFteJdzMEWqWzIzMuDb9uzHJ1CBzOezbQ Name: service-controller-token-dgqch Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: service-controller kubernetes.io/service-account.uid: 0ce95734-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJzZXJ2aWNlLWNvbnRyb2xsZXItdG9rZW4tZGdxY2giLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoic2VydmljZS1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGNlOTU3MzQtZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOnNlcnZpY2UtY29udHJvbGxlciJ9.b7Y5beL-iAl64nYe_gTiD8ogwl5GTLpeE9GBxaEMKy3KbLiags3p-0ZqXLoDD_e91BaPxpmUyRfavWV5lZ85uzzBdH-q4aD6FnwHBmoTwbw3TK1znxCeri9xObQNmxDPH7CjVQLVHbksMFGum1L1xWCf3dw0p2ZrPbgzRzq93uUYjiW-w2H40Ub0q9TFTp9ic-T6wRq4DqT8XgBKa__nNblCHiM8hlZ6ufHFYeE4aAFcLLc1RaKvFxH7oO10AJ0fB45NwaMPa1iC0O5dTl57jQbh9mxusLtKAysAMMugObnCJ9yYSA65n9p9OsYTYZVp0OOCRGu7P4lO3XR3ZLr1-Q Name: statefulset-controller-token-j7858 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: statefulset-controller kubernetes.io/service-account.uid: 0ce28a94-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJzdGF0ZWZ1bHNldC1jb250cm9sbGVyLXRva2VuLWo3ODU4Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InN0YXRlZnVsc2V0LWNvbnRyb2xsZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwY2UyOGE5NC1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06c3RhdGVmdWxzZXQtY29udHJvbGxlciJ9.ju-pOwheRoeE61w_NiQ8jLXFFhIb0SA6uaOFcKR5SmaN4bhQYIdlK0IywWQwu6RzCzIgCr4XrGrWsxblNpBMQC2-dNIuELWIMAFR5gjKQZh4v2Zmh4ysXGhbj7repAOLYvcseN-tGW3hax4IywN0GI5Pw257xoHPXZToA9lnqnIPiVdzlSOe8WBfhv6omdeGmtrgNGkgAjh2LiXViwPW7V3DVPjIY5d3k0v3pqzHSuxNYTRdjqcg53TMNTUFe2LKTVbzmgLngdKufOPmtabtnxU21r1ua887BCxl9RO7FUm01CTBlofKF5wkaGr5_w6zXRVgLE6Fo4YrhILf1z1BaQ Name: token-cleaner-token-9fvhw Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: token-cleaner kubernetes.io/service-account.uid: 0eb7ce45-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0b2tlbi1jbGVhbmVyLXRva2VuLTlmdmh3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6InRva2VuLWNsZWFuZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwZWI3Y2U0NS1lOWQ0LTExZWItYjI1Yi0wMjQyYWMxMTAwMDkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06dG9rZW4tY2xlYW5lciJ9.QDNslXVqy5OPVb2mKpgsBDpWbTIaSgaZtjnvwwlsxyARYyLDuBHHpik2_IcmVI94riv3LF-WKI2m3fw9uAlkQRmebQCNCDYKWiJEttvEXIgb6vcwEIO3Bp2Q-nbwPjzxvGHmxluidzKZ4EIusyxqsxoMCPevMmsYHQWhegIvezE8tL_7oUPIx_rGW1lqB6ohZwjSPHTXvnmzqvP1zQDmwPMDN4Neqy3W-ahOAmBdlGx1cnPtEWY7z4cN7oMqY_l4CXwbZbq54Dh6M9ODZmiwd5TarLGv2fqaNpiG5YZwtlnRz9cAIC8GfGz_OAsj9bMLkt3yC_SHHdXeQ3UYQijYkA Name: ttl-controller-token-wpd5f Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: ttl-controller kubernetes.io/service-account.uid: 0d9b87be-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ0dGwtY29udHJvbGxlci10b2tlbi13cGQ1ZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJ0dGwtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBkOWI4N2JlLWU5ZDQtMTFlYi1iMjViLTAyNDJhYzExMDAwOSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTp0dGwtY29udHJvbGxlciJ9.jSUdoCKEBQYkAL5GtVQWCzFmH4FQ6GaenbDm9Cu5psOGhAd7_fpPejjmTinedpCmv0KgYIfOpzWyjQ-b1lJciZ9OFBapVDh--51kYR-enbhXpa5EP8T5O0PcnzXRojRZPMGPgQBWbn4JgTrfplzLRlrjFInTJVXA08a6B5rYXXmdABMCD08eaba9M4XeFuCIYoStmNhYw-fBPbtBegTif2ToT47-EojkMJnL8qI-oOL1nAbFHwaHSPC-czpf7VgSRBpnfTzbGrtrRwUX3vbYrrSTGo2RK87P9Qqb7HPWLnM1LDKHlgwkNmUpp9Dt8Tmun8XYWOpfQwaAqn6zRpbsPA Name: weave-net-token-fn28h Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: weave-net kubernetes.io/service-account.uid: 0d455a77-e9d4-11eb-b25b-0242ac110009 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJ3ZWF2ZS1uZXQtdG9rZW4tZm4yOGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoid2VhdmUtbmV0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiMGQ0NTVhNzctZTlkNC0xMWViLWIyNWItMDI0MmFjMTEwMDA5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOndlYXZlLW5ldCJ9.PpqFn31z-GiFAdE-Nd9-R8xuW8ESiisX53Vn_niAvFQ2DPII8q4150iNJDedLZ6nZiXBgV4e7b3NcZzNGhwRVAUyRgLrIqjdgNk-04YJ40bdP4d_5hSUylEn_QH1dgON6tMV9cK6skC0weFZzpPptGhA0bhyFdNl3DvGKGGmMgfyQsf3tEjZjEyqOuPLPtW0TPF2h98UmSsBzWbn-tCM_GEZTVY-3uxzfn2AfkuSI-e9ri1W4X8_W1z_cFIXk4Pwg0q7RWcJi44BCBJE079ezRLIWbowjXOXpqrZwe_jw_F-fJI_7ddxWDx68-wBwhjVIdNqnnMvzB-RKfgaIcWEiw 部署仪表板时，它使用 externalIPs 将服务绑定到端口 8443。这使得仪表板可供集群外部使用，并可在 https://2886795309-8443-elsy05.environments.katacoda.com/ 中查看\n使用 admin-user 令牌访问仪表板。\n对于生产环境，建议使用 kubectl proxy 来访问仪表板，而不是 externalIP。在 https://github.com/kubernetes/dashboard 上查看更多详细信息。\n","date":"2021-07-20T11:05:00+08:00","image":"https://www.catfish.top/p/k8s-basic-2/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-2/","title":"Kubernetes初探（二）"},{"content":" Katacoda在线课：Launch A Single Node Cluster\n本系列教程希望能通过交互式学习网站与传统方式结合，更高效、容易的学习知识。 本系列教程将使用 Katacoda在线学习平台 完成学习。\n Minikube 是一个可以轻松在本地运行 Kubernetes 的工具。 Minikube 是一个在本地上计算机的虚拟机内运行一个单节点 Kubernetes 集群，便于用户能够完成日常开发工作，同时也能够让新用户快速了解 Kubernetes 。\n详情见： https://github.com/kubernetes/minikube\n步骤 1 - 启动 Minikube Minikube 已经安装并配置到环境中。通过运行 minikube version 命令检查它是否已正确安装。\n$ minikube version minikube version: v1.8.1 commit: cbda04cf6bbe65e987ae52bb393c10099ab62014 通过运行 minikube start 命令启动集群。\n$ minikube start --wait=false * minikube v1.8.1 on Ubuntu 18.04 * Using the none driver based on user configuration * Running on localhost (CPUs=2, Memory=2460MB, Disk=145651MB) ... * OS release is Ubuntu 18.04.4 LTS * Preparing Kubernetes v1.17.3 on Docker 19.03.6 ... - kubelet.resolv-conf=/run/systemd/resolve/resolv.conf * Launching Kubernetes ... * Enabling addons: default-storageclass, storage-provisioner * Configuring local host environment ... * Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; 现在，您的在线终端中有一个正在运行的 Kubernetes 集群。 Minikube 会启动一个虚拟机为 K8S 集群提供运行环境。\n步骤 2 - 集群信息 用户使用 kubectl 客户端与集群交互。该工具用于管理 Kubernetes 和在集群上运行的应用程序.\n通过 kubectl cluster-info 命令查看集群详情信息和健康状态。\n$ kubectl cluster-info Kubernetes master is running at https://172.17.0.10:8443 KubeDNS is running at https://172.17.0.10:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 通过 kubectl get nodes 查看集群中的各节点信息。\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 9m15s v1.17.3 如果节点被标记为 NotReady 则它仍在启动组件阶段。\n此命令显示可用于托管我们的应用程序的所有节点。现在我们只有一个节点，可以看到它的状态是 Ready。\n步骤 3 - 部署容器 现在可以通过 Kubernetes 集群来部署容器。\nkubectl run 命令能够将容器部署到集群中。\n$ kubectl create deployment first-deployment --image=katacoda/docker-http-server deployment.apps/first-deployment created 部署状态可以通过 Pods 的运行状态得知。\n$ kubectl get pods NAME READY STATUS RESTARTS AGE first-deployment-666c48b44-n7qjz 1/1 Running 0 60s 容器运行后，可以根据需求使用不同的网络选项公开暴露接口。其中的一种解决方案是 NodePort ，它为容器提供动态端口。\n$ kubectl expose deployment first-deployment --port=80 --type=NodePort service/first-deployment exposed 下面的命令能够查询到绑定的端口，并且能够发送HTTP请求进行测试。\n$ export PORT=$(kubectl get svc first-deployment -o go-template=\u0026#39;{{range.spec.ports}}{{if .nodePort}}{{.nodePort}}{{\u0026#34;\\n\u0026#34;}}{{end}}{{end}}\u0026#39;) $ echo \u0026#34;Accessing host01:$PORT\u0026#34; Accessing host01:32492 $ curl host01:$PORT \u0026lt;h1\u0026gt;This request was processed by host: first-deployment-666c48b44-n7qjz\u0026lt;/h1\u0026gt; 结果显示的是处理请求的容器。\n步骤 4 - 仪表盘 使用 Minikube 命令启用仪表板\n$ minikube addons enable dashboard * The \u0026#39;dashboard\u0026#39; addon is enabled 通过使用 yaml 文件来定义部署 Kubernetes Dashboard 。该配置仅适用于Katacoda\n/opt/kubernetes-dashboard.yaml\napiVersion:v1kind:Namespacemetadata:labels:addonmanager.kubernetes.io/mode:Reconcilekubernetes.io/minikube-addons:dashboardname:kubernetes-dashboardselfLink:/api/v1/namespaces/kubernetes-dashboardspec:finalizers:- kubernetesstatus:phase:Active---apiVersion:v1kind:Servicemetadata:labels:app:kubernetes-dashboardname:kubernetes-dashboard-katacodanamespace:kubernetes-dashboardspec:ports:- port:80protocol:TCPtargetPort:9090nodePort:30000selector:k8s-app:kubernetes-dashboardtype:NodePort$ kubectl apply -f /opt/kubernetes-dashboard.yaml namespace/kubernetes-dashboard configured service/kubernetes-dashboard-katacoda created 可以使用 Kubernetes 仪表板查看部署到集群中的应用程序。仪表板将部署在 30000 端口，但需要一段时间才能完成启动。\n要查看 Dashboard 启动的进度，请使用 kubectl get pods -n kubernetes-dashboard -w 查看 kube-system 命名空间中的 Pod\n$ kubectl get pods -n kubernetes-dashboard -w NAME READY STATUS RESTARTS AGE dashboard-metrics-scraper-7b64584c5c-jmkls 1/1 Running 0 6m15s kubernetes-dashboard-79d9cd965-fcb4t 1/1 Running 0 6m14s 启动好后，仪表盘的在线访问地址为： https://2886795306-30000-ollie07.environments.katacoda.com/\n","date":"2021-07-20T10:21:00+08:00","image":"https://www.catfish.top/p/k8s-basic-1/kubernates_hu999cd8b4a0602898549f5ade1550b92a_32166_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/k8s-basic-1/","title":"Kubernetes初探（一）"},{"content":"介绍 Katacoda 是一个面向软件工程师的交互式学习和培训平台，可在浏览器中使用真实环境学习和测试新技术，帮助开发人员学习，并掌握最佳实践。\nKatacoda 的目标是消除新技术和技能的障碍。\nKatacoda 提供了一个平台来构建实时交互式演示和培训环境。运行环境可以根据应用要求进行定制。分步指导路径旨在确保用户以最佳方式学习。用户可以根据设计好的引导步骤，通过浏览器上的终端界面操作一套完整的环境，一步步的学习和实践。Katacoda 的出现很好的解决了这些问题。课程设计者可以定制应用程序所需环境，并设计循序渐进的指导路径，旨在确保用户以最佳方式学习。\nKatacoda 同样也是Kubernetes官网的学习工具，能够快速帮助开发者掌握K8s知识与应用，同时能够节省大量搭建部署环境的时间与精力，能够让开发者专注于学习掌握K8s。\nPlayground功能更是为开发者带来一种新的开发体验。借助于Katacoda平台，能够快速构建满足应用要求的各类环境，开发者能够在这之中，专注于完成创造、验证等工作。\n使用体验 课程 Playground 中文翻译-目录 Kubernetes Basic 原文：https://www.katacoda.com/courses/kubernetes\nLaunch A Single Node Cluster 原文：https://www.katacoda.com/courses/kubernetes/launch-single-node-cluster\n","date":"2021-07-19T17:57:00+08:00","image":"https://www.catfish.top/p/katacoda/katacoda_huf47240e26664e9c6e2f7130233d16c2d_114448_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.catfish.top/p/katacoda/","title":"Katacoda 在线学习神器"},{"content":"概述 Google工程师Jeffrey Dean 和 Sanjay Ghemawat在2004年发表了一篇论文MapReduce: Simplified Data Processing on Large Clusters，MapReduce作为大规模数据处理的需求下孕育而生的产物，被创造的初衷是为了解决Google公司内部搜索引擎中大规模数据的并行化处理。\n引用维基百科中对MapReduce的介绍：\n MapReduce是Google提出的一个软件架构，用于大规模数据集（大于1TB）的并行运算。\n 概念“Map（映射）”和“Reduce（归纳）”，及他们的主要思想，都是从函数式编程语言借来的，还有从矢量编程语言借来的特性。当前的软件实现是指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归纳）函数，用来保证所有映射的键值对中的每一个共享相同的键组。\n本系列将根据MIT6.824课程进行学习 MapReduce\n 简单看来，MapReduce的架构正如其名，分为Map和Reduce两部分。\n MapReduce Overview \n作为一个计算框架，其最大的核心便在于计算二字，以往处理计算的模式为单机运行，在大数据的情况下只能使用具有更强算力的计算机来完成计算工作，而算力的提升是需要花费极大的成本，这样在成本上是极为不划算的。在这一背景下MapReduce孕育而生，有个这样一个框架，就可以将数台不同算力的计算机组成一个集群，和适度的调度下并行计算提高效率降低成本。\n根据上图，MapReduce中存在3中角色，Master，Worker（Map），Worker(Reduce）,Master负责Map，Reduce两层的调度管理,Worker(Map)负责进行Map操作，Worker（Reduce）负责进行Reduce操作\n现在以该课程lab1为例来进行细致的学习，整套课程将由Go语言来实现。 lab1主要是对MapReduce模型进行初步的学习，实现一个本机的MapReduce模型，完成对多个文件的词频统计。\n示例程序流程 入口由上层程序控制，这个地方从master调度开始，预先定义Reduce任务个数m，再根据文本文件输入数量n。\nmaster将n传递给doMap也就是Map调度层，告诉Map调度层执行n次Map计算，每个Map计算层对应输入各个文本文件的数据。\nMap调度层将输出m*n个文件作为Map和Reduce的中间数据传递媒介，举例假设现在m为2、n为3，输出文件mid-0-0 mid-0-1 mid-1-0 mid-1-1 mid-2-0 mid-2-1这六个文件,其中mid-0-0 mid-0-1为第一个文本Map操作后的到的切分开的两个中间数据文件，剩下的以此类推。\n在Reduce调度层中便会将这六个文件交由Reduce计算层处理，将件mid-0-0 mid-1-0 mid-2-0交由编号为0的Reduce计算任务处理，显然，编号为1的任务则负责剩下3个文件的规约操作。Reduce调度层中仍然会将每个Reduce计算层任务得到的数据分别存入文件，根据Reduce任务的数量m为2，则文件编号分别为res-0 res-1，这样Map、Reduce两种操作完成，但整个任务还为完成。\nMerge操作则是最后一个步骤通常由master来完成，通过merge操作将上述的res-0 res-1合并，将所有结果存入到一个文件中，这样，整个MapReduce实现的多文本词频统计程序执行完毕。\n 数据结构 type KeyValue struct { Key string Value string } 实现 Master调度 master.go\nfunc (mr *Master) run(jobName string, files []string, nreduce int, schedule func(phase jobPhase), finish func(), ) { mr.jobName = jobName mr.files = files mr.nReduce = nreduce fmt.Printf(\u0026#34;%s: Starting Map/Reduce task %s\\n\u0026#34;, mr.address, mr.jobName) schedule(mapPhase) //map层进行操作 \tschedule(reducePhase) //reduce层进行操作 \tfinish() //结束所有worker \tmr.merge() //合并  fmt.Printf(\u0026#34;%s: Map/Reduce task completed\\n\u0026#34;, mr.address) mr.doneChannel \u0026lt;- true } 这部分展示了master如何对map、reduce进行调度的过程，首先执行所有的map操作，执行完毕后执行所有的reduce操作进行规约，最后merge合并所有reduce之后的结果，把所有的结果汇总并存储。\n Map调度层 common_map.go\nfunc doMap( jobName string, // MapReduce任务名 \tmapTaskNumber int, // Map任务序号 \tinFile string, nReduce int, // Reduce Worker数量 (\u0026#34;R\u0026#34; in the paper) \tmapF func(file string, contents string) []KeyValue, ) { dat, err := ioutil.ReadFile(inFile) //读文本文件 \tif err != nil { panic(err) } res := mapF(inFile, string(dat)) //根据文件中的内容进行单文件Map操作 \tm := make([]map[string]string, nReduce) //对于每个文本文件，将使用KeyValue的slice来存放，之后会对这部分数据进行切分，根据nReduce（reduce层worker数量）来决定划分数目 \tfor _, kv := range res { index := ihash(kv.Key) % nReduce //序号划分，根据Key做hash处理，对结果模你Reduce得到划分序号 \tif m[index] == nil{ m[index] = make(map[string]string) } m[index][kv.Key] = kv.Value } for i := 0; i \u0026lt; nReduce; i++ { filename := reduceName(jobName, mapTaskNumber, i) jsonObj, err := json.Marshal(m[i]) //使用json作为中间数据 \tif err != nil { panic(err) } ioutil.WriteFile(filename, jsonObj, 0644) //将数据写入到文件中 //将json数据写入到文件中，供Reduce操作使用 \t} Map调度层提供数据的输入与输出，不关心计算过程\n Map计算层 wc.go\n//filename 为输入文件文件名 //contents 为文本内容 func mapF(filename string, contents string) []mapreduce.KeyValue { var res []mapreduce.KeyValue m := make(map[string] int) reg := regexp.MustCompile(\u0026#34;\\n|\\r|\\t|[ ]+|[\\\\-]+|\\\\(|\\\\)\u0026#34;) contents = reg.ReplaceAllString(contents,\u0026#34; \u0026#34;) reg = regexp.MustCompile(\u0026#34;[^(a-zA-Z )]\u0026#34;) contents = reg.ReplaceAllString(contents,\u0026#34;\u0026#34;) //fmt.Println(contents) \ts := strings.Split(contents,\u0026#34; \u0026#34;) for i := 0;i \u0026lt; len(s);i++ { str := strings.TrimSpace(s[i]) m[strings.ToLower(str)]++ } for k,v := range m{ val := strconv.Itoa(v) res = append(res, mapreduce.KeyValue{k, val }) } fmt.Println(res) return res } Map计算层根据输入的文本内容完成下列步骤：\n 使用正则表达式将\\n \\t \\r替换为空格 使用正则表达式将无关字符删除 使用strings.Split方法根据空格进行划分 取出划分后的每个词，使用strings.TrimSpace方法去除两端空格，使用map[string] int结构的m来完成统计 将map[string] int转换为[]KeyValue返回  Map计算层和Map调度层共同组成的Map的结构，一个负责计算，一个负责IO，分工明确\n Reduce调度层 common_reduce.go\nfunc doReduce( jobName string, // MapReduce任务名 \treduceTaskNumber int, // Reduce任务序号 \toutFile string, // 输出文件路径 \tnMap int, // Map任务数 (\u0026#34;M\u0026#34; in the paper) \treduceF func(key string, values []string) string, ) { m := make(map[string][]string) for i := 0; i \u0026lt; nMap; i++ { filename := reduceName(jobName, i, reduceTaskNumber) fmt.Println(filename) data, err := ioutil.ReadFile(filename) if err != nil { panic(err) } var tm map[string]string err = json.Unmarshal(data, \u0026amp;tm) if err != nil { panic(err) } for k, v := range tm { if err != nil { panic(err) } m[k] = append(m[k], v) } } dataM := make([]KeyValue,0) for k, v := range m { ans := reduceF(k, v) dataM = append(dataM, KeyValue{k,ans}) } jsonData, err := json.Marshal(dataM) if err != nil { panic(err) } ioutil.WriteFile(outFile, jsonData, 0644) } Reduce调度层完成的工作是将Map执行后的文件读入为数据交有Reduce来规约，同样也不负责Reduce具体的过程，仅负责数据的读入（文件读入）和数据的保存（文件写出）\n Reduce计算层 wc.go\n//key为文本中的单词 //values为各个Map处理后得到的单词的个数 func reduceF(key string, values []string) string { sum :=0 for _,v := range values{ num,err := strconv.Atoi(v) //string转int \tif err!=nil{ panic(err) } sum+=num //累加 \t} return strconv.Itoa(sum) //将int转string返回 } Reduce计算层则仅仅负责数据的规约，步骤如下：\n 遍历整个values切片 将values的数值累加 将累加总和作为结果返回   Merge func (mr *Master) merge() { kvs := make(map[string]string) //总的数据存放 \tfor i := 0; i \u0026lt; mr.nReduce; i++ { p := mergeName(mr.jobName, i) //merge文件名生成 \tfile,err :=ioutil.ReadFile(p) if err != nil { log.Fatal(\u0026#34;Merge: \u0026#34;, err) } var jsonObj []KeyValue err = json.Unmarshal(file,\u0026amp;jsonObj) //json解码 \tif err != nil{ log.Fatal(\u0026#34;Merge: \u0026#34;, err) } for _,v:=range jsonObj{ kvs[v.Key] = v.Value } } } Merge操作便将Reduce操作得到的数据进行合并，完成最后一步工作。\n 总结 通过这几天短暂的学习，MapReduce模型确实能够极大的利用现有计算资源来打造一个算力强劲的计算机集群，但是本实例中也存在几个局限性：\n 本实例仅在单机中运行 操作间数据交换使用文件，在性能上可能会有一定影响 任务划分的科学性也应该是性能上应该考虑的问题，集群中计算机算力各不相同的时候，能者多劳，任务的划分应更加合理，才可能避免水桶原理造成的性能上的损失  如有不足之处，恳请提出批评！\n","date":"2017-09-07T01:21:00+08:00","image":"https://www.catfish.top/p/mit6.824-1/hadoop_hu7a934a9f3335148cadbaca544cf45563_202913_120x120_fill_box_smart1_3.png","permalink":"https://www.catfish.top/p/mit6.824-1/","title":"MIT 6.824 分布式系统初探（一）"},{"content":"字符编码 字集码是把字符集中的字符编码为指定集合中某一对象（例如：比特模式、自然数序列、8位组或者电脉冲），以便文本在计算机中存储和通过通信网络的传递。\n常见的例子包括将拉丁字母表编码成摩斯电码和ASCII。其中，ASCII将字母、数字和其它符号编号，并用7比特的二进制来表示这个整数。通常会额外使用一个扩充的比特，以便于以1个字节的方式存储。 常见的字符编码有： ASCII、UTF-8、Unicode、GBK等\n详见WikiPedia\n ASCII ASCII ( A merican S tandard C ode for I nformation I nterchange) 即美国信息交换标准代码。\nASCII第一次以规范标准的型态发表是在1967年，最后一次更新则是在1986年，至今为止共定义了128个字符；其中33个字符无法显示（一些终端提供了扩展，使得这些字符可显示为诸如笑脸、扑克牌花式等8-bit符号），且这33个字符多数都已是陈废的控制字符。控制字符的用途主要是用来操控已经处理过的文字。在33个字符之外的是95个可显示的字符，包含用键盘敲下空白键所产生的空白字符也算1个可显示字符（显示为空白）。\nASCII的局限在于只能显示26个基本拉丁字母、阿拉伯数目字和英式标点符号，因此只能用于显示现代美国英语（而且在处理英语当中的外来词如naïve、café、élite等等时，所有重音符号都不得不去掉，即使这样做会违反拼写规则）。而EASCII虽然解决了部分西欧语言的显示问题，但对更多其他语言依然无能为力。因此现在的软件系统大多采用Unicode。\n详见WikiPedia\n Unicode  Unicode 是为了解决传统的字符编码方案的局限而产生的，例如ISO 8859-1所定义的字符虽然在不同的国家中广泛地使用，可是在不同国家间却经常出现不兼容的情况。\n很多传统的编码方式都有一个共同的问题，即容许电脑处理双语环境（通常使用拉丁字母以及其本地语言），但却无法同时支持多语言环境（指可同时处理多种语言混合的情况）。\n目前，几乎所有电脑系统都支持基本拉丁字母，并各自支持不同的其他编码方式。Unicode为了和它们相互兼容，其首256字符保留给ISO 8859-1所定义的字符，使既有的西欧语系文字的转换不需特别考量；并且把大量相同的字符重复编到不同的字符码中去，使得旧有纷杂的编码方式得以和Unicode编码间互相直接转换，而不会丢失任何信息。举例来说，全角格式区块包含了主要的拉丁字母的全角格式，在中文、日文、以及韩文字形当中，这些字符以全角的方式来呈现，而不以常见的半角形式显示，这对竖排文字和等宽排列文字有重要作用。\n详见WikiPedia\n UTF-8  UTF-8 （8-bit Unicode Transformation Format）是一种针对Unicode的可变长度字符编码，也是一种前缀码。\n它可以用来表示Unicode标准中的任何字符，且其编码中的第一个字节仍与ASCII兼容，这使得原来处理ASCII字符的软件无须或只须做少部分修改，即可继续使用。因此，它逐渐成为电子邮件、网页及其他存储或发送文字的应用中，优先采用的编码。\nUTF-8 现已经作为通用的字符编码，应用于各中网页编码，数据编码，数据库字符编码等。编码的统一能够写出的程序或网页在中文环境下大大减少乱码的出现。dddddddddddddddddd\n详见WikiPedia\n GBK  汉字内码扩展规范 ，称GBK，全名为《汉字内码扩展规范(GBK)》1.0版，由中华人民共和国全国信息技术标准化技术委员会1995年12月1日制订，国家技术监督局标准化司和电子工业部科技与质量监督司1995年12月15日联合以《技术标函[1995]229号》文件的形式公布。\nGBK的K为汉语拼音Kuo Zhan（扩展）中“扩”字的声母。英文全称Chinese Internal Code Extension Specification。\nGBK 只为“技术规范指导性文件”，不属于国家标准。国家质量技术监督局于2000年3月17日推出了GB 18030-2000标准，以取代GBK。\n BOM ( Byte Order Mark )  这个才是重点，BOM头。\n在UTF-8编码文件中BOM在文件头部，占用三个字节，用来标示该文件属于UTF-8编码，现在已经有很多软件识别BOM头，但是还有些不能识别BOM头，比如Windows自带的记事本软件，这也是用记事本编辑UTF-8编码后执行就会出错的原因了。\nWindows下\n  支持BOM的编辑器 Notepad++\n  不支持BOM的编辑器 Windows自带的记事本 （坑）\n  正因有BOM头的存在，使我在本地Jekyll的调试环境中，页面不能正常显示。\n请使用Windows的童鞋一定注意该问题\n目前已经切换为 Hugo + GitHub Action 的方式来完成静态网站的生成\n","date":"2016-05-18T00:36:00+08:00","image":"https://www.catfish.top/p/bom/bom_hu2fca0881760928b759bf10a8d0d84e1c_194611_120x120_fill_q75_box_smart1.jpg","permalink":"https://www.catfish.top/p/bom/","title":"Blog 第一帖 - 字符坑 BOM"}]